### Install Core LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/install

Installs the main LangChain.js package and the core utilities. This is the primary package needed to start using LangChain.js.

```bash
npm install langchain @langchain/core
```

```bash
pnpm add langchain @langchain/core
```

```bash
yarn add langchain @langchain/core
```

```bash
bun add langchain @langchain/core
```

--------------------------------

### Create and Run Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Assembles the agent by combining the configured model, system prompt, and other components. This snippet shows the initial setup for agent creation.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  systemPrompt: systemPrompt,

```

--------------------------------

### Start Agent Server using Langgraph Dev

Source: https://docs.langchain.com/oss/javascript/langchain/studio

This command starts the Langchain Agent server for development. It allows you to view your agent in the Studio UI. Note that Safari may block localhost connections, requiring the use of the `--tunnel` flag for a secure connection.

```shell
langgraph dev

```

```shell
langgraph dev --tunnel

```

--------------------------------

### Install LangGraph CLI with In-Memory Support

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Installs the LangGraph CLI with in-memory support, which is necessary for setting up a local agent server. Python version 3.11 or higher is required for this installation. This command is executed in a shell environment.

```shell
pip install --upgrade "langgraph-cli[inmem]"
```

--------------------------------

### LangGraph Configuration File for Agent Setup

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Defines the configuration for a LangGraph application in a `langgraph.json` file. This JSON specifies project dependencies, lists the available graphs (linking the agent definition from `src/agent.py`), and points to the environment file (`.env`) for configuration settings. This file is used by the LangGraph CLI to set up and run the agent.

```json
{
  "dependencies": ["ப்பான."],
  "graphs": {
    "agent": "./src/agent.py:agent"
  },
  "env": ".env"
}
```

--------------------------------

### Create and Run Agent Chat UI Locally (Bash)

Source: https://docs.langchain.com/oss/javascript/langchain/ui

This snippet demonstrates how to create a new Agent Chat UI project using npx, navigate into the project directory, install dependencies, and start the development server. It's a quick way to set up a local instance for customization or testing.

```bash
# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```

--------------------------------

### Example: Using JSON Schema for Tool Input (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

An alternative way to define tool input schemas using JSON schema objects. Note that JSON schemas are not validated at runtime.

```typescript
const getWeather = tool(
  ({ city }) => `It's always sunny in ${city}!`,
  {
    name: "get_weather_for_location",
    description: "Get the weather for a given city",
    schema: {
      type: "object",
      properties: {
        city: {
          type: "string",
          description: "The city to get the weather for"
        }
      },
      required: ["city"]
    },
  }
);
```

--------------------------------

### Install LangChain.js and dependencies for SQL agent

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the necessary LangChain.js packages, TypeORM, SQLite3, and Zod for building the SQL agent. This setup is crucial for database interaction and data validation.

```bash
npm i langchain @langchain/core typeorm sqlite3 zod
```

```bash
yarn add langchain @langchain/core typeorm sqlite3 zod
```

```bash
pnpm add langchain @langchain/core typeorm sqlite3 zod
```

--------------------------------

### Install Project Dependencies using Pip

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Installs the project's Python dependencies using pip. The `-e` flag installs the package in editable mode, meaning changes to the source code will be reflected immediately without needing to reinstall. This command is typically run from the root of the LangGraph application directory.

```shell
pip install -e .
```

--------------------------------

### LangChain.js AI Agent with Tools and Memory

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

This TypeScript code sets up an AI agent using LangChain.js. It defines tools for getting weather and user location, configures a chat model, and specifies a structured response format. The agent is initialized with a system prompt, tools, response format, and a memory checkpointer. It then demonstrates invoking the agent for a weather query and a follow-up 'thank you' message, showcasing conversation context and tool usage.

```typescript
import { createAgent, tool, initChatModel } from "langchain";
import { MemorySaver, type Runtime } from "@langchain/langgraph";
import * as z from "zod";

// Define system prompt
const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

  You have access to two tools:

  - get_weather_for_location: use this to get the weather for a specific location
  - get_user_location: use this to get the user's location

  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;

// Define tools
const getWeather = tool(
  ({ city }) => `It's always sunny in ${city}!`,
  {
    name: "get_weather_for_location",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string(),
    }),
  }
);

const getUserLocation = tool(
  (_, config: Runtime<{ user_id: string}>) => {
    const { user_id } = config.context;
    return user_id === "1" ? "Florida" : "SF";
  },
  {
    name: "get_user_location",
    description: "Retrieve user information based on user ID",
    schema: z.object({}),
  }
);

// Configure model
const model = await initChatModel(
  "claude-sonnet-4-5-20250929",
  { temperature: 0 }
);

// Define response format
const responseFormat = z.object({
  punny_response: z.string(),
  weather_conditions: z.string().optional(),
});

// Set up memory
const checkpointer = new MemorySaver();

// Create agent
const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  systemPrompt: systemPrompt,
  tools: [getUserLocation, getWeather],
  responseFormat,
  checkpointer,
});

// Run agent
// `thread_id` is a unique identifier for a given conversation.
const config = {
  configurable: { thread_id: "1" },
  context: { user_id: "1" },
};

const response = await agent.invoke(
  { messages: [{ role: "user", content: "what is the weather outside?" }] },
  config
);
console.log(response.structuredResponse);
// {
//   punny_response: "Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
//   weather_conditions: "It's always sunny in Florida!"
// }

// Note that we can continue the conversation using the same `thread_id`.
const thankYouResponse = await agent.invoke(
  { messages: [{ role: "user", content: "thank you!" }] },
  config
);
console.log(thankYouResponse.structuredResponse);
// {
//   punny_response: "You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
//   weather_conditions: undefined
// }
```

--------------------------------

### Install Langchain.js Package

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Installs the necessary 'langchain' package for building Langchain.js applications. Supports installation via npm, yarn, or pnpm.

```bash
npm install langchain
```

```bash
yarn add langchain
```

```bash
pnpm add langchain
```

--------------------------------

### Extended Agentic RAG Example for LangGraph Docs (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/retrieval

This extended example shows a more complex Agentic RAG setup for querying LangGraph documentation. It includes a custom `fetchDocumentation` tool with domain restrictions and a detailed system prompt to guide the agent. The example fetches content from `llms.txt` and then invokes the agent with a user's question.

```typescript
import { tool, createAgent, HumanMessage } from "langchain";
import * as z from "zod";

const ALLOWED_DOMAINS = ["https://langchain-ai.github.io/"];
const LLMS_TXT = "https://langchain-ai.github.io/langgraph/llms.txt";

const fetchDocumentation = tool(
    async (input) => {
      if (!ALLOWED_DOMAINS.some((domain) => input.url.startsWith(domain))) {
        return `Error: URL not allowed. Must start with one of: ${ALLOWED_DOMAINS.join(", ")}`;
      }
      const response = await fetch(input.url);
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      return response.text();
    },
    {
      name: "fetch_documentation",
      description: "Fetch and convert documentation from a URL",
      schema: z.object({
        url: z.string().describe("The URL of the documentation to fetch"),
      }),
    }
  );

const llmsTxtResponse = await fetch(LLMS_TXT);
const llmsTxtContent = await llmsTxtResponse.text();

const systemPrompt = `
  You are an expert TypeScript developer and technical assistant.
  Your primary role is to help users with questions about LangGraph and related tools.

  Instructions:

  1. If a user asks a question you're unsure about — or one that likely involves API usage,
     behavior, or configuration — you MUST use the `fetch_documentation` tool to consult the relevant docs.
  2. When citing documentation, summarize clearly and include relevant context from the content.
  3. Do not use any URLs outside of the allowed domain.
  4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.

  You can access official documentation from the following approved sources:

  ${llmsTxtContent}

  You MUST consult the documentation to get up to date documentation
  before answering a user's question about LangGraph.

  Your answers should be clear, concise, and technically accurate.
  `;

const tools = [fetchDocumentation];

const agent = createAgent({
    model: "claude-sonnet-4-0",
    tools,
    systemPrompt,
    name: "Agentic RAG",
  });

const response = await agent.invoke({
    messages: [
      new HumanMessage(
        "Write a short example of a langgraph agent using the " +
        "prebuilt create react agent. the agent should be able " +
        "to look up stock pricing information."
      ),
    ],
  });

console.log(response.messages.at(-1)?.content);
```

--------------------------------

### Synchronize Project Dependencies using Uv

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Synchronizes project dependencies using the `uv` tool. This command ensures that the installed packages match the project's requirements, potentially offering faster dependency management than pip. It is executed from the root of the LangGraph application directory.

```shell
uv sync
```

--------------------------------

### Clone and Run Agent Chat UI Locally (Bash)

Source: https://docs.langchain.com/oss/javascript/langchain/ui

This snippet shows how to clone the Agent Chat UI repository from GitHub, navigate into the cloned directory, install project dependencies using pnpm, and then start the development server. This method is useful for developers who want to work directly with the source code.

```bash
# Clone the repository
git clone https://github.com/langchain-ai/agent-chat-ui.git
cd agent-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```

--------------------------------

### Install @modelcontextprotocol/sdk

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Install the @modelcontextprotocol/sdk library using your preferred package manager (npm, pnpm, yarn, or bun). This is the foundational step for building MCP servers.

```bash
npm install @modelcontextprotocol/sdk
```

```bash
pnpm add @modelcontextprotocol/sdk
```

```bash
yarn add @modelcontextprotocol/sdk
```

```bash
bun add @modelcontextprotocol/sdk
```

--------------------------------

### Tool Execution Loop with Model and Tools

Source: https://docs.langchain.com/oss/javascript/langchain/models

This example illustrates the manual tool execution loop. It shows how to get tool calls from a model, execute the requested tool, and then pass the tool's results back to the model to generate a final response. This pattern is crucial when not using an agent.

```typescript
// Bind (potentially multiple) tools to the model
const modelWithTools = model.bindTools([get_weather])

// Step 1: Model generates tool calls
const messages = [{"role": "user", "content": "What's the weather in Boston?"}]
const ai_msg = await modelWithTools.invoke(messages)
messages.push(ai_msg)

// Step 2: Execute tools and collect results
for (const tool_call of ai_msg.tool_calls) {
    // Execute the tool with the generated arguments
    const tool_result = await get_weather.invoke(tool_call)
    messages.push(tool_result)
}

// Step 3: Pass results back to model for final response
const final_response = await modelWithTools.invoke(messages)
console.log(final_response.text)
// "The current weather in Boston is 72°F and sunny."
```

--------------------------------

### Install Pinecone Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/pinecone` package for integrating with Pinecone, a managed vector database service. Also requires the `@pinecone-database/pinecone` client library.

```bash
npm i @langchain/pinecone
```

```bash
yarn add @langchain/pinecone
```

```bash
pnpm add @langchain/pinecone
```

--------------------------------

### Install Chroma Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/community` package for integrating with the Chroma vector store. Chroma is a popular open-source vector database.

```bash
npm i @langchain/community
```

```bash
yarn add @langchain/community
```

```bash
pnpm add @langchain/community
```

--------------------------------

### Install Memory Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/classic` package required for the MemoryVectorStore. This is a lightweight, in-memory solution suitable for testing or small workloads.

```bash
npm i @langchain/classic
```

```bash
yarn add @langchain/classic
```

```bash
pnpm add @langchain/classic
```

--------------------------------

### Create Tools for Weather Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Implements tools for retrieving weather information and user location. It utilizes Zod for input schema validation and demonstrates runtime configuration for dynamic behavior.

```typescript
import { type Runtime } from "@langchain/langgraph";
import { tool } from "langchain";
import * as z from "zod";

const getWeather = tool(
  (input) => `It's always sunny in ${input.city}!`,
  {
    name: "get_weather_for_location",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string().describe("The city to get the weather for"),
    }),
  }
);

type AgentRuntime = Runtime<{ user_id: string }>;

const getUserLocation = tool(
  (_, config: AgentRuntime) => {
    const { user_id } = config.context;
    return user_id === "1" ? "Florida" : "SF";
  },
  {
    name: "get_user_location",
    description: "Retrieve user information based on user ID",
  }
);
```

--------------------------------

### Install Qdrant Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/qdrant` package for Qdrant vector store integration. Qdrant is a popular open-source vector similarity search engine.

```bash
npm i @langchain/qdrant
```

```bash
yarn add @langchain/qdrant
```

```bash
pnpm add @langchain/qdrant
```

--------------------------------

### Configure Language Model for Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Initializes a chat model with specific parameters like temperature, timeout, and max tokens for consistent and controlled responses.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel(
  "claude-sonnet-4-5-20250929",
  { temperature: 0.5, timeout: 10, maxTokens: 1000 }
);
```

--------------------------------

### Install @langchain/mcp-adapters

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Installs the @langchain/mcp-adapters library using various package managers (npm, pnpm, yarn, bun). This library is necessary for integrating MCP tools with LangChain agents.

```bash
npm install @langchain/mcp-adapters
```

```bash
pnpm add @langchain/mcp-adapters
```

```bash
yarn add @langchain/mcp-adapters
```

```bash
bun add @langchain/mcp-adapters
```

--------------------------------

### Install and Configure VertexAI Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/google-vertexai` package and sets the `GOOGLE_APPLICATION_CREDENTIALS` environment variable for VertexAI embeddings.

```bash
npm i @langchain/google-vertexai
yarn add @langchain/google-vertexai
pnpm add @langchain/google-vertexai
```

```bash
GOOGLE_APPLICATION_CREDENTIALS=credentials.json
```

--------------------------------

### Install Azure Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the Azure package for Langchain.js, supporting integration with Azure OpenAI services. Available for npm, pnpm, yarn, and bun.

```bash
npm install @langchain/azure
```

```bash
pnpm install @langchain/azure
```

```bash
yarn add @langchain/azure
```

```bash
bun add @langchain/azure
```

--------------------------------

### Install and Use Chroma Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/community` package and initializes a Chroma vector store. Chroma is a popular vector database that can be run locally or in a client-server mode.

```bash
npm i @langchain/community
yarn add @langchain/community
pnpm add @langchain/community
```

```typescript
import { Chroma } from "@langchain/community/vectorstores/chroma";

const vectorStore = new Chroma(embeddings, {

```

--------------------------------

### Install and Configure MistralAI Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/mistralai` package and sets the `MISTRAL_API_KEY` environment variable for MistralAI embeddings.

```bash
npm i @langchain/mistralai
yarn add @langchain/mistralai
pnpm add @langchain/mistralai
```

```bash
MISTRAL_API_KEY=your-api-key
```

--------------------------------

### Define System Prompt for Weather Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Defines the agent's role and behavior as an expert weather forecaster who speaks in puns. It specifies the available tools and how to handle location information.

```typescript
const systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n- get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;
```

--------------------------------

### Install LangChain Community and PDF Parsing Packages

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary LangChain community package and the pdf-parse library for handling PDF documents. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/community pdf-parse
```

```bash
yarn add @langchain/community pdf-parse
```

```bash
pnpm add @langchain/community pdf-parse
```

--------------------------------

### Install LangChain.js Integrations

Source: https://docs.langchain.com/oss/javascript/langchain/install

Installs specific provider integration packages for LangChain.js, such as OpenAI or Anthropic. These packages extend LangChain's capabilities by connecting to different LLM providers and tools.

```bash
# Installing the OpenAI integration
npm install @langchain/openai
# Installing the Anthropic integration
npm install @langchain/anthropic
```

```bash
# Installing the OpenAI integration
pnpm install @langchain/openai
# Installing the Anthropic integration
pnpm install @langchain/anthropic
```

```bash
# Installing the OpenAI integration
yarn add @langchain/openai
# Installing the Anthropic integration
yarn add @langchain/anthropic
```

```bash
# Installing the OpenAI integration
bun add @langchain/openai
# Installing the Anthropic integration
bun add @langchain/anthropic
```

--------------------------------

### Install and Configure Cohere Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/cohere` package and sets the `COHERE_API_KEY` environment variable for Cohere embeddings.

```bash
npm i @langchain/cohere
yarn add @langchain/cohere
pnpm add @langchain/cohere
```

```bash
COHERE_API_KEY=your-api-key
```

--------------------------------

### Quick Start: LangChain Agent with Automatic LangSmith Tracing

Source: https://docs.langchain.com/oss/javascript/langchain/observability

This TypeScript example demonstrates creating a LangChain agent with 'create_agent' and running an invocation. All steps, including tool calls and model interactions, are automatically traced to LangSmith without additional code.

```typescript
import { createAgent } from "@langchain/agents";

function sendEmail(to: string, subject: string, body: string): string {
    // ... email sending logic
    return `Email sent to ${to}`;
}

function searchWeb(query: string): string {
    // ... web search logic
    return `Search results for: ${query}`;
}

const agent = createAgent({
    model: "gpt-4o",
    tools: [sendEmail, searchWeb],
    systemPrompt: "You are a helpful assistant that can send emails and search the web."
});

// Run the agent - all steps will be traced automatically
const response = await agent.invoke({
    messages: [{ role: "user", content: "Search for the latest AI news and email a summary to john@example.com" }]
});
```

--------------------------------

### ReAct Loop Example: Tool Use in Agent

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Illustrates the ReAct (Reasoning + Acting) pattern where an agent alternates between reasoning and tool calls to fulfill a user request. This example shows how an agent identifies popular headphones, checks inventory, and provides a final answer.

```text
================================ Human Message =================================

Find the most popular wireless headphones right now and check if they're in stock

================================== Ai Message ==================================
Tool Calls:
  search_products (call_abc123)
 Call ID: call_abc123
  Args:
    query: wireless headphones

================================= Tool Message =================================

Found 5 products matching "wireless headphones". Top 5 results: WH-1000XM5, ...

================================== Ai Message ==================================
Tool Calls:
  check_inventory (call_def456)
 Call ID: call_def456
  Args:
    product_id: WH-1000XM5

================================= Tool Message =================================

Product WH-1000XM5: 10 units in stock

================================== Ai Message ==================================

I found wireless headphones (model WH-1000XM5) with 10 units in stock...
```

--------------------------------

### Install Langgraph CLI

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the latest version of the Langgraph command-line interface globally. This is a prerequisite for running agents within the Langgraph environment.

```shell
npm i -g langgraph-cli@latest
```

--------------------------------

### Add Memory to Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Initializes a MemorySaver for managing conversational state. For production environments, a persistent checkpointer that saves to a database is recommended.

```typescript
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();
```

--------------------------------

### Extended Example: GitHub vs GitLab Tool Selection in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This extended example shows how to dynamically select between GitHub and GitLab tools based on runtime context. It defines specific tools for creating issues in each platform and uses middleware with `contextSchema` to filter the available tools. The agent is then invoked with a specific provider context to demonstrate the selection.

```typescript
import * as z from "zod";
import { createAgent, createMiddleware, tool, HumanMessage } from "langchain";

const githubCreateIssue = tool(
  async ({ repo, title }) => ({
    url: `https://github.com/${repo}/issues/1`,
    title,
  }),
  {
    name: "github_create_issue",
    description: "Create an issue in a GitHub repository",
    schema: z.object({ repo: z.string(), title: z.string() }),
  }
);

const gitlabCreateIssue = tool(
  async ({ project, title }) => ({
    url: `https://gitlab.com/${project}/-/issues/1`,
    title,
  }),
  {
    name: "gitlab_create_issue",
    description: "Create an issue in a GitLab project",
    schema: z.object({ project: z.string(), title: z.string() }),
  }
);

const allTools = [githubCreateIssue, gitlabCreateIssue];

const toolSelector = createMiddleware({
  name: "toolSelector",
  contextSchema: z.object({ provider: z.enum(["github", "gitlab"]) }),
  wrapModelCall: (request, handler) => {
    const provider = request.runtime.context.provider;
    const toolName = provider === "gitlab" ? "gitlab_create_issue" : "github_create_issue";
    const selectedTools = request.tools.filter((t) => t.name === toolName);
    const modifiedRequest = { ...request, tools: selectedTools };
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: allTools,
  middleware: [toolSelector],
});

// Invoke with GitHub context
await agent.invoke(
  {
    messages: [
      new HumanMessage("Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"),
    ],
  },
  {
    context: { provider: "github" },
  }
);
```

--------------------------------

### Install Google GenAI Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the Google Generative AI package for Langchain.js using npm, pnpm, yarn, or bun. This enables integration with Google's Gemini models.

```bash
npm install @langchain/google-genai
```

```bash
pnpm install @langchain/google-genai
```

```bash
yarn add @langchain/google-genai
```

```bash
bun add @langchain/google-genai
```

--------------------------------

### Install Anthropic Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the Anthropic package for Langchain.js using common package managers (npm, pnpm, yarn, bun). This enables integration with Anthropic's chat models.

```bash
npm install @langchain/anthropic
```

```bash
pnpm install @langchain/anthropic
```

```bash
yarn add @langchain/anthropic
```

```bash
pnpm add @langchain/anthropic
```

--------------------------------

### System Message: Basic Instructions (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Example of creating a SystemMessage with basic instructions to prime the model's behavior. This is used to set the tone or provide initial context for the assistant.

```typescript
import { SystemMessage, HumanMessage, AIMessage } from "langchain";

const systemMsg = new SystemMessage("You are a helpful coding assistant.");

const messages = [
  systemMsg,
  new HumanMessage("How do I create a REST API?"),
];
const response = await model.invoke(messages);
```

--------------------------------

### Install and Use Memory Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/classic` package and initializes a `MemoryVectorStore`. This vector store operates in memory and is suitable for smaller datasets or testing.

```bash
npm i @langchain/classic
yarn add @langchain/classic
pnpm add @langchain/classic
```

```typescript
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);
```

--------------------------------

### Install OpenAI Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using OpenAI embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/openai
```

```bash
yarn add @langchain/openai
```

```bash
pnpm add @langchain/openai
```

--------------------------------

### Install AgentEvals and LangChain Core Packages

Source: https://docs.langchain.com/oss/javascript/langchain/test

Installs the AgentEvals package and the LangChain Core library using npm. These are essential dependencies for performing integration tests on agentic applications.

```bash
npm install agentevals @langchain/core
```

--------------------------------

### Inject User Writing Style Middleware for Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware injects a user's email writing style from a store into the LLM's prompt. It requires a `userId` from the runtime context and reads writing style preferences (tone, greeting, sign-off, example email) from a store. The retrieved style is formatted into a user message to guide the LLM's response generation, particularly for drafting emails. Dependencies include 'langchain' and 'zod'.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const injectWritingStyle = createMiddleware({
  name: "InjectWritingStyle",
  contextSchema,
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's writing style examples
    const store = request.runtime.store;  // [!code highlight]
    const writingStyle = await store.get(["writing_style"], userId);  // [!code highlight]

    if (writingStyle) {
      const style = writingStyle.value;
      // Build style guide from stored examples
      const styleContext = `Your writing style:
    - Tone: ${style.tone || 'professional'}
    - Typical greeting: "${style.greeting || 'Hi'}"
    - Typical sign-off: "${style.signOff || 'Best'}"
    - Example email you've written:
    ${style.exampleEmail || ''}`;

      // Append at end - models pay more attention to final messages
      const messages = [
        ...request.messages,
        { role: "user", content: styleContext }
      ];
      request = request.override({ messages });  // [!code highlight]
    }

    return handler(request);
  },
});
```

--------------------------------

### Streaming Semantic Events with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates using `streamEvents()` to stream semantic events from Langchain chat models. This method simplifies filtering by event type and aggregates the full message. The example logs the start input, individual text tokens as they are streamed, and the final complete message output.

```typescript
const stream = await model.streamEvents("Hello");
for await (const event of stream) {
    if (event.event === "on_chat_model_start") {
        console.log(`Input: ${event.data.input}`);
    }
    if (event.event === "on_chat_model_stream") {
        console.log(`Token: ${event.data.chunk.text}`);
    }
    if (event.event === "on_chat_model_end") {
        console.log(`Full message: ${event.data.output.text}`);
    }
}
```

--------------------------------

### Install OpenAI Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the OpenAI package for Langchain.js using various package managers like npm, pnpm, yarn, and bun. This package is required to integrate OpenAI chat models.

```bash
npm install @langchain/openai
```

```bash
pnpm install @langchain/openai
```

```bash
yarn add @langchain/openai
```

```bash
bun add @langchain/openai
```

--------------------------------

### Install and Configure AWS Bedrock Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/aws` package and sets the `BEDROCK_AWS_REGION` environment variable for AWS Bedrock embeddings.

```bash
npm i @langchain/aws
yarn add @langchain/aws
pnpm add @langchain/aws
```

```bash
BEDROCK_AWS_REGION=your-region
```

--------------------------------

### Install and Configure Azure OpenAI Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/openai` package and sets up environment variables for Azure OpenAI embeddings. Requires `AZURE_OPENAI_API_INSTANCE_NAME`, `AZURE_OPENAI_API_KEY`, and `AZURE_OPENAI_API_VERSION` to be set.

```bash
npm i @langchain/openai
yarn add @langchain/openai
pnpm add @langchain/openai
```

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

--------------------------------

### Install MistralAI Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using MistralAI embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/mistralai
```

```bash
yarn add @langchain/mistralai
```

```bash
pnpm add @langchain/mistralai
```

--------------------------------

### Implementing Dynamic System Prompts with Middleware

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Shows how to dynamically adjust the system prompt at runtime using middleware. This example uses Zod for schema validation and a middleware function to tailor the prompt based on the 'userRole' context, providing different instructions for 'expert' and 'beginner' users.

```typescript
import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.enum(["expert", "beginner"]),
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [/* ... */],
  contextSchema,
  middleware: [
    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {
      const userRole = runtime.context.userRole || "user";
      const basePrompt = "You are a helpful assistant.";

      if (userRole === "expert") {
        return `${basePrompt} Provide detailed technical responses.`;
      } else if (userRole === "beginner") {
        return `${basePrompt} Explain concepts simply and avoid jargon.`;
      }
      return basePrompt;
    }),
  ],
});

// The system prompt will be set dynamically based on context
const result = await agent.invoke(
  { messages: [{ role: "user", content: "Explain machine learning" }] },
  { context: { userRole: "expert" } }
);
```

--------------------------------

### Create Basic Agent with Claude and Weather Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

This snippet demonstrates how to create a basic AI agent using Langchain. It utilizes Claude Sonnet 4.5 as the language model and defines a 'getWeather' tool. The agent is configured to answer user questions by invoking the defined tools. Ensure ANTHROPIC_API_KEY environment variable is set.

```typescript
import { createAgent, tool } from "langchain";
import * as z from "zod";

const getWeather = tool(
  (input) => `It's always sunny in ${input.city}!`,
  {
    name: "get_weather",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string().describe("The city to get the weather for"),
    }),
  }
);

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  tools: [getWeather],
});

console.log(
  await agent.invoke({
    messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
  })
);

```

--------------------------------

### Install MongoDB Atlas Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/mongodb` package for integrating with MongoDB Atlas Vector Search. This package requires the `mongodb` driver as well.

```bash
npm i @langchain/mongodb
```

```bash
yarn add @langchain/mongodb
```

```bash
pnpm add @langchain/mongodb
```

--------------------------------

### Install Cohere Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using Cohere embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/cohere
```

```bash
yarn add @langchain/cohere
```

```bash
pnpm add @langchain/cohere
```

--------------------------------

### Define Structured Response Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Defines a Zod object schema for the agent's response, ensuring predictable output by specifying fields like 'punny_response' and 'weather_conditions'.

```typescript
const responseFormat = z.object({
  punny_response: z.string(),
  weather_conditions: z.string().optional(),
});
```

--------------------------------

### Install Google VertexAI Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using Google VertexAI embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/google-vertexai
```

```bash
yarn add @langchain/google-vertexai
```

```bash
pnpm add @langchain/google-vertexai
```

--------------------------------

### Create a LangChain Agent with a Tool

Source: https://docs.langchain.com/oss/javascript/langchain/overview

This TypeScript example demonstrates how to create a LangChain agent. It defines a simple 'getWeather' tool using Zod for schema validation and then initializes an agent with a specific model and the defined tool. The agent is then invoked with a user query.

```typescript
import * as z from "zod";
// npm install @langchain/anthropic to call the model
import { createAgent, tool } from "langchain";

const getWeather = tool(
  ({ city }) => `It's always sunny in ${city}!`,
  {
    name: "get_weather",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string(),
    }),
  },
);

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  tools: [getWeather],
});

console.log(
  await agent.invoke({
    messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
  })
);
```

--------------------------------

### Math Server Implementation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

A TypeScript example demonstrating how to create a Math MCP server using stdio transport. This server exposes 'add' and 'multiply' tools, defining their input schemas and handling tool calls. It's suitable for local testing and environments where stdio is preferred.

```typescript
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";

const server = new Server(
    {
        name: "math-server",
        version: "0.1.0",
    },
    {
        capabilities: {
        tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "add",
            description: "Add two numbers",
            inputSchema: {
            type: "object",
            properties: {
                a: {
                type: "number",
                description: "First number",
                },
                b: {
                type: "number",
                description: "Second number",
                },
            },
            required: ["a", "b"],
            },
        },
        {
            name: "multiply",
            description: "Multiply two numbers",
            inputSchema: {
            type: "object",
            properties: {
                a: {
                type: "number",
                description: "First number",
                },
                b: {
                type: "number",
                description: "Second number",
                },
            },
            required: ["a", "b"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "add": {
        const { a, b } = request.params.arguments as { a: number; b: number };
        return {
            content: [
            {
                type: "text",
                text: String(a + b),
            },
            ],
        };
        }
        case "multiply": {
        const { a, b } = request.params.arguments as { a: number; b: number };
        return {
            content: [
            {
                type: "text",
                text: String(a * b),
            },
            ],
        };
        }
        default:
        throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

async function main() {
    const transport = new StdioServerTransport();
    await server.connect(transport);
    console.error("Math MCP server running on stdio");
}

main();

```

--------------------------------

### Install AWS Bedrock Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the AWS package for Langchain.js using npm, pnpm, yarn, or bun. This is necessary for integrating with AWS Bedrock services, including Converse API.

```bash
npm install @langchain/aws
```

```bash
pnpm install @langchain/aws
```

```bash
yarn add @langchain/aws
```

```bash
bun add @langchain/aws
```

--------------------------------

### Personal Assistant Supervisor Example using TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript example demonstrates a multi-agent system where a supervisor agent coordinates specialized sub-agents for calendar and email tasks. It utilizes Langchain's tool and createAgent functionalities, along with Zod for schema validation. The example includes stubbed API tools for calendar event creation, email sending, and checking time slot availability, and wraps these specialized agents as tools for the supervisor. Dependencies include 'langchain', '@langchain/anthropic', and 'zod'.

```typescript
/**
 * Personal Assistant Supervisor Example
 *
 * This example demonstrates the tool calling pattern for multi-agent systems.
 * A supervisor agent coordinates specialized sub-agents (calendar and email)
 * that are wrapped as tools.
 */

import { tool, createAgent } from "langchain";
import { ChatAnthropic } from "@langchain/anthropic";
import { z } from "zod";

// =============================================================================
// Step 1: Define low-level API tools (stubbed)
// ============================================================================

const createCalendarEvent = tool(
  async ({ title, startTime, endTime, attendees, location }) => {
    // Stub: In practice, this would call Google Calendar API, Outlook API, etc.
    return `Event created: ${title} from ${startTime} to ${endTime} with ${attendees.length} attendees`;
  },
  {
    name: "create_calendar_event",
    description: "Create a calendar event. Requires exact ISO datetime format.",
    schema: z.object({
      title: z.string(),
      startTime: z.string().describe("ISO format: '2024-01-15T14:00:00'"),
      endTime: z.string().describe("ISO format: '2024-01-15T15:00:00'"),
      attendees: z.array(z.string()).describe("email addresses"),
      location: z.string().optional().default(""),
    }),
  }
);

const sendEmail = tool(
  async ({ to, subject, body, cc }) => {
    // Stub: In practice, this would call SendGrid, Gmail API, etc.
    return `Email sent to ${to.join(", ")} - Subject: ${subject}`;
  },
  {
    name: "send_email",
    description:
      "Send an email via email API. Requires properly formatted addresses.",
    schema: z.object({
      to: z.array(z.string()).describe("email addresses"),
      subject: z.string(),
      body: z.string(),
      cc: z.array(z.string()).optional().default([]),
    }),
  }
);

const getAvailableTimeSlots = tool(
  async ({ attendees, date, durationMinutes }) => {
    // Stub: In practice, this would query calendar APIs
    return ["09:00", "14:00", "16:00"];
  },
  {
    name: "get_available_time_slots",
    description:
      "Check calendar availability for given attendees on a specific date.",
    schema: z.object({
      attendees: z.array(z.string()),
      date: z.string().describe("ISO format: '2024-01-15'"),
      durationMinutes: z.number(),
    }),
  }
);

// =============================================================================
// Step 2: Create specialized sub-agents
// ============================================================================

const llm = new ChatAnthropic({
  model: "claude-haiku-4-5-20251001",
});

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: `
  You are a calendar scheduling assistant.
  Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm')
  into proper ISO datetime formats.
  Use get_available_time_slots to check availability when needed.
  Use create_calendar_event to schedule events.
  Always confirm what was scheduled in your final response.
    `.trim(),
});

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: `
  You are an email assistant.
  Compose professional emails based on natural language requests.
  Extract recipient information and craft appropriate subject lines and body text.
  Use send_email to send the message.
  Always confirm what was sent in your final response.
    `.trim(),
});

// =============================================================================
// Step 3: Wrap sub-agents as tools for the supervisor
// ============================================================================

const scheduleEvent = tool(
  async ({ request }) => {
    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: request }],
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "schedule_event",
    description: `
  Schedule calendar events using natural language.

  Use this when the user wants to create, modify, or check calendar appointments.
  Handles date/time parsing, availability checking, and event creation.

  Input: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2pm')
      `,
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

const manageEmail = tool(
  async ({ request }) => {
    const result = await emailAgent.invoke({
      messages: [{ role: "user", content: request }],
    });

```

--------------------------------

### Setting a Static System Prompt for an Agent

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Demonstrates how to initialize an agent with a static system prompt string. This prompt guides the agent's overall behavior and response style. If no system prompt is provided, the agent infers its task from the messages.

```typescript
const agent = createAgent({
  model,
  tools,
  systemPrompt: "You are a helpful assistant. Be concise and accurate.",
});
```

--------------------------------

### Create Calendar Agent with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This code snippet demonstrates how to initialize a calendar agent using Langchain. It defines a system prompt to guide the agent's behavior, specifying its role as a scheduling assistant. It also configures the agent with necessary tools such as 'create_calendar_event' and 'get_available_time_slots', and specifies the language model to be used.

```typescript
import { createAgent } from "langchain";

const CALENDAR_AGENT_PROMPT = `
You are a calendar scheduling assistant.
Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm')
into proper ISO datetime formats.
Use get_available_time_slots to check availability when needed.
Use create_calendar_event to schedule events.
Always confirm what was scheduled in your final response.
`.trim();

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: CALENDAR_AGENT_PROMPT,
});
```

--------------------------------

### LangChain.js Video Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Provides examples of creating HumanMessage objects with video content for LangChain chat models in JavaScript. Demonstrates inputting video via base64 encoding or provider file IDs. Verify model support for video processing.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this video." },
    {
      type: "video",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this video." },
    { type: "video", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Install LangChain Dependencies (Bash)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This snippet provides commands to install the necessary LangChain.js dependencies using different package managers: npm, yarn, and pnpm. It includes the core 'langchain' package along with community and textsplitter modules required for RAG applications.

```bash
npm i langchain @langchain/community @langchain/textsplitters

```

```bash
yarn add langchain @langchain/community @langchain/textsplitters

```

```bash
pnpm add langchain @langchain/community @langchain/textsplitters

```

--------------------------------

### Install AWS Bedrock Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using AWS Bedrock embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/aws
```

```bash
yarn add @langchain/aws
```

```bash
pnpm add @langchain/aws
```

--------------------------------

### Invoking an Agent

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Provides a basic example of how to invoke an agent. The agent is called with a user message, and it processes this message according to its configuration, including its system prompt and available tools.

```typescript
await agent.invoke({
  messages: [{ role: "user", content: "What's the weather in San Francisco?" }],
})
```

--------------------------------

### LangChain Tool Calling with Zod Schema for Product Review Analysis

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example demonstrates how to use `toolStrategy` with a Zod schema to define the structure for analyzing product reviews. It shows the import of necessary LangChain and Zod components, the definition of the `ProductReview` schema, and the creation of an agent configured to output structured JSON. The output is the structured analysis of a given review.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductReview = z.object({
    rating: z.number().min(1).max(5).optional(),
    sentiment: z.enum(["positive", "negative"]),
    keyPoints: z.array(z.string()).describe("The key points of the review. Lowercase, 1-3 words each."),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy(ProductReview)
})

result = agent.invoke({
    "messages": [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

--------------------------------

### Initialize AWS Bedrock Converse Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Provides an example of initializing an AWS Bedrock chat model using the `initChatModel` function. This requires proper AWS credential configuration, which should be followed from the provided documentation link.

```typescript
import { initChatModel } from "langchain";

// Follow the steps here to configure your credentials:

```

--------------------------------

### Create and Use VectorStoreRetriever

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This example shows how to create a Retriever from a VectorStore using the 'asRetriever' method. It configures the retriever with a specific search type ('mmr') and search arguments. The 'batch' method is then used to invoke the retriever with multiple queries.

```typescript
const retriever = vectorStore.asRetriever({
  searchType: "mmr",
  searchKwargs: {
    fetchK: 1,
  },
});

await retriever.batch([
  "When was Nike incorporated?",
  "What was Nike's revenue in 2023?",
]);
```

--------------------------------

### Initialize Chat Model with Parameters

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to initialize a chat model with specific runtime parameters using the `initChatModel` function. This example sets the `temperature`, `timeout`, and `max_tokens` for the "claude-sonnet-4-5-20250929" model, allowing for fine-tuned response generation.

```typescript
const model = await initChatModel(
    "claude-sonnet-4-5-20250929",
    { temperature: 0.7, timeout: 30, max_tokens: 1000 }
)
```

--------------------------------

### Run the SQL Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Executes a sample query using the created agent. It demonstrates how to stream the agent's responses, logging each step (human input, tool calls, and AI output) to the console. This allows observation of the agent's reasoning and execution process.

```typescript
const question = "Which genre, on average, has the longest tracks?";
const stream = await agent.stream(
  { messages: [{ role: "user", content: question }] },
  { streamMode: "values" }
);
for await (const step of stream) {
  const message = step.messages.at(-1);
  console.log(`${message.role}: ${JSON.stringify(message.content, null, 2)}`);
}

```

--------------------------------

### Initialize Azure Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Provides code examples for initializing an Azure OpenAI chat model via `initChatModel` or `AzureChatOpenAI`. Requires API key, endpoint, and API version, along with the model name (e.g., 'azure_openai:gpt-4.1').

```typescript
import { initChatModel } from "langchain";

process.env.AZURE_OPENAI_API_KEY = "your-api-key";
process.env.AZURE_OPENAI_ENDPOINT = "your-endpoint";
process.env.OPENAI_API_VERSION = "your-api-version";

const model = await initChatModel("azure_openai:gpt-4.1");
```

```typescript
import { AzureChatOpenAI } from "@langchain/openai";

const model = new AzureChatOpenAI({
  model: "gpt-4.1",
  azureOpenAIApiKey: "your-api-key",
  azureOpenAIApiEndpoint: "your-endpoint",
  azureOpenAIApiVersion: "your-api-version"
});
```

--------------------------------

### Test Supervisor Agent with Single-Domain Request in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Demonstrates how to use the supervisor agent to handle a simple, single-domain request. The example shows sending a query to the supervisor agent, streaming the response, and logging the formatted messages. This tests the agent's ability to correctly route a request to the appropriate tool and process its output.

```typescript
const query = "Schedule a team standup for tomorrow at 9am";

const stream = await supervisorAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Create Agent with Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Demonstrates how to create an agent using `create_agent` and include custom or built-in middleware. This example shows passing `summarizationMiddleware` and `humanInTheLoopMiddleware` to the `middleware` option. Ensure LangChain and necessary middleware are imported.

```typescript
import {
  createAgent,
  summarizationMiddleware,
  humanInTheLoopMiddleware,
} from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [summarizationMiddleware, humanInTheLoopMiddleware],
});
```

--------------------------------

### Instantiate and Use AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to instantiate an AIMessage object, which represents the output of a model invocation. AIMessages can contain multimodal data, tool calls, and provider-specific metadata. This example also demonstrates invoking a model and logging the type of the response.

```typescript
const response = await model.invoke("Explain AI");
console.log(typeof response);  // AIMessage
```

--------------------------------

### Production Checkpointer with PostgreSQL (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This code example shows how to configure a production-ready checkpointer for short-term memory persistence using a PostgreSQL database. It utilizes `PostgresSaver` and requires a database connection string.

```typescript
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const checkpointer = PostgresSaver.fromConnString(DB_URI);
```

--------------------------------

### Setup Agentic RAG Chain with Dynamic System Prompt - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Configures an agent to use a dynamic system prompt that incorporates retrieved documents for context. It uses middleware to fetch relevant documents based on the last user query and injects them into the system message before model inference. This approach allows the LLM to decide when to search.

```typescript
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";
import { SystemMessage } from "@langchain/core/messages";

const agent = createAgent({
  model,
  tools: [],
  middleware: [
    dynamicSystemPromptMiddleware(async (state) => {
        const lastQuery = state.messages[state.messages.length - 1].content;

        const retrievedDocs = await vectorStore.similaritySearch(lastQuery, 2);

        const docsContent = retrievedDocs
        .map((doc) => doc.pageContent)
        .join("\n\n");

        // Build system message
        const systemMessage = new SystemMessage(
        `You are a helpful assistant. Use the following context in your response:\n\n${docsContent}`
        );

        // Return system + existing messages
        return [systemMessage, ...state.messages];
    })
  ]
});

```

--------------------------------

### LangChain.js Audio Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to use HumanMessage objects for audio inputs in LangChain.js. Examples cover audio data provided as base64 encoded strings or through provider-managed file IDs. Compatibility depends on the specific chat model's capabilities.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this audio." },
    {
      type: "audio",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this audio." },
    { type: "audio", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Basic Agent with Short-Term Memory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This snippet demonstrates the fundamental setup of an agent with short-term memory using a `MemorySaver` checkpointer. It shows how to invoke the agent with initial messages and a thread ID for persistence.

```typescript
import { createAgent } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const agent = createAgent({
    model: "claude-sonnet-4-5-20250929",
    tools: [],
    checkpointer,
});

await agent.invoke(
    { messages: [{ role: "user", content: "hi! i am Bob" }] },
    { configurable: { thread_id: "1" } }
);
```

--------------------------------

### LangChain.js PDF Document Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to construct HumanMessage objects for PDF document inputs in LangChain.js. This includes examples for providing documents via URL, base64 encoding, or a file ID. Note that specific providers like OpenAI may require additional keys for file handling.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From URL
const messageFromUrl = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this document." },
    { type: "file", source_type: "url", url: "https://example.com/path/to/document.pdf" },
  ],
});

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this document." },
    {
      type: "file",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this document." },
    { type: "file", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Define a Simple Email Sending Agent in Python

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Defines a Python function `send_email` to simulate sending an email and then creates an agent using `langchain.agents.create_agent`. This agent is configured to use the GPT-4o model and is given a system prompt instructing it to always use the `send_email` tool. The agent is intended for use with Studio for local debugging and interaction.

```python
from langchain.agents import create_agent

def send_email(to: str, subject: str, body: str):
    """Send an email"""
    email = {
        "to": to,
        "subject": subject,
        "body": body
    }
    # ... email sending logic

    return f"Email sent to {to}"

agent = create_agent(
    "gpt-4o",
    tools=[send_email],
    system_prompt="You are an email assistant. Always use the send_email tool.",
)
```

--------------------------------

### Initialize Qdrant Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the QdrantVectorStore from an existing collection. Requires the Qdrant service URL and the name of the collection to connect to.

```typescript
import { QdrantVectorStore } from "@langchain/qdrant";

const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {
  url: process.env.QDRANT_URL,
  collectionName: "langchainjs-testing",

```

--------------------------------

### Initialize and Interact with SQLite Database (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Initializes a SqlDatabase wrapper using Langchain from a TypeORM DataSource configured for SQLite. It provides functions to get the database instance and retrieve table schema information. Requires '@langchain/classic/sql_db' and 'typeorm' packages.

```typescript
import { SqlDatabase } from "@langchain/classic/sql_db";
import { DataSource } from "typeorm";

let db: SqlDatabase | undefined;
async function getDb() {
  if (!db) {
    const dbPath = await resolveDbFile();
    const datasource = new DataSource({ type: "sqlite", database: dbPath });
    db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });
  }
  return db;
}

async function getSchema() {
  const db = await getDb();
  return await db.getTableInfo();
}
```

--------------------------------

### Dynamically Select Tools with Middleware in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This example demonstrates how to create a middleware to dynamically select a relevant subset of tools for an agent at runtime. It improves performance and accuracy by reducing the number of tools the model needs to consider. All available tools must be registered upfront, and the middleware filters them based on the request's state and runtime context.

```typescript
import { createAgent, createMiddleware } from "langchain";

const toolSelectorMiddleware = createMiddleware({
  name: "ToolSelector",
  wrapModelCall: (request, handler) => {
    // Select a small, relevant subset of tools based on state/context
    const relevantTools = selectRelevantTools(request.state, request.runtime);
    const modifiedRequest = { ...request, tools: relevantTools };
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: allTools, // All available tools need to be registered upfront
  // Middleware can be used to select a smaller subset that's relevant for the given run.
  middleware: [toolSelectorMiddleware],
});
```

--------------------------------

### Test Supervisor Agent with Multi-Domain Request in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Illustrates how the supervisor agent handles a complex, multi-domain request that requires coordinating actions across different tools. The example sends a query involving both scheduling a meeting and sending an email reminder. It then streams and logs the output, showcasing the agent's capability to sequence and synthesize results from multiple sub-agents.

```typescript
const query =
  "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
  "and send them an email reminder about reviewing the new mockups.";

const stream = await supervisorAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### LangChain Tool Calling with Union Types for Multiple Schemas

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example shows how to configure `toolStrategy` to handle multiple potential output structures by providing an array of Zod schemas. The agent can then return a structured response conforming to either the `ProductReview` schema or the `CustomerComplaint` schema. The code demonstrates defining both schemas and initializing an agent that can process inputs relevant to either structure.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductReview = z.object({
    rating: z.number().min(1).max(5).optional(),
    sentiment: z.enum(["positive", "negative"]),
    keyPoints: z.array(z.string()).describe("The key points of the review. Lowercase, 1-3 words each."),
});

const CustomerComplaint = z.object({
    issueType: z.enum(["product", "service", "shipping", "billing"]),
    severity: z.enum(["low", "medium", "high"]),
    description: z.string().describe("Brief description of the complaint"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy([ProductReview, CustomerComplaint])
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

--------------------------------

### Construct RAG Agent with Retrieval Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This code snippet shows how to construct a RAG agent using Langchain.js. It takes a list of tools (including the previously defined `retrieve` tool) and a `SystemMessage` that guides the agent's behavior. The `createAgent` function is used to instantiate the agent, specifying the model and the system prompt.

```typescript
import { createAgent } from "langchain";
import { SystemMessage } from "@langchain/core/messages";

const tools = [retrieve];
const systemPrompt = new SystemMessage(
    "You have access to a tool that retrieves context from a blog post. " +
    "Use the tool to help answer user queries."
)

const agent = createAgent({ model: "gpt-5", tools, systemPrompt });
```

--------------------------------

### Example Usage Metadata JSON

Source: https://docs.langchain.com/oss/javascript/langchain/messages

A sample JSON object representing the structure of `usage_metadata` that can be found within an AIMessage. It includes details on input, output, and total token counts, along with token details for reasoning and caching.

```json
{
  "output_tokens": 304,
  "input_tokens": 8,
  "total_tokens": 312,
  "input_token_details": {
    "cache_read": 0
  },
  "output_token_details": {
    "reasoning": 256
  }
}
```

--------------------------------

### Agent with Provider Strategy (Zod Schema)

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Example of creating an agent that uses a Zod schema for structured output via the `providerStrategy`. This approach is highly reliable when the model provider supports native structured output.

```typescript
import * as z from "zod";
import { createAgent, providerStrategy } from "langchain";

const ContactInfo = z.object({
    name: z.string().describe("The name of the person"),
    email: z.string().describe("The email address of the person"),
    phone: z.string().describe("The phone number of the person"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: providerStrategy(ContactInfo)
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
});

result.structuredResponse;
// { name: "John Doe", email: "john@example.com", phone: "(555) 123-4567" }
```

--------------------------------

### Setup Two-Step RAG Chain with Source Document Handling - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Illustrates setting up a two-step RAG chain that always performs a search and includes retrieved documents in the application state. This involves defining a state schema that includes a 'context' field for documents and using a pre-model hook middleware to populate this field and inject context into the prompt.

```typescript
import { createMiddleware, Document, createAgent } from "langchain";
import { MessagesZodSchema } from "@langchain/langgraph";

const StateSchema = z.object({
  messages: MessagesZodSchema,
  context: z.array(z.custom<Document>()),
})

const retrieveDocumentsMiddleware = createMiddleware({
  stateSchema: StateSchema,
  beforeModel: async (state) => {

```

--------------------------------

### Similarity Search in Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This example shows how to perform a similarity search on a vector store using a string query. It retrieves documents that are semantically similar to the provided question. The output includes the most similar Document object.

```typescript
const results1 = await vectorStore.similaritySearch(
  "When was Nike incorporated?"
);

console.log(results1[0]);
```

--------------------------------

### Configure LangSmith API Key Environment Variable

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Sets the `LANGSMITH_API_KEY` environment variable in a `.env` file. This API key is essential for authenticating with LangSmith services, which are used by Studio for tracing and debugging agent behavior. It is critical to ensure this file is not committed to version control.

```bash
LANGSMITH_API_KEY=lsv2...
```

--------------------------------

### Invoke a Chat Model

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a chat model to get a response. The `invoke` method takes a string prompt as input and returns the model's generated message. This is a fundamental operation for interacting with chat models.

```typescript
const response = await model.invoke("Why do parrots talk?");
```

--------------------------------

### Combine Multiple Guardrails in Langchain Agent

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

This example demonstrates how to stack multiple middleware functions to create layered protection for a Langchain agent. Guardrails execute in the order they are added to the middleware array, allowing for sequential application of different validation and control mechanisms.

```typescript
import { createAgent, piiRedactionMiddleware, humanInTheLoopMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, sendEmailTool],
  middleware: [
    // Layer 1: Deterministic input filter (before agent)
    contentFilterMiddleware(["hack", "exploit"]),

    // Layer 2: PII protection (before and after model)
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToInput: true,
    }),
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToOutput: true,
    }),

    // Layer 3: Human approval for sensitive tools
    humanInTheLoopMiddleware({
      interruptOn: {
        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },
      }
    }),

    // Layer 4: Model-based safety check (after agent)
    safetyGuardrailMiddleware(),
  ],
});
```

--------------------------------

### Batch requests to a model with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Batching a collection of independent requests to a model can significantly improve performance and reduce costs by allowing parallel processing. This example demonstrates sending multiple prompts to a model and logging the responses.

```typescript
const responses = await model.batch([
  "Why do parrots have colorful feathers?",
  "How do airplanes fly?",
  "What is quantum computing?",
  "Why do parrots have colorful feathers?",
  "How do airplanes fly?",
  "What is quantum computing?",
]);
for (const response of responses) {
  console.log(response);
}
```

--------------------------------

### LangChain.js Schema Validation Error Example

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This snippet demonstrates how LangChain.js provides specific error feedback when structured output, defined by a Zod schema, fails to match the expected format. It shows an example where the model attempts to generate output that violates the schema constraints, and the subsequent tool response indicates the validation error.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductRating = z.object({
    rating: z.number().min(1).max(5).describe("Rating from 1-5"),
    comment: z.string().describe("Review comment"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy(ProductRating),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content: "Parse this: Amazing product, 10/10!",
        },
    ],
});

console.log(result);
```

--------------------------------

### Weather Server Implementation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

A TypeScript example for creating a Weather MCP server using SSE (Server-Sent Events) transport with Express.js. This server defines a 'get_weather' tool and handles requests to provide weather information for a given location. It's suitable for web-based applications.

```typescript
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import express from "express";

const app = express();
app.use(express.json());

const server = new Server(
    {
        name: "weather-server",
        version: "0.1.0",
    },
    {
        capabilities: {
        tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "get_weather",
            description: "Get weather for location",
            inputSchema: {
            type: "object",
            properties: {
                location: {
                type: "string",
                description: "Location to get weather for",
                },
            },
            required: ["location"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "get_weather": {
        const { location } = request.params.arguments as { location: string };
        return {
            content: [
            {
                type: "text",
                text: `It's always sunny in ${location}`,
            },
            ],
        };
        }
        default:
        throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

app.post("/mcp", async (req, res) => {
    const transport = new SSEServerTransport("/mcp", res);
    await server.connect(transport);
});

const PORT = process.env.PORT || 8000;
app.listen(PORT, () => {

```

--------------------------------

### Model Invocation with Configuration

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a model with various configuration options using the `RunnableConfig` object.

```APIDOC
## POST /models/invoke

### Description
Invokes a language model with a given prompt and optional runtime configuration.

### Method
POST

### Endpoint
`/models/invoke`

### Parameters
#### Request Body
- **prompt** (string) - Required - The input prompt for the model.
- **config** (object) - Optional - Configuration object for the invocation.
  - **runName** (string) - Optional - A custom name for this specific run.
  - **tags** (string[]) - Optional - Labels for categorization and filtering.
  - **metadata** (object) - Optional - Custom key-value pairs for additional context.
  - **callbacks** (CallbackHandler[]) - Optional - Handlers for monitoring execution events.
  - **maxConcurrency** (number) - Optional - Maximum parallel calls for batch operations.
  - **recursion_limit** (number) - Optional - Maximum recursion depth for chains.

### Request Example
```json
{
  "prompt": "Tell me a joke",
  "config": {
    "runName": "joke_generation",
    "tags": ["humor", "demo"],
    "metadata": {"user_id": "123"},
    "callbacks": [my_callback_handler]
  }
}
```

### Response
#### Success Response (200)
- **text** (string) - The model's generated response.

#### Response Example
```json
{
  "text": "Why don't scientists trust atoms? Because they make up everything!"
}
```
```

--------------------------------

### LangChain Tool Calling with JSON Schema for Product Review Analysis

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This code snippet illustrates using `toolStrategy` with a JSON schema for structured product review analysis. It defines a `productReviewSchema` in JSON Schema format and then initializes an agent with this schema. The example shows how to invoke the agent and log the resulting structured output, which is a JSON object representing the review's analysis.

```typescript
import { createAgent, toolStrategy } from "langchain";

const productReviewSchema = {
    "type": "object",
    "description": "Analysis of a product review.",
    "properties": {
        "rating": {
            "type": ["integer", "null"],
            "description": "The rating of the product (1-5)",
            "minimum": 1,
            "maximum": 5
        },
        "sentiment": {
            "type": "string",
            "enum": ["positive", "negative"],
            "description": "The sentiment of the review"
        },
        "key_points": {
            "type": "array",
            "items": {"type": "string"},
            "description": "The key points of the review"
        }
    },
    "required": ["sentiment", "key_points"]
}

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy(productReviewSchema)
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

--------------------------------

### Configure Tool Call Limits in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Demonstrates how to set up global and tool-specific limits for tool calls using the `toolCallLimitMiddleware` in Langchain.js. It shows examples of setting thread limits, run limits, and different exit behaviors for agent execution.

```typescript
import { createAgent, toolCallLimitMiddleware } from "langchain";

// Global limit: max 20 calls per thread, 10 per run
const globalLimiter = toolCallLimitMiddleware({
  threadLimit: 20,
  runLimit: 10,
});

// Tool-specific limit with default "continue" behavior
const searchLimiter = toolCallLimitMiddleware({
  toolName: "search",
  threadLimit: 5,
  runLimit: 3,
});

// Thread limit only (no per-run limit)
const databaseLimiter = toolCallLimitMiddleware({
  toolName: "query_database",
  threadLimit: 10,
});

// Strict enforcement with "error" behavior
const webScraperLimiter = toolCallLimitMiddleware({
  toolName: "scrape_webpage",
  runLimit: 2,
  exitBehavior: "error",
});

// Immediate termination with "end" behavior
const criticalToolLimiter = toolCallLimitMiddleware({
  toolName: "delete_records",
  runLimit: 1,
  exitBehavior: "end",
});

// Use multiple limiters together
const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, databaseTool, scraperTool],
  middleware: [globalLimiter, searchLimiter, databaseLimiter, webScraperLimiter],
});
```

--------------------------------

### Streaming Tool Calls in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Explains how to stream responses from a model that supports tool calls. It demonstrates receiving `ToolCallChunk` objects as they are generated, allowing for progressive display of tool call information. The example also shows how to accumulate these chunks into complete tool calls.

```typescript
const stream = await modelWithTools.stream(
    "What's the weather in Boston and Tokyo?"
)
for await (const chunk of stream) {
    // Tool call chunks arrive progressively
    if (chunk.tool_call_chunks) {
        for (const tool_chunk of chunk.tool_call_chunks) {
        console.log(`Tool: ${tool_chunk.get('name', '')}`)
        console.log(`Args: ${tool_chunk.get('args', '')}`)
        }
    }
}

// Output:
// Tool: get_weather
// Args:
// Tool:
// Args: {"loc
// Tool:
// Args: ation": "BOS"}
// Tool: get_time
// Args:
// Tool:
// Args: {"timezone": "Tokyo"}

```

```typescript
let full: AIMessageChunk | null = null
const stream = await modelWithTools.stream("What's the weather in Boston?")
for await (const chunk of stream) {
    full = full ? full.concat(chunk) : chunk
    console.log(full.contentBlocks)
}

```

--------------------------------

### Stream Agent Progress with Updates Mode in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

This example demonstrates how to stream agent progress using the `stream` method with `streamMode: 'updates'`. It shows how to capture state updates after each agent step, including LLM node, Tool node, and final AI response. Dependencies include `zod` for schema definition and `langchain` for agent and tool creation.

```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "updates" }
)) {
    const [step, content] = Object.entries(chunk)[0];
    console.log(`step: ${step}`);
    console.log(`content: ${JSON.stringify(content, null, 2)}`);
}
/**
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         // ...
 *         "tool_calls": [
 *           {
 *             "name": "get_weather",
 *             "args": {
 *               "city": "San Francisco"
 *             },
 *             "type": "tool_call",
 *             "id": "call_0qLS2Jp3MCmaKJ5MAYtr4jJd"
 *           }
 *         ],
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: tools
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The weather in San Francisco is always sunny!",
 *         "name": "get_weather",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The latest update says: The weather in San Francisco is always sunny!\n\nIf you'd like real-time details (current temperature, humidity, wind, and today's forecast), I can pull the latest data for you. Want me to fetch that?",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 */
```

--------------------------------

### Initialize Google Gemini Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Demonstrates initializing a Google Gemini chat model using `initChatModel` or `ChatGoogleGenerativeAI`. Requires the Google API key and specifies the model (e.g., 'google-genai:gemini-2.5-flash-lite').

```typescript
import { initChatModel } from "langchain";

process.env.GOOGLE_API_KEY = "your-api-key";

const model = await initChatModel("google-genai:gemini-2.5-flash-lite");
```

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const model = new ChatGoogleGenerativeAI({
  model: "gemini-2.5-flash-lite",
  apiKey: "your-api-key"
});
```

--------------------------------

### Text Content Block Example (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates the structure of a standard text content block. It includes the 'type' property, which must be 'text', the 'text' content, and an optional 'annotations' array for associated metadata.

```typescript
{
    type: "text",
    text: "Hello world",
    annotations: []
}
```

--------------------------------

### Define a Tool for Searching Orders in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Defines a tool named 'search_orders' using Langchain.js and Zod for schema validation. This tool searches user orders based on provided status and limit, guiding the model's reasoning for order-related queries.

```typescript
import {
  tool
} from "@langchain/core/tools";
import {
  z
} from "zod";

const searchOrders = tool(
  async ({ userId, status, limit = 10 }) => {
    // Implementation here
  },
  {
    name: "search_orders",
    description: `Search for user orders by status.

    Use this when the user asks about order history or wants to check
    order status. Always filter by the provided status.`,
    schema: z.object({
      userId: z.string().describe("Unique identifier for the user"),
      status: z.enum(["pending", "shipped", "delivered"]).describe("Order status to filter by"),
      limit: z.number().default(10).describe("Maximum number of results to return"),
    }),
  }
);

```

--------------------------------

### Streaming Agent Responses in LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/agents

This example shows how to stream agent responses incrementally instead of waiting for a final output. It iterates over the streamed chunks, logs agent messages, and identifies tool calls as they occur, providing real-time feedback to the user.

```typescript
const stream = await agent.stream(
  {
    messages: [{
      role: "user",
      content: "Search for AI news and summarize the findings"
    }],
  },
  { streamMode: "values" }
);

for await (const chunk of stream) {
  // Each chunk contains the full state at that point
  const latestMessage = chunk.messages.at(-1);
  if (latestMessage?.content) {
    console.log(`Agent: ${latestMessage.content}`);
  } else if (latestMessage?.tool_calls) {
    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);
    console.log(`Calling tools: ${toolCallNames.join(", ")}`);
  }
}
```

--------------------------------

### Define Fetch URL Tool and Create Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/retrieval

This snippet demonstrates how to define a simple tool for fetching content from a URL and then create an agent using this tool. The agent is configured with a model, the defined tool, and a system prompt.

```typescript
import { tool, createAgent } from "langchain";

const fetchUrl = tool(
    (url: string) => {
        return `Fetched content from ${url}`;
    },
    { name: "fetch_url", description: "Fetch text content from a URL" }
);

const agent = createAgent({
    model: "claude-sonnet-4-0",
    tools: [fetchUrl],
    systemPrompt,
});
```

--------------------------------

### Parallel Tool Calls in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Illustrates how to invoke a model that can call multiple tools in parallel. It shows how to bind tools, invoke the model with a multi-part query, and process the resulting tool calls. The example also includes how to disable parallel calls.

```typescript
const modelWithTools = model.bind_tools([get_weather])

const response = await modelWithTools.invoke(
    "What's the weather in Boston and Tokyo?"
)


// The model may generate multiple tool calls
console.log(response.tool_calls)
// [
//   { name: 'get_weather', args: { location: 'Boston' }, id: 'call_1' },
//   { name: 'get_time', args: { location: 'Tokyo' }, id: 'call_2' } 
// ]


// Execute all tools (can be done in parallel with async)
const results = []
for (const tool_call of response.tool_calls || []) {
    if (tool_call.name === 'get_weather') {
        const result = await get_weather.invoke(tool_call)
        results.push(result)
    }
}

```

```python
model.bind_tools([get_weather], parallel_tool_calls=False)
```

--------------------------------

### Reasoning Content Block Example (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates the format for a reasoning content block, used to represent model reasoning steps. It requires a 'type' property set to 'reasoning' and a 'reasoning' string detailing the thought process.

```typescript
{
    type: "reasoning",
    reasoning: "The user is asking about..."
}
```

--------------------------------

### Pass Conversational Context to Sub-Agent in LangGraphJS

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript example demonstrates how to pass additional conversational context to sub-agents within LangGraphJS. It utilizes `getCurrentTaskInput` to access the message history and constructs a more detailed prompt for the `calendarAgent`.

```typescript
import { getCurrentTaskInput } from "@langchain/langgraph";
import type { InternalAgentState } from "langchain";
import { HumanMessage } from "@langchain/core/messages";

const scheduleEvent = tool(
  async ({ request }, config) => {
    // Customize context received by sub-agent
    // Access full thread messages from the config
    const currentMessages = getCurrentTaskInput<InternalAgentState>(config).messages;

    const originalUserMessage = currentMessages.find(HumanMessage.isInstance);

    const prompt = `
    You are assisting with the following user inquiry:

    ${originalUserMessage?.content || "No context available"}

    You are tasked with the following sub-request:

    ${request}
        `.trim();

    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: prompt }],
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### LangChain.js Image Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to create HumanMessage objects with image content for LangChain chat models. Supports images from URLs, base64 encoded data, and provider-managed file IDs. Ensure the model supports image inputs.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From URL
const messageFromUrl = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this image." },
    {
      type: "image",
      source_type: "url",
      url: "https://example.com/path/to/image.jpg"
    },
  ],
});

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this image." },
    {
      type: "image",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this image." },
    { type: "image", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Generate Structured Output with Zod Schema in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet demonstrates how to define an output schema using Zod and then use it with a LangChain model to get structured output. The output is automatically validated against the Zod schema. It requires the 'zod' library.

```typescript
import * as z from "zod";

const Movie = z.object({
  title: z.string().describe("The title of the movie"),
  year: z.number().describe("The year the movie was released"),
  director: z.string().describe("The director of the movie"),
  rating: z.number().describe("The movie's rating out of 10"),
});

const modelWithStructure = model.withStructuredOutput(Movie);

const response = await modelWithStructure.invoke("Provide details about the movie Inception");
console.log(response);
// {
//   title: "Inception",
//   year: 2010,
//   director: "Christopher Nolan",
//   rating: 8.8,
// }
```

--------------------------------

### Initializing Chat Model with Standard Content Serialization (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Provides an example of initializing a chat model in TypeScript with the `outputVersion: "v1"` option to ensure that message content is serialized using LangChain's standard content block format for external applications.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel(
  "gpt-5-nano",
  { outputVersion: "v1" }
);

// Alternatively, set the environment variable LC_OUTPUT_VERSION = "v1"
```

--------------------------------

### Constructing Human Messages with Different Content Types (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to create `HumanMessage` objects in TypeScript with different content formats: a simple string, a provider-native format (OpenAI example with text and image URL), and a list of LangChain's standard content blocks.

```typescript
import { HumanMessage } from "langchain/dist/messages/ai.js";

// String content
const humanMessageFromString = new HumanMessage("Hello, how are you?");

// Provider-native format (e.g., OpenAI)
const humanMessageFromProviderNative = new HumanMessage({
  content: [
    { type: "text", text: "Hello, how are you?" },
    {
      type: "image_url",
      image_url: { url: "https://example.com/image.jpg" },
    },
  ],
});

// List of standard content blocks
const humanMessageFromStandardBlocks = new HumanMessage({
  contentBlocks: [
    { type: "text", text: "Hello, how are you?" },
    { type: "image", url: "https://example.com/image.jpg" },
  ],
});
```

--------------------------------

### Intelligent Tool Selection with LLM Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This example shows how to configure an agent to use `llmToolSelectorMiddleware` for intelligent tool selection. It's beneficial when an agent has many tools, helping to reduce token usage and improve model focus by filtering irrelevant tools. The middleware allows specifying a cheaper model for selection, limiting the number of tools, and always including specific tools.

```typescript
import { createAgent, llmToolSelectorMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [tool1, tool2, tool3, tool4, tool5, ...], // Many tools
  middleware: [
    llmToolSelectorMiddleware({
      model: "gpt-4o-mini", // Use cheaper model for selection
      maxTools: 3, // Limit to 3 most relevant tools
      alwaysInclude: ["search"], // Always include certain tools
    }),
  ],
});

```

--------------------------------

### Get Raw AI Message with Structured Output in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet demonstrates how to retrieve both the parsed structured output and the raw AIMessage object, which includes metadata like token counts. This is achieved by setting `includeRaw: true` when calling `withStructuredOutput`.

```typescript
import * as z from "zod";

const Movie = z.object({
  title: z.string().describe("The title of the movie"),
  year: z.number().describe("The year the movie was released"),
  director: z.string().describe("The director of the movie"),
  rating: z.number().describe("The movie's rating out of 10"),
});

const modelWithStructure = model.withStructuredOutput(Movie, { includeRaw: true });

const response = await modelWithStructure.invoke("Provide details about the movie Inception");
console.log(response);
// {
//   raw: AIMessage { ... },
//   parsed: { title: "Inception", ... }
// }
```

--------------------------------

### Get User Preferences from Store in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This tool retrieves user preferences from the store using a provided preference key. It accesses the user ID from the runtime context and then queries the store for persisted preferences. This allows tools to personalize responses based on user settings.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const getPreference = tool(
  async ({ preferenceKey }, { runtime }) => {
    const userId = runtime.context.userId;

    // Read from Store: get existing preferences
    const store = runtime.store;
    const existingPrefs = await store.get(["preferences"], userId);

    if (existingPrefs) {
      const value = existingPrefs.value?.[preferenceKey];
      return value ? `${preferenceKey}: ${value}` : `No preference set for ${preferenceKey}`;
    } else {
      return "No preferences found";
    }
  },
  {
    name: "get_preference",
    description: "Get user preference from Store",
    schema: z.object({
      preferenceKey: z.string(),
    }),
  }
);

```

--------------------------------

### Define Customer Support Ticket Schema with Zod

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Defines a structured schema for customer support tickets using Zod. It specifies fields for category, priority, summary, and customer sentiment, along with their types and descriptions. This schema guides models to output data in a predictable format.

```typescript
import { z } from "zod";

const customerSupportTicket = z.object({
  category: z.enum(["billing", "technical", "account", "product"]).describe(
    "Issue category"
  ),
  priority: z.enum(["low", "medium", "high", "critical"]).describe(
    "Urgency level"
  ),
  summary: z.string().describe(
    "One-sentence summary of the customer's issue"
  ),
  customerSentiment: z.enum(["frustrated", "neutral", "satisfied"]).describe(
    "Customer's emotional tone"
  ),
}).describe("Structured ticket information extracted from customer message");
```

--------------------------------

### Define a Tool to Get Available Time Slots (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines a Langchain tool named 'get_available_time_slots' using Zod for schema validation. This tool is a stub that simulates checking calendar availability for given attendees on a specific date and returns available time slots.

```typescript
import { tool } from "langchain";
import { z } from "zod";

const getAvailableTimeSlots = tool(
  async ({ attendees, date, durationMinutes }) => {
    // Stub: In practice, this would query calendar APIs
    return ["09:00", "14:00", "16:00"];
  },
  {
    name: "get_available_time_slots",
    description: "Check calendar availability for given attendees on a specific date.",
    schema: z.object({
      attendees: z.array(z.string()),
      date: z.string().describe("ISO format: '2024-01-15'"),
      durationMinutes: z.number(),
    }),
  }
);

```

--------------------------------

### Initialize OpenAI Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Demonstrates two methods to initialize an OpenAI chat model in Langchain.js: using the `initChatModel` utility function and instantiating the `ChatOpenAI` class directly. Both methods require setting the OpenAI API key and specify the desired model (e.g., 'gpt-4.1').

```typescript
import { initChatModel } from "langchain";

process.env.OPENAI_API_KEY = "your-api-key";

const model = await initChatModel("gpt-4.1");
```

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4.1",
  apiKey: "your-api-key"
});
```

--------------------------------

### Retrieve Complete Reasoning Output from LangChain.js Models

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a LangChain.js model to get a complete response and then extract all reasoning steps from its content blocks. This method collects all 'reasoning' type content blocks and joins their 'reasoning' property to form a single string output, providing a consolidated view of the model's thought process.

```typescript
const response = await model.invoke("Why do parrots have colorful feathers?");
const reasoningSteps = response.contentBlocks.filter(b => b.type === "reasoning");
console.log(reasoningSteps.map(step => step.reasoning).join(" "));
```

--------------------------------

### Summarization Middleware Setup in LangChain (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This snippet demonstrates how to integrate the `summarizationMiddleware` into a LangChain agent. The middleware automatically summarizes older messages when the conversation exceeds a token limit, replacing them with a summary in the state. It requires specifying the summarization model, a token threshold for triggering summarization, and the number of recent messages to retain.

```typescript
import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      maxTokensBeforeSummary: 4000, // Trigger summarization at 4000 tokens
      messagesToKeep: 20, // Keep last 20 messages after summary
    }),
  ],
});
```

--------------------------------

### Configure MistralAI API Key

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the MISTRAL_API_KEY environment variable for authenticating with the MistralAI API.

```bash
MISTRAL_API_KEY=your-api-key
```

--------------------------------

### Create Supervisor Agent in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines and creates a supervisor agent using Langchain.js. This agent is configured with a language model, a set of high-level tools (scheduling and email management), and a system prompt to guide its decision-making process. It acts as the central orchestrator for handling user requests that may involve multiple sub-agent actions.

```typescript
const SUPERVISOR_PROMPT = `
You are a helpful personal assistant.
You can schedule calendar events and send emails.
Break down user requests into appropriate tool calls and coordinate the results.
When a request involves multiple actions, use multiple tools in sequence.
`.trim();

const supervisorAgent = createAgent({
  model: llm,
  tools: [scheduleEvent, manageEmail],
  systemPrompt: SUPERVISOR_PROMPT,
});
```

--------------------------------

### Index Data and Create Retrieval Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This snippet loads content from a URL using Cheerio, splits it into manageable chunks using RecursiveCharacterTextSplitter, indexes these chunks into a vector store, and defines a retrieval tool. The tool searches the vector store for relevant documents based on a query and returns their serialized content and the documents themselves. Dependencies include cheerio, langchain, and @langchain/community.

```typescript
import "cheerio";
import { createAgent, tool } from "langchain";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import * as z from "zod";

// Load and chunk contents of blog
const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector
  }
);

const docs = await cheerioLoader.load();

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);

// Index chunks
await vectorStore.addDocuments(allSplits)

// Construct a tool for retrieving context
const retrieveSchema = z.object({ query: z.string() });

const retrieve = tool(
  async ({ query }) => {
    const retrievedDocs = await vectorStore.similaritySearch(query, 2);
    const serialized = retrievedDocs
      .map(
        (doc) => `Source: ${doc.metadata.source}\nContent: ${doc.pageContent}`
      )
      .join("\n");
    return [serialized, retrievedDocs];
  },
  {
    name: "retrieve",
    description: "Retrieve information related to a query.",
    schema: retrieveSchema,
    responseFormat: "content_and_artifact",
  }
);

const agent = createAgent({ model: "gpt-5", tools: [retrieve] });

```

--------------------------------

### Create Custom Content Filter Middleware for LangChain Agents

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

Shows how to build a custom middleware function in LangChain that intercepts agent requests before execution. This example implements a content filter to block requests containing specific banned keywords, returning an AIMessage and halting further processing.

```typescript
import { createMiddleware, AIMessage } from "langchain";

const contentFilterMiddleware = (bannedKeywords: string[]) => {
  const keywords = bannedKeywords.map(kw => kw.toLowerCase());

  return createMiddleware({
    name: "ContentFilterMiddleware",
    beforeAgent: (state) => {
      // Get the first user message
      if (!state.messages || state.messages.length === 0) {
        return;
      }

      const firstMessage = state.messages[0];
      if (firstMessage._getType() !== "human") {
        return;
      }

      const content = firstMessage.content.toString().toLowerCase();

      // Check for banned keywords
      for (const keyword of keywords) {
        if (content.includes(keyword)) {
          // Block execution before any processing
          return {
            messages: [
              new AIMessage(
                "I cannot process requests containing inappropriate content. Please rephrase your request."
              )
            ],
            jumpTo: "end",
          };
        }
      }

      return;
    },
  });
};

// Use the custom guardrail
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, calculatorTool],
  middleware: [
    contentFilterMiddleware(["hack", "exploit", "malware"]),
  ],
});

// This request will be blocked before any processing
const result = await agent.invoke({
  messages: [{ role: "user", content: "How do I hack into a database?" }]
});

```

--------------------------------

### LangSmith Vitest/Jest Integration for Trajectory Accuracy Evaluation

Source: https://docs.langchain.com/oss/javascript/langchain/test

Integrate LangSmith with Vitest or Jest to run evaluations for trajectory accuracy. This approach uses specific LangSmith modules for test descriptions and assertions, logging outputs directly to LangSmith. It requires installing `langsmith/vitest` or `langsmith/jest` and `agentevals`.

```typescript
import * as ls from "langsmith/vitest";
// import * as ls from "langsmith/jest";

import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";

const trajectoryEvaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT,
});

ls.describe("trajectory accuracy", () => {
  ls.test("accurate trajectory", {
    inputs: {
      messages: [
        {
          role: "user",
          content: "What is the weather in SF?"
        }
      ]
    },
    referenceOutputs: {
      messages: [
        new HumanMessage("What is the weather in SF?"),
        new AIMessage({
          content: "",
          tool_calls: [
            { id: "call_1", name: "get_weather", args: { city: "SF" } }
          ]
        }),
        new ToolMessage({
          content: "It's 75 degrees and sunny in SF.",
          tool_call_id: "call_1"
        }),
        new AIMessage("The weather in SF is 75 degrees and sunny."),
      ],
    },
  }, async ({ inputs, referenceOutputs }) => {
    const result = await agent.invoke({
      messages: [new HumanMessage("What is the weather in SF?")]
    });

    ls.logOutputs({ messages: result.messages });

    await trajectoryEvaluator({
      inputs,
      outputs: result.messages,
      referenceOutputs,
    });
  });
});
```

--------------------------------

### Initialize FAISS Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the FaissStore with embeddings. FAISS is optimized for large-scale similarity search.

```typescript
import { FaissStore } from "@langchain/community/vectorstores/faiss";

const vectorStore = new FaissStore(embeddings, {});
```

--------------------------------

### Reject Tool Call with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Employ the 'reject' decision type to prevent a tool call from executing and provide feedback. The 'message' field in the decision object contains the explanation for rejection, which is added to the conversation history to guide the agent.

```typescript
await agent.invoke(
    new Command({
        // Decisions are provided as a list, one per action under review.
        // The order of decisions must match the order of actions
        // listed in the `__interrupt__` request.
        resume: {
            decisions: [
                {
                    type: "reject",
                    // An explanation about why the action was rejected
                    message: "No, this is wrong because ..., instead do this ...",
                }
            ]
        }
    }),
    config  // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Constructing AIMessage from Streamed Chunks

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to aggregate streamed AIMessageChunk objects into a single AIMessage. The code iterates through the stream, concatenating each chunk to a running `full` message. This allows the complete response to be treated as a single message, suitable for conversational history or further processing. The example logs the accumulating text and the final content blocks.

```typescript
let full: AIMessageChunk | null = null;
for await (const chunk of stream) {
  full = full ? full.concat(chunk) : chunk;
  console.log(full.text);
}

console.log(full.contentBlocks);
// [{"type": "text", "text": "The sky is typically blue..."}]
```

--------------------------------

### Initialize Pinecone Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the PineconeStore, connecting to a specified Pinecone index. Requires Pinecone API key and index name, along with optional configurations like `maxConcurrency`.

```typescript
import { PineconeStore } from "@langchain/pinecone";
import { Pinecone as PineconeClient } from "@pinecone-database/pinecone";

const pinecone = new PineconeClient({
  apiKey: process.env.PINECONE_API_KEY,
});
const pineconeIndex = pinecone.Index("your-index-name");

const vectorStore = new PineconeStore(embeddings, {
  pineconeIndex,
  maxConcurrency: 5,
});
```

--------------------------------

### LangChain.js Handle Specific Exceptions for Structured Output Errors

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example shows how to handle specific exceptions, such as `ToolInputParsingException`, when dealing with structured output errors. The `handleError` function can selectively return custom messages for certain error types while letting others propagate, allowing for fine-grained control over error responses.

```typescript
import { ToolInputParsingException } from "@langchain/core/tools";

const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        return error.message;
    }
)

// Only validation errors get retried with default message:
// { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': ...\n Please fix your mistakes." }
```

--------------------------------

### Access Short-Term Memory in Middleware for Dynamic Prompts (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example shows how to leverage short-term memory within middleware to construct dynamic prompts. It defines a context schema, a `getWeather` tool, and an agent. The agent's `prompt` function utilizes the `context.userName` to personalize the system message, addressing the user by name. The agent is then invoked with user messages and specific context, demonstrating how the prompt is dynamically generated based on the provided context.

```typescript
import * as z from "zod";
import { createAgent, tool, SystemMessage } from "langchain";

const contextSchema = z.object({
    userName: z.string(),
});

const getWeather = tool(
    async ({ city }, config) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get user info",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getWeather],
    contextSchema,
    prompt: (state, config) => {
        return [
        new SystemMessage(
            `You are a helpful assistant. Address the user as ${config.context?.userName}.`
        ),
        ...state.messages,
    };
});

const result = await agent.invoke(
    {
        messages: [{ role: "user", content: "What is the weather in SF?" }],
    },
    {
        context: {
        userName: "John Smith",
        },
    }
);

for (const message of result.messages) {
    console.log(message);
}
/**
 * HumanMessage {
 *   "content": "What is the weather in SF?",
 *   // ...
 * }
 * AIMessage {
 *   // ...
 *   "tool_calls": [
 *     {
 *       "name": "get_weather",
 *       "args": {
 *         "city": "San Francisco"
 *       },
 *       "type": "tool_call",
 *       "id": "call_tCidbv0apTpQpEWb3O2zQ4Yx"
 *     }
 *   ],
 *   // ...
 * }
 * ToolMessage {
 *   "content": "The weather in San Francisco is always sunny!",
 *   "tool_call_id": "call_tCidbv0apTpQpEWb3O2zQ4Yx"
 *   // ...
 * }
 * AIMessage {
 *   "content": "John Smith, here's the latest: The weather in San Francisco is always sunny!\n\nIf you'd like more details (temperature, wind, humidity) or a forecast for the next few days, I can pull that up. What would you like?",
 *   // ...
 * }
 */
```

--------------------------------

### Pass Conversational Context to Sub-agents in LangGraph.js

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript example shows how to enrich sub-agents with additional conversational context beyond the immediate request. It utilizes `getCurrentTaskInput` to access the full message history and constructs a more informative prompt for the `calendarAgent`, enabling it to understand historical context for tasks like scheduling. Dependencies include `@langchain/langgraph` and `@langchain/core/messages`.

```typescript
import { getCurrentTaskInput } from "@langchain/langgraph";
import type { InternalAgentState } from "langchain";
import { HumanMessage } from "@langchain/core/messages";

const scheduleEvent = tool(
  async ({ request }, config) => {
    // Customize context received by sub-agent
    // Access full thread messages from the config
    const currentMessages = getCurrentTaskInput<InternalAgentState>(config).messages;

    const originalUserMessage = currentMessages.find(HumanMessage.isInstance);

    const prompt = `
    You are assisting with the following user inquiry:

    ${originalUserMessage?.content || "No context available"}

    You are tasked with the following sub-request:

    ${request}
    `.trim();

    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: prompt }],
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### Run Agent and Handle Human Review Interruption (Python)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

This code demonstrates how to run the configured agent and handle the interruption that occurs when the `HumanInTheLoopMiddleware` pauses execution. It prints the pending tool request details when an interruption occurs.

```python
question = "Which genre on average has the longest tracks?"
config = {"configurable": {"thread_id": "1"}}

for step in agent.stream(
    {"messages": [{"role": "user", "content": question}]},
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        print("INTERRUPTED:")
        interrupt = step["__interrupt__"][0]
        for request in interrupt.value["action_requests"]:
            print(request["description"])
    else:
        pass
```

--------------------------------

### Langgraph Configuration File

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Defines the configuration for a Langgraph agent, specifying dependencies, graph entry points, and environment file. This JSON file is essential for initializing and running the agent.

```json
{
  "dependencies": [""],
  "graphs": {
      "agent": "./sqlAgent.ts:agent",
      "graph": "./sqlAgentLanggraph.ts:graph"
  },
  "env": ".env"
}
```

--------------------------------

### Integrate Message Deletion into Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example shows how to integrate the `deleteMessages` function as a `postModelHook` within a LangGraph agent. It defines an agent using `createAgent` and configures it to use `MemorySaver` as a checkpointer. The `deleteMessages` function is called after the model generates a response to manage the message history.

```typescript
import { RemoveMessage } from "@langchain/core/messages";
import { AgentState, createAgent } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const deleteMessages = (state: AgentState) => {
    const messages = state.messages;
    if (messages.length > 2) {
        // remove the earliest two messages
        return {
        messages: messages
            .slice(0, 2)
            .map((m) => new RemoveMessage({ id: m.id! })),
        };
    }
    return {};
};

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [],
    prompt: "Please be concise and to the point.",
    postModelHook: deleteMessages,
    checkpointer: new MemorySaver(),
});

const config = { configurable: { thread_id: "1" } };

const streamA = await agent.stream(
    { messages: [{ role: "user", content: "hi! I'm bob" }] },
    { ...config, streamMode: "values" }
);
for await (const event of streamA) {
    const messageDetails = event.messages.map((message) => [
        message.getType(),
        message.content,
    ]);
    console.log(messageDetails);
}

const streamB = await agent.stream(
    {
        messages: [{ role: "user", content: "what's my name?" }],
    },
    { ...config, streamMode: "values" }
);
for await (const event of streamB) {
    const messageDetails = event.messages.map((message) => [
        message.getType(),
        message.content,
    ]);
    console.log(messageDetails);
}
```

--------------------------------

### Define a Basic Tool with Zod Schema - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Demonstrates how to define a basic tool using the `tool` function from the `langchain` package and `zod` for input schema definition. This tool, `searchDatabase`, takes a query and a limit as input.

```typescript
import * as z from "zod"
import { tool } from "langchain"

const searchDatabase = tool(
  ({ query, limit }) => `Found ${limit} results for '${query}'`,
  {
    name: "search_database",
    description: "Search the customer database for records matching the query.",
    schema: z.object({
      query: z.string().describe("Search terms to look for"),
      limit: z.number().describe("Maximum number of results to return"),
    }),
  }
);
```

--------------------------------

### Modify Agent Short-Term Memory with Tools (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example demonstrates how to update an agent's short-term memory by returning state updates from a tool. It defines a custom state schema with messages and a userName, and then creates two tools: `updateUserInfo` to fetch and set user information, and `greet` to use that information. The `createAgent` function is used to build the agent, which is then invoked with initial messages and context. The `updateUserInfo` tool returns a `Command` object containing `update` instructions to modify the agent's state, including the `userName` and the message history.

```typescript
import * as z from "zod";
import { tool, createAgent } from "langchain";
import { MessagesZodState, Command } from "@langchain/langgraph";

const CustomState = z.object({
    messages: MessagesZodState.shape.messages,
    userName: z.string().optional(),
});

const updateUserInfo = tool(
    async (_, config) => {
        const userId = config.context?.userId;
        const name = userId === "user_123" ? "John Smith" : "Unknown user";
        return new Command({
        update: {
            userName: name,
            // update the message history
            messages: [
            {
                role: "tool",
                content: "Successfully looked up user information",
                tool_call_id: config.toolCall?.id,
            },
            ],
        },
        });
    },
    {
        name: "update_user_info",
        description: "Look up and update user info.",
        schema: z.object({}),
    }
);

const greet = tool(
    async (_, config) => {
        const userName = config.context?.userName;
        return `Hello ${userName}!`;
    },
    {
        name: "greet",
        description: "Use this to greet the user once you found their info.",
        schema: z.object({}),
    }
);

const agent = createAgent({
    model, // Assuming 'model' is defined elsewhere
    tools: [updateUserInfo, greet],
    stateSchema: CustomState,
});

await agent.invoke(
    { messages: [{ role: "user", content: "greet the user" }] },
    { context: { userId: "user_123" } }
);
```

--------------------------------

### LangChain JavaScript Error Handling for Multiple Structured Outputs

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Illustrates LangChain's automatic error handling for scenarios where a model incorrectly generates multiple structured outputs. The agent identifies the error, provides feedback in a `ToolMessage`, and prompts the model to retry, ensuring robust structured data extraction. This example uses 'zod' for defining schemas and 'langchain' for agent creation.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ContactInfo = z.object({
    name: z.string().describe("Person's name"),
    email: z.string().describe("Email address"),
});

const EventDetails = z.object({
    event_name: z.string().describe("Name of the event"),
    date: z.string().describe("Event date"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy([ContactInfo, EventDetails]),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content:
            "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th",
        },
    ],
});

console.log(result);

/**
 * {
 *   messages: [
 *     { role: "user", content: "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_1" }, { name: "EventDetails", args: { event_name: "Tech Conference", date: "March 15th" }, id: "call_2" } ] },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_1", name: "ContactInfo" },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_2", name: "EventDetails" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_3" } ] },
 *     { role: "tool", content: "Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}", tool_call_id: "call_3", name: "ContactInfo" }
 *   ],
 *   structuredResponse: { name: "John Doe", email: "john@email.com" }
 * }
 */
```

--------------------------------

### Custom State Schema Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Demonstrates how to extend an agent's state with custom properties using middleware. This example defines a `stateSchema` using Zod for `modelCallCount` and `userId`, and uses `beforeModel` and `afterModel` hooks to interact with these custom state properties. TypeScript enforces the presence of these properties when invoking the agent.

```typescript
import { createMiddleware, createAgent, HumanMessage } from "langchain";
import * as z from "zod";

// Middleware with custom state requirements
const callCounterMiddleware = createMiddleware({
  name: "CallCounterMiddleware",
  stateSchema: z.object({
    modelCallCount: z.number().default(0),
    userId: z.string().optional(),
  }),
  beforeModel: (state) => {
    // Access custom state properties
    if (state.modelCallCount > 10) {
      return { jumpTo: "end" };
    }
    return;
  },
  afterModel: (state) => {
    // Update custom state
    return { modelCallCount: state.modelCallCount + 1 };
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [callCounterMiddleware] as const,
});

// TypeScript enforces required state properties
const result = await agent.invoke({
  messages: [new HumanMessage("Hello")],
  modelCallCount: 0, // Optional due to default value
  userId: "user-123", // Optional
});
```

--------------------------------

### Execute Agent with User Query (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This snippet demonstrates how to execute the previously created LangChain.js agent with a user's input query. It initializes an agent input object with a user message and then streams the agent's execution steps, printing each message from the agent's response. This allows for observing the agent's reasoning process and final output.

```typescript
let inputMessage = `What is Task Decomposition?`;

let agentInputs = { messages: [{ role: "user", content: inputMessage }] };

for await (const step of await agent.stream(agentInputs, {
  streamMode: "values",
})) {
  const lastMessage = step.messages[step.messages.length - 1];
  prettyPrint(lastMessage);
  console.log("-----\n");
}

```

--------------------------------

### LangChain.js Disable Error Handling for Structured Output

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example shows how to completely disable error handling for structured output by setting `handleError` to `false`. When error handling is disabled, all errors will be raised directly, providing the raw error information without any intermediary processing or custom messages.

```typescript
const responseFormat = toolStrategy(ProductRating, {
    handleError: false  // All errors raised
)
```

--------------------------------

### LangChain.js Custom Error Message for Structured Output

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This code illustrates how to provide a custom error message when structured output fails validation. By passing a string to the `handleError` option within `toolStrategy`, you can define a user-friendly message that replaces the default error feedback, guiding the user on how to correct their input.

```typescript
const responseFormat = toolStrategy(ProductRating, {
    handleError: "Please provide a valid rating between 1-5 and include a comment."
})

// Error message becomes:
// { role: "tool", content: "Please provide a valid rating between 1-5 and include a comment." }
```

--------------------------------

### Summarize Agent Messages with Summarization Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This code demonstrates how to use `summarizationMiddleware` to automatically summarize the message history of a Langchain agent. It configures the middleware to use a specific model (`gpt-4o-mini`), set a token threshold for summarization (`maxTokensBeforeSummary`), and define the number of messages to keep (`messagesToKeep`). The example invokes the agent multiple times and then retrieves the final response, showing how summarization helps manage conversation context. Dependencies include `langchain` and `@langchain/langgraph`.

```typescript
import { createAgent, summarizationMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const agent = createAgent({
  model: "gpt-4o",
  tools: [],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      maxTokensBeforeSummary: 4000,
      messagesToKeep: 20,
    }),
  ],
  checkpointer,
});

const config = { configurable: { thread_id: "1" } };
await agent.invoke({ messages: "hi, my name is bob" }, config);
await agent.invoke({ messages: "write a short poem about cats" }, config);
await agent.invoke({ messages: "now do the same but for dogs" }, config);
const finalResponse = await agent.invoke({ messages: "what's my name?" }, config);

console.log(finalResponse.messages.at(-1)?.content);
// Your name is Bob!
```

--------------------------------

### Pass Conversation History to Subagent via State in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent

This example illustrates how to pass the full conversation history to a subagent by accessing the main agent's state. It customizes the tool definition to retrieve messages and other state keys from `getCurrentTaskInput<AgentState>()` and passes them to the subagent's `invoke` method. This allows for more context-aware subagent execution.

```typescript
import { createAgent, tool, AgentState, ToolMessage } from "langchain";
import { Command } from "@langchain/langgraph";
import * as z from "zod";

// Example of passing the full conversation history to the sub agent via the state.
const callSubagent1 = tool(
  async ({query}) => {
    const state = getCurrentTaskInput<AgentState>();
    // Apply any logic needed to transform the messages into a suitable input
    const subAgentInput = someLogic(query, state.messages);
    const result = await subagent1.invoke({
      messages: subAgentInput,
      // You could also pass other state keys here as needed.
      // Make sure to define these in both the main and subagent's
      // state schemas.
      exampleStateKey: state.exampleStateKey
    });
    return result.messages.at(-1)?.content;
  },
  {
    name: "subagent1_name",
    description: "subagent1_description",
  }
);

```

--------------------------------

### Configure Middleware to Jump to 'end' in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript example demonstrates how to configure a middleware function, `conditionalMiddleware`, to conditionally jump to the end of the agent's execution. The `afterModel` hook checks `someCondition(state)`; if true, it returns an object with `jumpTo: "end"`. This allows for controlled early termination based on specific state conditions.

```typescript
import { createMiddleware } from "langchain";

const conditionalMiddleware = createMiddleware({
  name: "ConditionalMiddleware",
  afterModel: (state) => {
    if (someCondition(state)) {
      return { jumpTo: "end" };
    }
    return;
  },
});
```

--------------------------------

### Create Supervisor Agent and Manage Email Tool

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines the 'manage_email' tool for sending emails via natural language and initializes the supervisor agent with scheduling and email tools. The supervisor agent uses a system prompt to understand its role as a personal assistant that can coordinate tool calls.

```javascript
const lastMessage = result.messages[result.messages.length - 1];
      return lastMessage.text;
    },
    {
      name: "manage_email",
      description: `
  Send emails using natural language.

  Use this when the user wants to send notifications, reminders, or any email communication.
  Handles recipient extraction, subject generation, and email composition.

  Input: Natural language email request (e.g., 'send them a reminder about the meeting')
      `.trim(),
      schema: z.object({
        request: z.string().describe("Natural language email request"),
      }),
    }
  );

  // ============================================================================
  // Step 4: Create the supervisor agent
  // ============================================================================

  const supervisorAgent = createAgent({
    model: llm,
    tools: [scheduleEvent, manageEmail],
    systemPrompt: `
  You are a helpful personal assistant.
  You can schedule calendar events and send emails.
  Break down user requests into appropriate tool calls and coordinate the results.
  When a request involves multiple actions, use multiple tools in sequence.
    `.trim(),
  });
```

--------------------------------

### Access Agent Short-Term Memory in a Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example shows how to access an agent's short-term memory (state) from within a tool using the `tool_runtime` parameter. The `stateSchema` defines the structure of the memory. The `getUserInfo` tool is defined using `langchain`'s `tool` utility, and it accesses the `userId` from the `config.context`. The agent is created with this tool and the state schema. When invoked, the agent passes context including `userId`, which the tool can then use. Dependencies include `zod` and `langchain`.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";

const stateSchema = z.object({
    userId: z.string(),
});

const getUserInfo = tool(
    async (_, config) => {
        const userId = config.context?.userId;
        return { userId };
    },
    {
        name: "get_user_info",
        description: "Get user info",
        schema: z.object({}),
    }
);

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getUserInfo],
    stateSchema,
});

const result = await agent.invoke(
    {
        messages: [{ role: "user", content: "what's my name?" }],
    },
    {
        context: {
        userId: "user_123",
        },
    }
);

console.log(result.messages.at(-1)?.content);
// Outputs: "User is John Smith."
```

--------------------------------

### Initialize Chroma Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the Chroma vector store with embeddings and collection configuration. Requires a `collectionName` to specify the target collection within Chroma.

```typescript
import { Chroma } from "@langchain/community/vectorstores/chroma";

const vectorStore = new Chroma(embeddings, {
  collectionName: "a-test-collection",
});
```

--------------------------------

### Configure Google VertexAI Credentials

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to the JSON key file for authentication with Google Cloud.

```bash
GOOGLE_APPLICATION_CREDENTIALS=credentials.json
```

--------------------------------

### Context Management with Context Editing Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript snippet illustrates how to manage conversation context in Langchain agents using `contextEditingMiddleware`. It's ideal for long conversations requiring cleanup, such as removing failed tool attempts or implementing custom context management. The example shows how to clear old tool uses based on a token limit.

```typescript
import { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    contextEditingMiddleware({
      edits: [
        new ClearToolUsesEdit({ maxTokens: 1000 }), // Clear old tool uses
      ],
    }),
  ],
});

```

--------------------------------

### Execute Supervisor Agent with User Request

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Demonstrates how to use the supervisor agent to process a complex user request that requires coordinating multiple tools (scheduling and email). It streams the agent's execution steps and logs the formatted messages to the console.

```javascript
// ============================================================================
  // Step 5: Use the supervisor
  // ============================================================================
  
  // Example: User request requiring both calendar and email coordination
  const userRequest =
    "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
    "and send them an email reminder about reviewing the new mockups.";

  console.log("User Request:", userRequest);
  console.log(`\n${"=".repeat(80)}\n`);

  const stream = await supervisorAgent.stream({
    messages: [{ role: "user", content: userRequest }],
  });

  for await (const step of stream) {
    for (const update of Object.values(step)) {
      if (update && typeof update === "object" && "messages" in update) {
        for (const message of update.messages) {
          console.log(message.toFormattedString());
        }
      }
    }
  }
```
```

--------------------------------

### Load MCP Tools Statelessly with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Demonstrates how to load MCP tools for stateless operations using the MultiServerMCPClient. This requires initializing the client and then loading tools associated with a specific server.

```typescript
import { loadMCPTools } from "@langchain/mcp-adapters/tools.js";
import { MultiServerMCPClient } from "@langchain/mcp-adapters/client.js";

const client = new MultiServerMCPClient({
  servers: [
    {
      name: "weather",
      url: "http://localhost:8080",
    },
  ],
});

const tools = await loadMCPTools(client, "weather");
console.log(`Weather MCP server running on port ${PORT}`);
});
```

--------------------------------

### Create LangChain Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Initializes a ReAct agent using `createAgent`. This agent is configured with a specific language model (`gpt-5`), a list of tools (including `executeSql`), and a custom system prompt obtained from `getSystemPrompt`. This sets up the agent for interactive SQL analysis.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-5",
  tools: [executeSql],
  systemPrompt: getSystemPrompt,
});

```

--------------------------------

### Initialize PGVector Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the PGVectorStore using `initialize`. This method is used to set up the connection and configuration for interacting with a PostgreSQL database containing vector data.

```typescript
import { PGVectorStore } from "@langchain/community/vectorstores/pgvector";

const vectorStore = await PGVectorStore.initialize(embeddings, {})
```

--------------------------------

### Initialize AWS Bedrock Chat Model (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Initializes a chat model using AWS Bedrock with the 'bedrock:gpt-4.1' identifier. Ensure AWS credentials are configured according to Bedrock documentation.

```javascript
const model = await initChatModel("bedrock:gpt-4.1");
```

--------------------------------

### Create LangChain Agent with RAG Middleware in JavaScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This JavaScript code demonstrates how to create a LangChain agent using the `createAgent` function. It configures the agent with a language model, an empty list of tools, and the `retrieveDocumentsMiddleware` to enable Retrieval-Augmented Generation capabilities.

```javascript
const agent = createAgent({
    model,
    tools: [],
    middleware: [retrieveDocumentsMiddleware],
  });
```

--------------------------------

### Initialize Qdrant Vector Store with Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Initializes the Qdrant vector store from an existing collection. This requires the embeddings object, the Qdrant URL, and the collection name.

```typescript
import { QdrantVectorStore } from "@langchain/qdrant";

const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {
  url: process.env.QDRANT_URL,
  collectionName: "langchainjs-testing",
});
```

--------------------------------

### Load Web Content with CheerioWebBaseLoader

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Loads content from a specified URL using the CheerioWebBaseLoader, targeting paragraph elements. It returns a list of Document objects, with the first document's page content length logged to the console. This is the first step in preparing data for indexing.

```typescript
import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";

const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector,
  }
);

const docs = await cheerioLoader.load();

console.assert(docs.length === 1);
console.log(`Total characters: ${docs[0].pageContent.length}`);
```

--------------------------------

### Configure Cohere API Key

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the COHERE_API_KEY environment variable for authenticating with the Cohere API.

```bash
COHERE_API_KEY=your-api-key
```

--------------------------------

### Invoke Model with Configuration (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates invoking a model with various configuration options. This includes setting a custom `runName`, `tags` for categorization, `metadata` for additional context, and `callbacks` for event handling. These configurations are useful for debugging, logging, and tracking invocations.

```typescript
const response = await model.invoke(
    "Tell me a joke",
    {
        runName: "joke_generation",      // Custom name for this run
        tags: ["humor", "demo"],          // Tags for categorization
        metadata: {"user_id": "123"},     // Custom metadata
        callbacks: [my_callback_handler], // Callback handlers
    }
)
```

--------------------------------

### Create and Test Email Agent in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines an email agent prompt and creates an agent using a language model and the `sendEmail` tool. It then tests the agent with a natural language request to send a reminder email, streams the response, and logs the formatted messages. The agent demonstrates inferring recipients, crafting professional emails, and confirming the action.

```typescript
const EMAIL_AGENT_PROMPT = `
You are an email assistant.
Compose professional emails based on natural language requests.
Extract recipient information and craft appropriate subject lines and body text.
Use send_email to send the message.
Always confirm what was sent in your final response.
`.trim();

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: EMAIL_AGENT_PROMPT,
});

const query = "Send the design team a reminder about reviewing the new mockups";

const stream = await emailAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Initialize Anthropic Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Shows how to initialize an Anthropic chat model using `initChatModel` or the `ChatAnthropic` class. Requires the Anthropic API key and specifies the model name (e.g., 'claude-sonnet-4-5-20250929').

```typescript
import { initChatModel } from "langchain";

process.env.ANTHROPIC_API_KEY = "your-api-key";

const model = await initChatModel("claude-sonnet-4-5-20250929");
```

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  model: "claude-sonnet-4-5-20250929",
  apiKey: "your-api-key"
});
```

--------------------------------

### Create Agent with Model Instance - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Initializes a Langchain.js agent using a pre-configured model instance. This provides granular control over model parameters like temperature, max tokens, and timeouts. It requires the `langchain` and the specific provider package (e.g., `@langchain/openai`).

```typescript
import { createAgent } from "langchain";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0.1,
  maxTokens: 1000,
  timeout: 30
});

const agent = createAgent({
  model,
  tools: []
});
```

--------------------------------

### Enable LangSmith Tracing with Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Set these environment variables to enable automatic tracing of LangChain agents to LangSmith. Ensure you replace '<your-api-key>' with your actual LangSmith API key.

```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>
```

--------------------------------

### Set Up LangSmith Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/test

Configure your environment variables to enable LangSmith tracing. This is a prerequisite for logging evaluator results and utilizing LangSmith's features for experiment tracking.

```bash
export LANGSMITH_API_KEY="your_langsmith_api_key"
export LANGSMITH_TRACING="true"
```

--------------------------------

### Test Calendar Agent with Natural Language Query

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This code snippet shows how to test the initialized calendar agent. It sends a natural language scheduling request to the agent and streams the response. The output is processed to display the agent's actions, including tool calls for checking availability and creating events, and the final natural language confirmation.

```typescript
const query = "Schedule a team meeting next Tuesday at 2pm for 1 hour";

const stream = await calendarAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Dynamic System Prompt based on User Preferences (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Creates a system prompt that incorporates user preferences, such as communication style, retrieved from a long-term store. It uses Zod for context schema validation and requires 'langchain' and 'zod' packages.

```typescript
import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

type Context = z.infer<typeof contextSchema>;

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  contextSchema,
  middleware: [
    dynamicSystemPromptMiddleware<Context>(async (state, runtime) => {
      const userId = runtime.context.userId;

      // Read from Store: get user preferences
      const store = runtime.store;
      const userPrefs = await store.get(["preferences"], userId);

      let base = "You are a helpful assistant.";

      if (userPrefs) {
        const style = userPrefs.value?.communicationStyle || "balanced";
        base += `\nUser prefers ${style} responses.`;
      }

      return base;
    }),
  ],
});
```

--------------------------------

### Use Multiple MCP Servers with LangChain Agent

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Connects to multiple MCP servers using `MultiServerMCPClient` from @langchain/mcp-adapters. It demonstrates setting up connections via 'stdio' for local tools and 'sse' for remote tools, then retrieves tools and creates an agent to invoke them.

```typescript
import { MultiServerMCPClient } from "@langchain/mcp-adapters";
import { ChatAnthropic } from "@langchain/anthropic";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({
    math: {
        transport: "stdio",  // Local subprocess communication
        command: "node",
        // Replace with absolute path to your math_server.js file
        args: ["/path/to/math_server.js"],
    },
    weather: {
        transport: "sse",  // Server-Sent Events for streaming
        // Ensure you start your weather server on port 8000
        url: "http://localhost:8000/mcp",
    },
});

const tools = await client.getTools();
const agent = createAgent({
    model: "claude-sonnet-4-5-20250929",
    tools,
});

const mathResponse = await agent.invoke({
    messages: [{ role: "user", content: "what's (3 + 5) x 12?" }],
});

const weatherResponse = await agent.invoke({
    messages: [{ role: "user", content: "what is the weather in nyc?" }],
});
```

--------------------------------

### Dynamic System Prompt based on Runtime Context (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Configures a system prompt dynamically based on runtime context, such as user role and deployment environment. It uses Zod for context schema definition and requires 'langchain' and 'zod' packages.

```typescript
import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.string(),
  deploymentEnv: z.string(),
});

type Context = z.infer<typeof contextSchema>;

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  contextSchema,
  middleware: [
    dynamicSystemPromptMiddleware<Context>((state, runtime) => {
      // Read from Runtime Context: user role and environment
      const userRole = runtime.context.userRole;
      const env = runtime.context.deploymentEnv;

      let base = "You are a helpful assistant.";

      if (userRole === "admin") {
        base += "\nYou have admin access. You can perform all operations.";
      } else if (userRole === "viewer") {
        base += "\nYou have read-only access. Guide users to read operations only.";
      }

      if (env === "production") {
        base += "\nBe extra careful with any data modifications.";
      }

      return base;
    }),
  ],
});
```

--------------------------------

### Tool Strategy Configuration

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Configure a `ToolStrategy` to enable tool calling for structured output generation. This strategy is compatible with most modern models that support tool calling.

```APIDOC
## Tool Strategy Configuration

### Description
Configure a `ToolStrategy` to enable tool calling for structured output generation. This strategy is compatible with most modern models that support tool calling.

### Method
`toolStrategy<StructuredResponseT>`

### Endpoint
N/A (Function within LangChain library)

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **responseFormat** (JsonSchemaFormat | ZodSchema<StructuredResponseT> | Array<ZodSchema<StructuredResponseT> | JsonSchemaFormat>) - Required - The schema defining the structured output format. Supports Zod Schema and JSON Schema.
- **options** (ToolStrategyOptions) - Optional - Configuration options for the tool strategy.
  - **options.toolMessageContent** (string) - Custom content for the tool message returned when structured output is generated. Defaults to a message showing the structured response data.
  - **options.handleError** (boolean | (error: ToolStrategyError) => string | Promise<string>) - Options parameter containing an optional `handleError` parameter for customizing the error handling strategy.
    - `true`: Catch all errors with default error template (default).
    - `False`: No retry, let exceptions propagate.
    - `(error: ToolStrategyError) => string | Promise<string>`: retry with the provided message or throw the error.

### Request Example
```ts
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductReview = z.object({
    rating: z.number().min(1).max(5).optional(),
    sentiment: z.enum(["positive", "negative"]),
    keyPoints: z.array(z.string()).describe("The key points of the review. Lowercase, 1-3 words each."),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy(ProductReview)
})

result = agent.invoke({
    "messages": [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

### Response
#### Success Response (200)
- **structuredResponse** (object) - The structured output generated by the model based on the provided schema.

#### Response Example
```json
{
  "rating": 5,
  "sentiment": "positive",
  "keyPoints": ["fast shipping", "expensive"]
}
```
```

--------------------------------

### Test RAG Agent with Multi-Step Query (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This code snippet demonstrates how to test a RAG agent constructed with Langchain.js. It defines an input message that requires iterative retrieval steps. The agent's `stream` method is called with the input, and the output messages are logged to the console, showing the agent's process of querying the retrieval tool and formulating an answer.

```typescript
let inputMessage = `What is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.`;

let agentInputs = { messages: [{ role: "user", content: inputMessage }] };

const stream = await agent.stream(agentInputs, {
  streamMode: "values",
});
for await (const step of stream) {
  const lastMessage = step.messages[step.messages.length - 1];
  console.log(`[${lastMessage.role}]: ${lastMessage.content}`);
  console.log("-----\n");
}
```

--------------------------------

### Configure Azure OpenAI Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets environment variables required for Azure OpenAI embeddings, including the instance name, API key, and API version.

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

--------------------------------

### Initialize Chat Model and Invoke with Messages (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates initializing a chat model and invoking it with a list of SystemMessage and HumanMessage objects. This is useful for setting up basic conversational context.

```typescript
import { initChatModel, HumanMessage, SystemMessage } from "langchain";

const model = await initChatModel("gpt-5-nano");

const systemMsg = new SystemMessage("You are a helpful assistant.");
const humanMsg = new HumanMessage("Hello, how are you?");

const messages = [systemMsg, humanMsg];
const response = await model.invoke(messages);  // Returns AIMessage
```

--------------------------------

### Download and Resolve SQLite Database Path (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Downloads the Chinook SQLite database from a GCS bucket if it does not exist locally. It saves the database to the local file system and returns its path. Requires 'node:fs/promises' and 'node:path' modules.

```typescript
import fs from "node:fs/promises";
import path from "node:path";

const url = "https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db";
const localPath = path.resolve("Chinook.db");

async function resolveDbPath() {
  if (await fs.exists(localPath)) {
    return localPath;
  }
  const resp = await fetch(url);
  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);
  const buf = Buffer.from(await resp.arrayBuffer());
  await fs.writeFile(localPath, buf);
  return localPath;
}
```

--------------------------------

### Dynamic Model Selection with Middleware in Javascript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Demonstrates selecting between different models (e.g., 'gpt-4o-mini', 'gpt-4o') at runtime based on conversation state, such as message count. This allows for cost optimization and sophisticated routing logic. Requires 'langchain' package.

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { createAgent, createMiddleware } from "langchain";

const basicModel = new ChatOpenAI({ model: "gpt-4o-mini" });
const advancedModel = new ChatOpenAI({ model: "gpt-4o" });

const dynamicModelSelection = createMiddleware({
  name: "DynamicModelSelection",
  wrapModelCall: (request, handler) => {
    // Choose model based on conversation complexity
    const messageCount = request.messages.length;

    return handler({
        ...request,
        model: messageCount > 10 ? advancedModel : basicModel,
    });
  },
});

const agent = createAgent({
  model: "gpt-4o-mini", // Base model (used when messageCount ≤ 10)
  tools,
  middleware: [dynamicModelSelection] as const,
});
```

--------------------------------

### Stream Custom Updates with LangChain Stream Writer (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Illustrates how to use `config.streamWriter` to send custom, real-time updates from tools as they execute. This is valuable for providing immediate user feedback on tool progress. It requires the `langchain` package and uses Zod for schema definition.

```typescript
import * as z from "zod";
import { tool } from "langchain";

const getWeather = tool(
  ({ city }, config) => {
    const writer = config.streamWriter;

    // Stream custom updates as the tool executes
    writer(`Looking up data for city: ${city}`);
    writer(`Acquired data for city: ${city}`);

    return `It's always sunny in ${city}!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string(),
    }),
  }
);

```

--------------------------------

### Select Model Based on Runtime Context (Cost/Environment)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware selects a language model based on information available in the runtime context, such as cost tier and environment. It defines a Zod schema for context (costTier, environment) and uses predefined models (premium, standard, budget). The model selection logic prioritizes a 'premium' cost tier in a 'production' environment, falls back to 'budget' if specified, and defaults to 'standard' otherwise.

```typescript
import * as z from "zod";
import { createMiddleware, initChatModel } from "langchain";

const contextSchema = z.object({
  costTier: z.string(),
  environment: z.string(),
});

// Initialize models once outside the middleware
const premiumModel = initChatModel("claude-sonnet-4-5-20250929");
const standardModel = initChatModel("gpt-4o");
const budgetModel = initChatModel("gpt-4o-mini");

const contextBasedModel = createMiddleware({
  name: "ContextBasedModel",
  contextSchema,
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: cost tier and environment
    const costTier = request.runtime.context.costTier;  // [!code highlight]
    const environment = request.runtime.context.environment;  // [!code highlight]

    let model;

    if (environment === "production" && costTier === "premium") {
      model = premiumModel;
    } else if (costTier === "budget") {
      model = budgetModel;
    } else {
      model = standardModel;
    }

    return handler({ ...request, model });  // [!code highlight]
  },
});
```

--------------------------------

### Use Langchain ClientSession for Stateful MCP Tool Usage

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Illustrates setting up a stateful server connection using `client.session()` for maintaining context between tool calls. This is essential for servers that require persistent state.

```typescript
import { loadMCPTools } from "@langchain/mcp-adapters/tools.js";
import { MultiServerMCPClient } from "@langchain/mcp-adapters/client.js";

const client = new MultiServerMCPClient({...});
const session = await client.session("math");
const tools = await loadMCPTools(session);
```

--------------------------------

### Access and Update Memory with LangChain Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Demonstrates how to utilize the LangChain store for persisting data across conversations. It shows how to retrieve user information using `store.get` and save information using `store.put`. This requires the `langchain` and `@langchain/openai` packages and utilizes Zod for schema validation.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";
import { InMemoryStore } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";

const store = new InMemoryStore();

// Access memory
const getUserInfo = tool(
  async ({ user_id }) => {
    const value = await store.get(["users"], user_id);
    console.log("get_user_info", user_id, value);
    return value;
  },
  {
    name: "get_user_info",
    description: "Look up user info.",
    schema: z.object({
      user_id: z.string(),
    }),
  }
);

// Update memory
const saveUserInfo = tool(
  async ({ user_id, name, age, email }) => {
    console.log("save_user_info", user_id, name, age, email);
    await store.put(["users"], user_id, { name, age, email });
    return "Successfully saved user info.";
  },
  {
    name: "save_user_info",
    description: "Save user info.",
    schema: z.object({
      user_id: z.string(),
      name: z.string(),
      age: z.number(),
      email: z.string(),
    }),
  }
);

const agent = createAgent({
  model: new ChatOpenAI({ model: "gpt-4o" }),
  tools: [getUserInfo, saveUserInfo],
  store,
});

// First session: save user info
await agent.invoke({
  messages: [
    {
      role: "user",
      content: "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev",
    },
  ],
});

// Second session: get user info
const result = await agent.invoke({
  messages: [
    { role: "user", content: "Get user info for user with id 'abc123'" },
  ],
});

console.log(result);
// Here is the user info for user with ID "abc123":
// - Name: Foo
// - Age: 25
// - Email: foo@langchain.dev

```

--------------------------------

### Define Agent System Prompt (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Generates a `SystemMessage` for an agent, defining its role as a 'careful SQLite analyst'. It includes the database schema, rules for query execution (read-only, limit rows, error handling), and instructions on how to interact with the `execute_sql` tool.

```typescript
import { SystemMessage } from "langchain";

const getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.

Authoritative schema (do not invent columns/tables):
${await getSchema()}

Rules:
- Think step-by-step.
- When you need data, call the tool `execute_sql` with ONE SELECT query.
- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.
- Limit to 5 rows unless user explicitly asks otherwise.
- If the tool returns 'Error:', revise the SQL and try again.
- Limit the number of attempts to 5.
- If you are not successful after 5 attempts, return a note to the user.
- Prefer explicit column lists; avoid SELECT *.
`);

```

--------------------------------

### Implement RAG Tool for Vector Store Search (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This code snippet demonstrates how to implement a custom tool for a RAG agent in Langchain.js. It defines a Zod schema for the query, creates a tool that wraps a vector store's `similaritySearch` method, and specifies the tool's name, description, schema, and response format. The tool retrieves documents based on a query and serializes their source and content, also returning the raw documents as artifacts.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";

const retrieveSchema = z.object({ query: z.string() });

const retrieve = tool(
  async ({ query }) => {
    const retrievedDocs = await vectorStore.similaritySearch(query, 2);
    const serialized = retrievedDocs
      .map(
        (doc) => `Source: ${doc.metadata.source}\nContent: ${doc.pageContent}`
      )
      .join("\n");
    return [serialized, retrievedDocs];
  },
  {
    name: "retrieve",
    description: "Retrieve information related to a query.",
    schema: retrieveSchema,
    responseFormat: "content_and_artifact",
  }
);
```

--------------------------------

### Dynamic Response Format Selection based on User Preferences in Store

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Uses middleware to select response formats based on user preferences stored in a data store. It retrieves user preferences, checks for a 'responseStyle' (defaulting to 'concise'), and applies either a 'verbose' or 'concise' schema accordingly.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const verboseResponse = z.object({
  answer: z.string().describe("Detailed answer"),
  sources: z.array(z.string()).describe("Sources used"),
});

const conciseResponse = z.object({
  answer: z.string().describe("Brief answer"),
});

const storeBasedOutput = createMiddleware({
  name: "StoreBasedOutput",
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's preferred response style
    const store = request.runtime.store;  // [!code highlight]
    const userPrefs = await store.get(["preferences"], userId);  // [!code highlight]

    if (userPrefs) {
      const style = userPrefs.value?.responseStyle || "concise";
      if (style === "verbose") {
        request.responseFormat = verboseResponse;  // [!code highlight]
      } else {
        request.responseFormat = conciseResponse;  // [!code highlight]
      }
    }

    return handler(request);
  },
});
```

--------------------------------

### Implement Agent Task Planning with To-Do List Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This snippet demonstrates how to equip Langchain agents with task planning and tracking capabilities using the `todoListMiddleware`. It defines custom tools for file operations and testing, then creates an agent that utilizes the middleware to break down complex requests into manageable steps. The agent's progress is tracked via the `result.todos` output.

```typescript
import { createAgent, HumanMessage, todoListMiddleware, tool } from "langchain";
import * as z from "zod";

const readFile = tool(
  async ({ filePath }) => {
    // Read file implementation
    return "file contents";
  },
  {
    name: "read_file",
    description: "Read contents of a file",
    schema: z.object({ filePath: z.string() }),
  }
);

const writeFile = tool(
  async ({ filePath, content }) => {
    // Write file implementation
    return `Wrote ${content.length} characters to ${filePath}`;
  },
  {
    name: "write_file",
    description: "Write content to a file",
    schema: z.object({ filePath: z.string(), content: z.string() }),
  }
);

const runTests = tool(
  async ({ testPath }) => {
    // Run tests implementation
    return "All tests passed!";
  },
  {
    name: "run_tests",
    description: "Run tests and return results",
    schema: z.object({ testPath: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [readFile, writeFile, runTests],
  middleware: [todoListMiddleware()] as const,
});

const result = await agent.invoke({
  messages: [
    new HumanMessage(
      "Refactor the authentication module to use async/await and ensure all tests pass"
    ),
  ],
});

// The agent will use write_todos to plan and track:
// 1. Read current authentication module code
// 2. Identify functions that need async conversion
// 3. Refactor functions to async/await
// 4. Update function calls throughout codebase
// 5. Run tests and fix any failures

console.log(result.todos); // Track the agent's progress through each step

```

--------------------------------

### Configure AWS Bedrock Chat Model (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Instantiates a ChatBedrockConverse model for AWS Bedrock with a specified model name and region. Requires '@langchain/aws' package and AWS credential configuration.

```typescript
import { ChatBedrockConverse } from "@langchain/aws";

// Follow the steps here to configure your credentials:
// https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

const model = new ChatBedrockConverse({
  model: "gpt-4.1",
  region: "us-east-2"
});
```

--------------------------------

### Set LangSmith Environment Variables for Tracing

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Configures environment variables to enable tracing in LangSmith, which is crucial for inspecting and debugging complex LangChain applications. Requires setting LANGSMITH_TRACING to 'true' and providing a LANGSMITH_API_KEY.

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

--------------------------------

### Initialize OpenAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the OpenAIEmbeddings class from the @langchain/openai package. Requires specifying the model to use, such as 'text-embedding-3-large'.

```typescript
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-large"
});
```

--------------------------------

### Agent with Provider Strategy (JSON Schema)

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Demonstrates creating an agent with a JSON schema for structured output using `providerStrategy`. This is an alternative to Zod schemas for models supporting native structured output.

```typescript
import { createAgent, providerStrategy } from "langchain";

const contactInfoSchema = {
    "type": "object",
    "description": "Contact information for a person.",
    "properties": {
        "name": {"type": "string", "description": "The name of the person"},
        "email": {"type": "string", "description": "The email address of the person"},
        "phone": {"type": "string", "description": "The phone number of the person"}
    },
    "required": ["name", "email", "phone"]
}

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: providerStrategy(contactInfoSchema)
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
});

result.structuredResponse;
// { name: "John Doe", email: "john@example.com", phone: "(555) 123-4567" }
```

--------------------------------

### Embed Queries and Compare Vector Lengths in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Demonstrates how to use an initialized embeddings model to generate vectors for text and assert that the generated vectors have the same length. It then logs the first 10 elements of the first vector.

```typescript
const vector1 = await embeddings.embedQuery(allSplits[0].pageContent);
const vector2 = await embeddings.embedQuery(allSplits[1].pageContent);

assert vector1.length === vector2.length;
console.log(`Generated vectors of length ${vector1.length}\n`);
console.log(vector1.slice(0, 10));
```

--------------------------------

### Initialize Google VertexAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the VertexAIEmbeddings class from the @langchain/google-vertexai package. Requires specifying the Vertex AI model, such as 'gemini-embedding-001'.

```typescript
import { VertexAIEmbeddings } from "@langchain/google-vertexai";

const embeddings = new VertexAIEmbeddings({
  model: "gemini-embedding-001"
});
```

--------------------------------

### Create Sample LangChain Documents

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Demonstrates how to create custom Document objects in TypeScript for LangChain. Each Document contains page content, metadata (like source), and optionally an ID, representing a piece of text with associated information.

```typescript
import { Document } from "@langchain/core/documents";

const documents = [
  new Document({
    pageContent:
      "Dogs are great companions, known for their loyalty and friendliness.",
    metadata: { source: "mammal-pets-doc" },
  }),
  new Document({
    pageContent: "Cats are independent pets that often enjoy their own space.",
    metadata: { source: "mammal-pets-doc" },
  }),
];
```

--------------------------------

### Configure Human-in-the-loop Middleware in Langchain Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Demonstrates how to add and configure the `humanInTheLoopMiddleware` when creating a Langchain agent. It shows how to specify which tool calls should trigger an interrupt and what decisions are allowed for each. It also highlights the necessity of configuring a checkpointer for persistence, using `MemorySaver` for demonstration and suggesting persistent options like `AsyncPostgresSaver` for production. The `descriptionPrefix` option for interrupt messages is also illustrated.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const agent = createAgent({
    model: "gpt-4o",
    tools: [writeFileTool, executeSQLTool, readDataTool],
    middleware: [
        humanInTheLoopMiddleware({
            interruptOn: {
                write_file: true, // All decisions (approve, edit, reject) allowed
                execute_sql: {
                    allowedDecisions: ["approve", "reject"],
                    // No editing allowed
                    description: "🚨 SQL execution requires DBA approval",
                },
                // Safe operation, no approval needed
                read_data: false,
            },
            // Prefix for interrupt messages - combined with tool name and args to form the full message
            // e.g., "Tool execution pending approval: execute_sql with query='DELETE FROM..."'
            // Individual tools can override this by specifying a "description" in their interrupt config
            descriptionPrefix: "Tool execution pending approval",
        }),
    ],
    // Human-in-the-loop requires checkpointing to handle interrupts.
    // In production, use a persistent checkpointer like AsyncPostgresSaver.
    checkpointer: new MemorySaver(), // [!code highlight]
});

```

--------------------------------

### Binding User Tools with `bindTools()`

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet demonstrates how to define a custom tool using `tool()` and bind it to a ChatOpenAI model using `bindTools()`. The model can then be invoked to potentially call the bound tool, and the response is processed to log any tool calls made.

```typescript
import { tool } from "langchain";
import * as z from "zod";
import { ChatOpenAI } from "@langchain/openai";

const getWeather = tool(
  (input) => `It's sunny in ${input.location}.`,
  {
    name: "get_weather",
    description: "Get the weather at a location.",
    schema: z.object({
      location: z.string().describe("The location to get the weather for"),
    }),
  },
);

const model = new ChatOpenAI({ model: "gpt-4o" });
const modelWithTools = model.bindTools([getWeather]);

const response = await modelWithTools.invoke("What's the weather like in Boston?");
const toolCalls = response.tool_calls || [];
for (const tool_call of toolCalls) {
  console.log(`Tool: ${tool_call.name}`);
  console.log(`Args: ${tool_call.args}`);
}
```

--------------------------------

### Display First 500 Characters of Loaded Document Content

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Logs the first 500 characters of the page content from the first Document object loaded by the CheerioWebBaseLoader. This is useful for quickly inspecting the beginning of the loaded content.

```typescript
console.log(docs[0].pageContent.slice(0, 500));
```

--------------------------------

### Configure Agent for Human Review of SQL Queries (Python)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

This code configures a LangChain agent to pause before executing the `sql_db_query` tool using the `HumanInTheLoopMiddleware`. It also sets up an `InMemorySaver` for checkpoiniting. The middleware intercepts tool calls, pausing execution for human approval.

```python
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver


agent = create_agent(
    model,
    tools,
    system_prompt=system_prompt,
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={"sql_db_query": True},
            description_prefix="Tool execution pending approval",
        ),
    ],
    checkpointer=InMemorySaver(),
)
```

--------------------------------

### Basic Text Streaming with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to stream basic text output from a Langchain model. It uses a for-await-of loop to iterate over the stream and log each text chunk to the console. This is useful for displaying responses as they are generated, enhancing user experience for longer outputs.

```typescript
const stream = await model.stream("Why do parrots have colorful feathers?");
for await (const chunk of stream) {
  console.log(chunk.text)
}
```

--------------------------------

### Write to State using Command in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This snippet demonstrates how to write to an agent's State to track session-specific information using the `Command` class. It defines an `authenticateUser` tool that updates the 'authenticated' status in the State based on a provided password. Dependencies include `@langchain/core/tools`, `langchain`, and `@langchain/langgraph`.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";
import { Command } from "@langchain/langgraph";

const authenticateUser = tool(
  async ({ password }, { runtime }) => {
    // Perform authentication
    if (password === "correct") {
      // Write to State: mark as authenticated using Command
      return new Command({
        update: { authenticated: true },
      });
    } else {
      return new Command({ update: { authenticated: false } });
    }
  },
  {
    name: "authenticate_user",
    description: "Authenticate user and update State",
    schema: z.object({
      password: z.string(),
    }),
  }
);

```

--------------------------------

### TypeScript SQL Agent Implementation

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

A detailed TypeScript implementation for a SQL agent using Langchain. It includes functions for downloading and accessing a SQLite database, sanitizing SQL queries for safety, and executing read-only SELECT statements. The agent is configured with a model, tools, and a system prompt that enforces specific rules for interaction with the database.

```typescript
import fs from "node:fs/promises";
import path from "node:path";
import { SqlDatabase } from "@langchain/classic/sql_db";
import { DataSource } from "typeorm";
import { SystemMessage, createAgent, tool } from "langchain"
import * as z from "zod";

const url = "https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db";
const localPath = path.resolve("Chinook.db");

async function resolveDbPath() {
  if (await fs.exists(localPath)) {
    return localPath;
  }
  const resp = await fetch(url);
  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);
  const buf = Buffer.from(await resp.arrayBuffer());
  await fs.writeFile(localPath, buf);
  return localPath;
}

let db: SqlDatabase | undefined;
async function getDb() {
  if (!db) {
    const dbPath = await resolveDbPath();
    const datasource = new DataSource({ type: "sqlite", database: dbPath });
    db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });
  }
  return db;
}

async function getSchema() {
  const db = await getDb();
  return await db.getTableInfo();
}

const DENY_RE = /\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\b/i;
const HAS_LIMIT_TAIL_RE = /\blimit\b\s+\d+(\s*,\s*\d+)?\s*;?\s*$/i;

function sanitizeSqlQuery(q) {
  let query = String(q ?? "").trim();

  // block multiple statements (allow one optional trailing ;)
  const semis = [...query].filter((c) => c === ";").length;
  if (semis > 1 || (query.endsWith(";") && query.slice(0, -1).includes(";"))) {
    throw new Error("multiple statements are not allowed.")
  }
  query = query.replace(/;+\s*$/g, "").trim();

  // read-only gate
  if (!query.toLowerCase().startsWith("select")) {
    throw new Error("Only SELECT statements are allowed")
  }
  if (DENY_RE.test(query)) {
    throw new Error("DML/DDL detected. Only read-only queries are permitted.")
  }

  // append LIMIT only if not already present
  if (!HAS_LIMIT_TAIL_RE.test(query)) {
    query += " LIMIT 5";
  }
  return query;
}

const executeSql = tool(
  async ({ query }) => {
    const q = sanitizeSqlQuery(query);
    try {
      const result = await db.run(q);
      return typeof result === "string" ? result : JSON.stringify(result, null, 2);
    } catch (e) {
      throw new Error(e?.message ?? String(e))
    }
  },
  {
    name: "execute_sql",
    description: "Execute a READ-ONLY SQLite SELECT query and return results.",
    schema: z.object({
      query: z.string().describe("SQLite SELECT query to execute (read-only)."),
    }),
  }
);

const getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.

Authoritative schema (do not invent columns/tables):
${await getSchema()}

Rules:
- Think step-by-step.
- When you need data, call the tool `execute_sql` with ONE SELECT query.
- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.
- Limit to 5 rows unless user explicitly asks otherwise.
- If the tool returns 'Error:', revise the SQL and try again.
- Limit the number of attempts to 5.
- If you are not successful after 5 attempts, return a note to the user.
- Prefer explicit column lists; avoid SELECT *.
`);

export const agent = createAgent({
  model: "gpt-5",
  tools: [executeSql],
  systemPrompt: getSystemPrompt,
});

```

--------------------------------

### Dynamic System Prompt based on Conversation State (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Generates a system prompt that adapts based on the current conversation length. It reads the message count from the state and appends instructions to be concise if the conversation exceeds 10 messages. Requires the 'langchain' package.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    dynamicSystemPromptMiddleware((state) => {
      // Read from State: check conversation length
      const messageCount = state.messages.length;

      let base = "You are a helpful assistant.";

      if (messageCount > 10) {
        base += "\nThis is a long conversation - be extra concise.";
      }

      return base;
    }),
  ],
});
```

--------------------------------

### Initialize Bedrock Chat Model using Class

Source: https://docs.langchain.com/oss/javascript/langchain/models

Instantiates a chat model from AWS Bedrock directly using the `ChatBedrockConverse` class. This provides more explicit control over model configuration. Follow the provided AWS documentation link to configure necessary credentials before use. The `model` and `region` are required parameters.

```typescript
import { ChatBedrockConverse } from "@langchain/aws";

const model = new ChatBedrockConverse({
  model: "gpt-4.1",
  region: "us-east-2"
});
```

--------------------------------

### Streaming Tool Calls and Reasoning with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/models

Illustrates how to stream advanced content types like tool calls and reasoning from a Langchain model. The code iterates through content blocks within each chunk, checking their type to handle reasoning, tool call chunks, and text separately. This allows for more complex interactions where the model might use tools or provide intermediate reasoning steps.

```typescript
const stream = await model.stream("What color is the sky?");
for await (const chunk of stream) {
  for (const block of chunk.contentBlocks) {
    if (block.type === "reasoning") {
      console.log(`Reasoning: ${block.reasoning}`);
    } else if (block.type === "tool_call_chunk") {
      console.log(`Tool call chunk: ${block}`);
    } else if (block.type === "text") {
      console.log(block.text);
    } else {
      ...
    }
  }
}
```

--------------------------------

### Execute and Stream Agentic RAG Chain - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Demonstrates how to execute the configured agentic RAG chain with a user input message and stream the response. It processes the streamed messages, printing each step and the final output, showing how retrieved context is incorporated into the model's prompt.

```typescript
let inputMessage = `What is Task Decomposition?`;

let chainInputs = { messages: [{ role: "user", content: inputMessage }] };

const stream = await agent.stream(chainInputs, {
  streamMode: "values",
})
for await (const step of stream) {
  const lastMessage = step.messages[step.messages.length - 1];
  prettyPrint(lastMessage);
  console.log("-----\n");
}

```

--------------------------------

### Create SQL Execution Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Defines an `executeSql` tool using `langchain`'s `tool` function. This tool takes a SQL query, sanitizes it using `sanitizeSqlQuery`, executes it against a database (`db.run`), and returns the result as a JSON string. It includes error handling for execution failures.

```typescript
import { tool } from "langchain"
import * as z from "zod";

const executeSql = tool(
  async ({ query }) => {
    const q = sanitizeSqlQuery(query);
    try {
      const result = await db.run(q);
      return typeof result === "string" ? result : JSON.stringify(result, null, 2);
    } catch (e) {
      throw new Error(e?.message ?? String(e))
    }
  },
  {
    name: "execute_sql",
    description: "Execute a READ-ONLY SQLite SELECT query and return results.",
    schema: z.object({
      query: z.string().describe("SQLite SELECT query to execute (read-only)."),
    }),
  }
);

```

--------------------------------

### Initialize MongoDB Atlas Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes MongoDB Atlas Vector Search. This requires connection details to your MongoDB Atlas cluster, including database name, collection name, and index details.

```typescript
import { MongoDBAtlasVectorSearch } from "@langchain/mongodb"
import { MongoClient } from "mongodb";

const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
const collection = client
  .db(process.env.MONGODB_ATLAS_DB_NAME)
  .collection(process.env.MONGODB_ATLAS_COLLECTION_NAME);

const vectorStore = new MongoDBAtlasVectorSearch(embeddings, {
  collection: collection,
  indexName: "vector_index",
  textKey: "text",
  embeddingKey: "embedding",
});
```

--------------------------------

### Fetch User Data using Runtime Context in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This tool fetches user data by utilizing configuration from the runtime context, such as API keys and database connection strings. It uses these details to perform a database query. The function returns a summary of the results found for the specified user.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
  apiKey: z.string(),
  dbConnection: z.string(),
});

const fetchUserData = tool(
  async ({ query }, { runtime }) => {
    // Read from Runtime Context: get API key and DB connection
    const { userId, apiKey, dbConnection } = runtime.context;

    // Use configuration to fetch data
    const results = await performDatabaseQuery(dbConnection, query, apiKey);

    return `Found ${results.length} results for user ${userId}`;
  },
  {
    name: "fetch_user_data",
    description: "Fetch data using Runtime Context configuration",
    schema: z.object({
      query: z.string(),
    }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [fetchUserData],
  contextSchema,
});

```

--------------------------------

### Initialize Memory Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes a MemoryVectorStore with an embeddings object. This vector store keeps data in memory, making it fast but not persistent.

```typescript
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);
```

--------------------------------

### Structured Output Generation with Zod in LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/agents

This snippet demonstrates how to configure a LangChain agent to return output in a specific format using the `responseFormat` parameter and Zod schemas. It takes user input, extracts contact information, and logs the structured response.

```typescript
import * as z from "zod";
import { createAgent } from "langchain";

const ContactInfo = z.object({
  name: z.string(),
  email: z.string(),
  phone: z.string(),
});

const agent = createAgent({
  model: "gpt-4o",
  responseFormat: ContactInfo,
});

const result = await agent.invoke({
  messages: [
    {
      role: "user",
      content: "Extract contact info from: John Doe, john@example.com, (555) 123-4567",
    },
  ],
});

console.log(result.structuredResponse);
// {
//   name: 'John Doe',
//   email: 'john@example.com',
//   phone: '(555) 123-4567'
// }
```

--------------------------------

### Stream Agent Execution and Collect Interrupts (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Executes an agent with a given query and configuration, streaming the execution steps. It captures and logs any interrupt events encountered during the stream, storing them in an `interrupts` array for later inspection. This allows for programmatic handling of user review steps.

```typescript
const query =
  "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
  "and send them an email reminder about reviewing the new mockups.";

const config = { configurable: { thread_id: "6" } };

const interrupts: any[] = [];
const stream = await supervisorAgent.stream(
  { messages: [{ role: "user", content: query }] },
  config
);

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    } else if (Array.isArray(update)) {
      const interrupt = update[0];
      interrupts.push(interrupt);
      console.log(`\nINTERRUPTED: ${interrupt.id}`);
    }
  }
}
```

--------------------------------

### Resume Agent Execution After Human Review (Python)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

This code shows how to resume the agent's execution after a human review interruption, specifically by sending an 'approve' command. It continues streaming the agent's output, processing messages and potential further interruptions.

```python
from langgraph.types import Command

for step in agent.stream(
    Command(resume={"decisions": [{"type": "approve"}]}),
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        print("INTERRUPTED:")
        interrupt = step["__interrupt__"][0]
        for request in interrupt.value["action_requests"]:
            print(request["description"])
    else:
        pass
```

--------------------------------

### Integrate InMemoryStore into LangChain Agent Tools

Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory

Shows how to create a LangChain agent tool that reads long-term memory from an InMemoryStore. It defines a Zod schema for context, initializes the store, writes sample data, defines a `getUserInfo` tool that accesses the store via runtime context, and creates an agent that uses this tool and store, invoking it with specific context.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";
import { InMemoryStore, type Runtime } from "@langchain/langgraph";

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.
const store = new InMemoryStore(); // [!code highlight]
const contextSchema = z.object({
    userId: z.string(),
});

// Write sample data to the store using the put method
await store.put( // [!code highlight]
    ["users"], // Namespace to group related data together (users namespace for user data)
    "user_123", // Key within the namespace (user ID as key)
    {
        name: "John Smith",
        language: "English",
    } // Data to store for the given user
);

const getUserInfo = tool(
  // Look up user info.
  async (_, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    // Access the store - same as that provided to `createAgent`
    const userId = runtime.context?.userId;
    if (!userId) {
      throw new Error("userId is required");
    }
    // Retrieve data from store - returns StoreValue object with value and metadata
    const userInfo = await runtime.store.get(["users"], userId);
    return userInfo?.value ? JSON.stringify(userInfo.value) : "Unknown user";
  },
  {
    name: "getUserInfo",
    description: "Look up user info by userId from the store.",
    schema: z.object({}),
  }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getUserInfo],
    contextSchema,
    // Pass store to agent - enables agent to access store when running tools
    store, // [!code highlight]
});

// Run the agent
const result = await agent.invoke(
    { messages: [{ role: "user", content: "look up user information" }] },
    { context: { userId: "user_123" } } // [!code highlight]
);

console.log(result.messages.at(-1)?.content);
/**
 * Outputs:
 * User Information:
 * - Name: John Smith
 * - Language: English
 */

```

--------------------------------

### Configure Sub-Agents with Human-in-the-Loop Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Configures two sub-agents (`calendarAgent`, `emailAgent`) to use `humanInTheLoopMiddleware`. This middleware allows for manual review of specific tool calls (`create_calendar_event`, `send_email`) by permitting various response types. A `supervisorAgent` is also set up with a `MemorySaver` checkpointer, which is essential for pausing and resuming the agent's execution flow.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: CALENDAR_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { create_calendar_event: true },
      descriptionPrefix: "Calendar event pending approval",
    }),
  ],
});

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: EMAIL_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { send_email: true },
      descriptionPrefix: "Outbound email pending approval",
    }),
  ],
});

const supervisorAgent = createAgent({
  model: llm,
  tools: [scheduleEvent, manageEmail],
  systemPrompt: SUPERVISOR_PROMPT,
  checkpointer: new MemorySaver(),
});
```

--------------------------------

### Define a Tool to Create Calendar Events (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines a Langchain tool named 'create_calendar_event' using Zod for schema validation. This tool is a stub that simulates creating a calendar event and requires precise ISO datetime formatting for its input.

```typescript
import { tool } from "langchain";
import { z } from "zod";

const createCalendarEvent = tool(
  async ({ title, startTime, endTime, attendees, location }) => {
    // Stub: In practice, this would call Google Calendar API, Outlook API, etc.
    return `Event created: ${title} from ${startTime} to ${endTime} with ${attendees.length} attendees`;
  },
  {
    name: "create_calendar_event",
    description: "Create a calendar event. Requires exact ISO datetime format.",
    schema: z.object({
      title: z.string(),
      startTime: z.string().describe("ISO format: '2024-01-15T14:00:00'"),
      endTime: z.string().describe("ISO format: '2024-01-15T15:00:00'"),
      attendees: z.array(z.string()).describe("email addresses"),
      location: z.string().optional(),
    }),
  }
);

```

--------------------------------

### Initialize Azure OpenAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the AzureOpenAIEmbeddings class from the @langchain/openai package. Requires specifying the Azure OpenAI deployment name for embeddings.

```typescript
import { AzureOpenAIEmbeddings } from "@langchain/openai";

const embeddings = new AzureOpenAIEmbeddings({
  azureOpenAIApiEmbeddingsDeploymentName: "text-embedding-ada-002"
});
```

--------------------------------

### Defining Tools for Langchain Agents in Javascript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Shows how to define custom tools (e.g., 'search', 'get_weather') with Zod schemas for input validation and descriptions. These tools enable agents to perform actions. Requires 'langchain' and 'zod' packages.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";

const search = tool(
  ({ query }) => `Results for: ${query}`,
  {
    name: "search",
    description: "Search for information",
    schema: z.object({
      query: z.string().describe("The query to search for"),
    }),
  }
);

const getWeather = tool(
  ({ location }) => `Weather in ${location}: Sunny, 72°F`,
  {
    name: "get_weather",
    description: "Get weather information for a location",
    schema: z.object({
      location: z.string().describe("The location to get weather for"),
    }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [search, getWeather],
});
```

--------------------------------

### Initialize MistralAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the MistralAIEmbeddings class from the @langchain/mistralai package. Requires specifying the MistralAI model, such as 'mistral-embed'.

```typescript
import { MistralAIEmbeddings } from "@langchain/mistralai";

const embeddings = new MistralAIEmbeddings({
  model: "mistral-embed"
});
```

--------------------------------

### Using Tool or Provider Strategies for Response Format

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Demonstrates how to explicitly control the structured output strategy using `toolStrategy` or `providerStrategy` when creating an agent. This allows for specific enforcement of how the agent should handle structured data.

```typescript
import { toolStrategy, providerStrategy } from "langchain";

const agent = createAgent({
    // use a provider strategy if supported by the model
    responseFormat: providerStrategy(z.object({ ... }))
    // or enforce a tool strategy
    responseFormat: toolStrategy(z.object({ ... }))
})
```

--------------------------------

### Wrap Email Agent as a Tool in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Wraps the email agent as a reusable tool named `manage_email` for a supervisor agent. This tool takes a natural language email request, invokes the email agent, and returns the agent's final response. The description clearly outlines its purpose, usage, and input schema, specifying a natural language string for email requests.

```typescript
const manageEmail = tool(
  async ({ request }) => {
    const result = await emailAgent.invoke({
      messages: [{ role: "user", content: request }]
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "manage_email",
    description: `
Send emails using natural language.

Use this when the user wants to send notifications, reminders, or any email communication.
Handles recipient extraction, subject generation, and email composition.

Input: Natural language email request (e.g., 'send them a reminder about the meeting')
    `,
    schema: z.object({
      request: z.string().describe("Natural language email request"),
    }),
  }
);
```

--------------------------------

### Define Agent Context Schema and Invoke Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/runtime

Demonstrates how to define a context schema using Zod for an agent created with `createAgent`. It shows how to pass contextual data, such as a user name, during agent invocation.

```typescript
import * as z from "zod";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userName: z.string(),
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [
    /* ... */
  ],
  contextSchema,
});

const result = await agent.invoke(
  { messages: [{ role: "user", content: "What's my name?" }] },
  { context: { userName: "John Smith" } }
);
```

--------------------------------

### Initialize AWS Bedrock Converse Chat Model in JavaScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Shows how to initialize an AWS Bedrock Converse chat model using LangChain.js. The 'initChatModel' function is used, and it requires AWS credentials to be configured following the official AWS documentation.

```typescript
import { initChatModel } from "langchain";

// Follow the steps here to configure your credentials:
// https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

// Example initialization (model name might vary based on Bedrock support):
// const model = await initChatModel("bedrock-converse:anthropic.claude-3-sonnet-20240229-v1:0");
```

--------------------------------

### Initialize Bedrock Converse Chat Model in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates initializing an AWS Bedrock Converse chat model using `initChatModel`. This method is suitable for integrating with AWS Bedrock services. Further configuration might be required based on specific Bedrock model and region.

```typescript
import { initChatModel } from "langchain";

// AWS credentials and region should be configured in the environment or via AWS SDK configuration.

const model = await initChatModel("bedrock_converse");

```

--------------------------------

### Dynamic Response Format Selection based on Runtime Context

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Implements middleware to dynamically select response formats based on runtime context, such as user role and environment. It applies an 'adminResponse' schema for administrators in a 'production' environment and a 'userResponse' schema otherwise.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.string(),
  environment: z.string(),
});

const adminResponse = z.object({
  answer: z.string().describe("Answer"),
  debugInfo: z.record(z.any()).describe("Debug information"),
  systemStatus: z.string().describe("System status"),
});

const userResponse = z.object({
  answer: z.string().describe("Answer"),
});

const contextBasedOutput = createMiddleware({
  name: "ContextBasedOutput",
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: user role and environment
    const userRole = request.runtime.context.userRole;  // [!code highlight]
    const environment = request.runtime.context.environment;  // [!code highlight]

    if (userRole === "admin" && environment === "production") {
      responseFormat = adminResponse;  // [!code highlight]
    } else {
      responseFormat = userResponse;  // [!code highlight]
    }

    return handler({ ...request, responseFormat });
  },
});
```

--------------------------------

### Implement Human-in-the-Loop Approval Workflow with LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

Demonstrates using LangChain's humanInTheLoopMiddleware to pause agent execution for sensitive operations, requiring explicit human approval before proceeding. It covers configuring which actions require approval and how to resume the agent's execution with the approval decision.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver, Command } from "@langchain/langgraph";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, sendEmailTool, deleteDatabaseTool],
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: {
        // Require approval for sensitive operations
        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },
        delete_database: { allowAccept: true, allowEdit: true, allowRespond: true },
        // Auto-approve safe operations
        search: false,
      }
    }),
  ],
  checkpointer: new MemorySaver(),
});

// Human-in-the-loop requires a thread ID for persistence
const config = { configurable: { thread_id: "some_id" } };

// Agent will pause and wait for approval before executing sensitive tools
let result = await agent.invoke(
  { messages: [{ role: "user", content: "Send an email to the team" }] },
  config
);

result = await agent.invoke(
  new Command({ resume: { decisions: [{ type: "approve" }] } }),
  config  // Same thread ID to resume the paused conversation
);

```

--------------------------------

### Create Agent with Static Model Identifier - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Initializes a Langchain.js agent using a static model identifier string. This approach is straightforward for common model configurations. It requires the `langchain` package and accepts a model identifier and a list of tools.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-5",
  tools: []
});
```

--------------------------------

### Define a Tool to Send Email (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines a Langchain tool named 'send_email' using Zod for schema validation. This tool is a stub that simulates sending an email and requires properly formatted email addresses for the 'to' and optional 'cc' fields.

```typescript
import { tool } from "langchain";
import { z } from "zod";

const sendEmail = tool(
  async ({ to, subject, body, cc }) => {
    // Stub: In practice, this would call SendGrid, Gmail API, etc.
    return `Email sent to ${to.join(', ')} - Subject: ${subject}`;
  },
  {
    name: "send_email",
    description: "Send an email via email API. Requires properly formatted addresses.",
    schema: z.object({
      to: z.array(z.string()).describe("email addresses"),
      subject: z.string(),
      body: z.string(),
      cc: z.array(z.string()).optional(),
    }),
  }
);

```

--------------------------------

### Server-Side Tool Use in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to use server-side tool invocation with Langchain JS. The model can bind to tools and interact with them within a single turn. The response's contentBlocks will contain a provider-agnostic format of tool calls and results, eliminating the need for separate ToolMessage objects.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel("gpt-4.1-mini");
const modelWithTools = model.bindTools([{ type: "web_search" }]);

const message = await modelWithTools.invoke("What was a positive news story from today?");
console.log(message.contentBlocks);
```

--------------------------------

### Handle Multimodal Output with LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a model that can generate multimodal output, such as images, and access these content blocks from the response. This requires a model with underlying multimodal capabilities and assumes the response is an AIMessage containing content blocks.

```typescript
const response = await model.invoke("Create a picture of a cat");
console.log(response.contentBlocks);
// [
//   { type: "text", text: "Here's a picture of a cat" },
//   { type: "image", data: "...", mimeType: "image/jpeg" },
// ]
```

--------------------------------

### Custom Output Formatting with Command in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent

This snippet demonstrates how to use the Command class in Langchain.js to format a subagent's output. It defines a tool that invokes a subagent, then wraps the subagent's response, including additional state keys, within a Command object. This allows for merging custom state with the subagent's response before returning it.

```typescript
import { tool, ToolMessage } from "langchain";
import { Command } from "@langchain/langgraph";
import * as z from "zod";

const callSubagent1 = tool(
  async ({ query }, config) => {
    const result = await subagent1.invoke({
      messages: [{ role: "user", content: query }]
    });

    // Return a Command to update multiple state keys
    return new Command({
      update: {
        // Pass back additional state from the subagent
        exampleStateKey: result.exampleStateKey,
        messages: [
          new ToolMessage({
            content: result.messages.at(-1)?.text,
            tool_call_id: config.toolCall?.id!
          })
        ]
      }
    });
  },
  {
    name: "subagent1_name",
    description: "subagent1_description",
    schema: z.object({
      query: z.string().describe("The query to send to subagent1")
    })
  }
);

```

--------------------------------

### Configure LangSmith Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Sets environment variables required for LangSmith tracing, enabling inspection of agent activity. This configuration can be done in bash or directly within a TypeScript Node.js environment.

```bash
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

```typescript
process.env.LANGSMITH_TRACING = "true";
process.env.LANGSMITH_API_KEY = "...";
```

--------------------------------

### Select Model Based on User Preferences (Store Middleware)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware selects a language model based on user preferences stored in a database. It defines a Zod schema for context (userId) and uses the store to retrieve user preferences, specifically a 'preferredModel'. If a preference is found and the model exists in the MODEL_MAP, that model is used; otherwise, a default model is applied. This allows for personalized model selection.

```typescript
import * as z from "zod";
import { createMiddleware, initChatModel } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

// Initialize available models once
const MODEL_MAP = {
  "gpt-4o": initChatModel("gpt-4o"),
  "gpt-4o-mini": initChatModel("gpt-4o-mini"),
  "claude-sonnet": initChatModel("claude-sonnet-4-5-20250929"),
};

const storeBasedModel = createMiddleware({
  name: "StoreBasedModel",
  contextSchema,
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's preferred model
    const store = request.runtime.store;  // [!code highlight]
    const userPrefs = await store.get(["preferences"], userId);  // [!code highlight]

    let model = request.model;

    if (userPrefs) {
      const preferredModel = userPrefs.value?.preferredModel;
      if (preferredModel && MODEL_MAP[preferredModel]) {
        model = MODEL_MAP[preferredModel];  // [!code highlight]
      }
    }

    return handler({ ...request, model });  // [!code highlight]
  },
});
```

--------------------------------

### Store-Based Tool Selection Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware filters tools based on user preferences or feature flags stored in a 'Store'. It retrieves enabled features for a given user ID and filters the available tools accordingly, allowing for personalized toolsets.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const storeBasedTools = createMiddleware({
  name: "StoreBasedTools",
  contextSchema,
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's enabled features
    const store = request.runtime.store;  // [!code highlight]
    const featureFlags = await store.get(["features"], userId);  // [!code highlight]

    let filteredTools = request.tools;

    if (featureFlags) {
      const enabledFeatures = featureFlags.value?.enabledTools || [];
      filteredTools = request.tools.filter(t => enabledFeatures.includes(t.name));  // [!code highlight]
    }

    return handler({ ...request, tools: filteredTools });  // [!code highlight]
  },
});
```

--------------------------------

### Write to Store for Persistent Data in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This code illustrates how to write to the Store to persist data across user sessions. The `savePreference` tool retrieves existing preferences, merges a new preference, and then saves the updated preferences back to the Store using `store.put`. It requires `zod`, `@langchain/core/tools`, and `langchain`.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const savePreference = tool(
  async ({ preferenceKey, preferenceValue }, { runtime }) => {
    const userId = runtime.context.userId;

    // Read existing preferences
    const store = runtime.store;
    const existingPrefs = await store.get(["preferences"], userId);

    // Merge with new preference
    const prefs = existingPrefs?.value || {};
    prefs[preferenceKey] = preferenceValue;

    // Write to Store: save updated preferences
    await store.put(["preferences"], userId, prefs);

    return `Saved preference: ${preferenceKey} = ${preferenceValue}`;
  },
  {
    name: "save_preference",
    description: "Save user preference to Store",
    schema: z.object({
      preferenceKey: z.string(),
      preferenceValue: z.string(),
    }),
  }
);

```

--------------------------------

### Customize Supervisor Information Flow in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript code snippet demonstrates how to customize the information returned to the supervisor. It shows two options: returning a simple confirmation string or a JSON string with structured event data. This is useful for controlling the granularity of information passed up the agent hierarchy.

```typescript
import { tool } from "langchain/tools";
import { z } from "zod";

// Assuming calendarAgent is initialized elsewhere
// const calendarAgent = ...;

const scheduleEvent = tool(
  async ({ request }) => {
    const result = await calendarAgent.invoke({ // Replace with actual calendar agent invocation
      messages: [{ role: "user", content: request }]
    });

    const lastMessage = result.messages[result.messages.length - 1];

    // Option 1: Return just the confirmation message
    return lastMessage.text;

    // Option 2: Return structured data
    // return JSON.stringify({
    //   status: "success",
    //   event_id: "evt_123",
    //   summary: lastMessage.text
    // });
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### Invoke Model with Single Message (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates the simplest way to call a chat model using the `invoke()` method with a single string message. This is useful for straightforward, single-turn queries. No external dependencies beyond the Langchain model instance are required.

```typescript
const response = await model.invoke("Why do parrots have colorful feathers?");
console.log(response);
```

--------------------------------

### Process Tool Calls from AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to bind tools to a model and invoke it to potentially receive tool calls within the AIMessage response. It then iterates through the `tool_calls` array in the response to log the name, arguments, and ID of each tool call.

```typescript
const modelWithTools = model.bindTools([getWeather]);
const response = await modelWithTools.invoke("What's the weather in Paris?");

for (const toolCall of response.tool_calls) {
  console.log(`Tool: ${toolCall.name}`);
  console.log(`Args: ${toolCall.args}`);
  console.log(`ID: ${toolCall.id}`);
}
```

--------------------------------

### Initialize AWS Bedrock Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the BedrockEmbeddings class from the @langchain/aws package. Requires specifying the Bedrock model to use, such as 'amazon.titan-embed-text-v1'.

```typescript
import { BedrockEmbeddings } from "@langchain/aws";

const embeddings = new BedrockEmbeddings({
  model: "amazon.titan-embed-text-v1"
});
```

--------------------------------

### Initialize and Use InMemoryStore for Long-Term Memory

Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory

Demonstrates initializing an InMemoryStore for LangChain agents, putting data into the store with namespaces and keys, retrieving specific memories, and searching for memories based on content filters and vector similarity. This requires an embedding function and assumes a production environment would use a DB-backed store.

```typescript
import { InMemoryStore } from "@langchain/langgraph";

const embed = (texts: string[]): number[][] => {
    // Replace with an actual embedding function or LangChain embeddings object
    return texts.map(() => [1.0, 2.0]);
};

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
const store = new InMemoryStore({ index: { embed, dims: 2 } }); // [!code highlight]
const userId = "my-user";
const applicationContext = "chitchat";
const namespace = [userId, applicationContext]; // [!code highlight]

await store.put( // [!code highlight]
    namespace,
    "a-memory",
    {
        rules: [
            "User likes short, direct language",
            "User only speaks English & TypeScript",
        ],
        "my-key": "my-value",
    }
);

// get the "memory" by ID
const item = await store.get(namespace, "a-memory"); // [!code highlight]

// search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity
const items = await store.search( // [!code highlight]
    namespace,
    {
        filter: { "my-key": "my-value" },
        query: "language preferences"
    }
);

```

--------------------------------

### Define Agent with Multiple Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript code illustrates how to create an agent using `createAgent` and pass an array of middleware functions (`middleware1`, `middleware2`, `middleware3`) to control its behavior. The order of middleware in the array determines their execution sequence, with specific rules for `before_*`, `after_*`, and `wrap_*` hooks.

```typescript
const agent = createAgent({
  model: "gpt-4o",
  middleware: [middleware1, middleware2, middleware3],
  tools: [...],
});
```

--------------------------------

### Run Vitest/Jest Evaluations

Source: https://docs.langchain.com/oss/javascript/langchain/test

Execute your LangSmith-integrated tests using your preferred test runner. This command will run the evaluation tests and log the results to LangSmith if configured.

```bash
vitest run test_trajectory.eval.ts
# or
jest test_trajectory.eval.ts
```

--------------------------------

### Load PDF Document with PDFLoader in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Loads a PDF file into a sequence of Document objects. Each Document represents a page from the PDF and includes its content and metadata. Dependencies include the '@langchain/community/document_loaders/fs/pdf' package.

```typescript
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const loader = new PDFLoader("../../data/nke-10k-2023.pdf");

const docs = await loader.load();
console.log(docs.length);
```

--------------------------------

### Add Documents to Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This code snippet demonstrates how to add a list of document splits to an instantiated vector store. It assumes 'vectorStore' and 'allSplits' have been previously defined. The 'addDocuments' method is used for this operation.

```typescript
await vectorStore.addDocuments(allSplits);
```

--------------------------------

### Access Runtime Context in a Tool - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Shows how to create a tool that accesses agent runtime context. The `getUserName` tool retrieves the user's name from the `config.context`. It also includes setting up an agent with a context schema and invoking it with context.

```typescript
import * as z from "zod"
import { ChatOpenAI } from "@langchain/openai"
import { createAgent } from "langchain"

const getUserName = tool(
  (_, config) => {
    return config.context.user_name
  },
  {
    name: "get_user_name",
    description: "Get the user's name.",
    schema: z.object({}),
  }
);

const contextSchema = z.object({
  user_name: z.string(),
});

const agent = createAgent({
  model: new ChatOpenAI({ model: "gpt-4o" }),
  tools: [getUserName],
  contextSchema,
});

const result = await agent.invoke(
  {
    messages: [{ role: "user", content: "What is my name?" }]
  },
  {
    context: { user_name: "John Smith" }
  }
);
```

--------------------------------

### Initialize Cohere Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the CohereEmbeddings class from the @langchain/cohere package. Requires specifying the Cohere model, such as 'embed-english-v3.0'.

```typescript
import { CohereEmbeddings } from "@langchain/cohere";

const embeddings = new CohereEmbeddings({
  model: "embed-english-v3.0"
});
```

--------------------------------

### Configure AWS Bedrock Environment Variable

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the AWS region environment variable required for AWS Bedrock embeddings.

```bash
BEDROCK_AWS_REGION=your-region
```

--------------------------------

### Dynamic Response Format Selection based on Conversation State

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Implements middleware to dynamically select response formats based on the number of messages in a conversation. A simple format is used for early conversation stages (less than 3 messages), and a detailed format is used for established conversations.

```typescript
import { createMiddleware } from "langchain";
import { z } from "zod";

const simpleResponse = z.object({
  answer: z.string().describe("A brief answer"),
});

const detailedResponse = z.object({
  answer: z.string().describe("A detailed answer"),
  reasoning: z.string().describe("Explanation of reasoning"),
  confidence: z.number().describe("Confidence score 0-1"),
});

const stateBasedOutput = createMiddleware({
  name: "StateBasedOutput",
  wrapModelCall: (request, handler) => {
    // request.state is a shortcut for request.state.messages
    const messageCount = request.messages.length;  // [!code highlight]

    if (messageCount < 3) {
      // Early conversation - use simple format
      responseFormat = simpleResponse; // [!code highlight]
    } else {
      // Established conversation - use detailed format
      responseFormat = detailedResponse; // [!code highlight]
    }

    return handler({ ...request, responseFormat });
  },
});
```

--------------------------------

### Anthropic Prompt Caching Middleware for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Optimize Anthropic model usage by caching repetitive prompt prefixes to reduce costs and latency. This middleware is particularly beneficial for applications with consistent system prompts or agents that reuse context. It allows configuring the time-to-live (TTL) for cached content, with supported values like '5m' for 5 minutes or '1h' for 1 hour.

```typescript
import { createAgent, HumanMessage, anthropicPromptCachingMiddleware } from "langchain";

const LONG_PROMPT = `
Please be a helpful assistant.

<Lots more context ...>
`;

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  prompt: LONG_PROMPT,
  middleware: [anthropicPromptCachingMiddleware({ ttl: "5m" })],
});

// cache store
await agent.invoke({
  messages: [new HumanMessage("Hi, my name is Bob")]
});

// cache hit, system prompt is cached
const result = await agent.invoke({
  messages: [new HumanMessage("What's my name?")]
});
```

--------------------------------

### Manually Construct and Insert AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to manually create an AIMessage object and insert it into a list of messages. This is useful when you want to simulate a model response or control the message history explicitly. It also shows combining SystemMessage and HumanMessage with the manually created AIMessage for model invocation.

```typescript
import { AIMessage, SystemMessage, HumanMessage } from "langchain";

const aiMsg = new AIMessage("I'd be happy to help you with that question!");

const messages = [
  new SystemMessage("You are a helpful assistant"),
  new HumanMessage("Can you help me?"),
  aiMsg,  // Insert as if it came from the model
  new HumanMessage("Great! What's 2+2?")
]

const response = await model.invoke(messages);
```

--------------------------------

### Configure Tool Calling Strategy for Structured Response

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

The `toolStrategy` function configures LangChain to use tool calling for generating structured responses when native support is unavailable. It accepts a schema (Zod or JSON Schema) and optional configuration for tool messages and error handling. This function is crucial for enabling structured output with compatible language models.

```typescript
function toolStrategy<StructuredResponseT>(
    responseFormat:
        | JsonSchemaFormat
        | ZodSchema<StructuredResponseT>
        | (ZodSchema<StructuredResponseT> | JsonSchemaFormat)[]
    options?: ToolStrategyOptions
): ToolStrategy<StructuredResponseT>
```

--------------------------------

### Parsing AIMessage Content Blocks with Anthropic Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how an `AIMessage` generated with Anthropic's provider-native content (including 'thinking' and 'text' blocks) can be lazily parsed into LangChain's standard `contentBlocks` representation.

```typescript
import { AIMessage } from "@langchain/core/messages";

const message = new AIMessage({
  content: [
    {
      "type": "thinking",
      "thinking": "...",
      "signature": "WaUjzkyp...",
    },
    {
      "type":"text",
      "text": "...",
      "id": "msg_abc123",
    },
  ],
  response_metadata: { model_provider: "anthropic" },
});

console.log(message.contentBlocks);
```

--------------------------------

### ContentBlock.Reasoning

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents model reasoning steps.

```APIDOC
## ContentBlock.Reasoning

### Description
Model reasoning steps.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"reasoning"`
- **reasoning** (string) - Required - The reasoning content

### Request Example
```json
{
    "type": "reasoning",
    "reasoning": "The user is asking about..."
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"reasoning"`
- **reasoning** (string) - The reasoning content

#### Response Example
```json
{
    "type": "reasoning",
    "reasoning": "The user is asking about..."
}
```
```

--------------------------------

### Customize Tool Message Content in LangChain JavaScript

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Demonstrates how to use the 'toolMessageContent' parameter within the 'toolStrategy' function to customize the assistant's message when structured output is generated. This allows for more user-friendly feedback in the conversation history. It requires the 'langchain' library and 'zod' for schema definition.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const MeetingAction = z.object({
    task: z.string().describe("The specific task to be completed"),
    assignee: z.string().describe("Person responsible for the task"),
    priority: z.enum(["low", "medium", "high"]).describe("Priority level"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy(MeetingAction, {
        toolMessageContent: "Action item captured and added to meeting notes!"
    })
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "From our meeting: Sarah needs to update the project timeline as soon as possible"}]
});

console.log(result);
/**
 * { 
 *   messages: [
 *     { role: "user", content: "From our meeting: Sarah needs to update the project timeline as soon as possible" },
 *     { role: "assistant", content: "Action item captured and added to meeting notes!", tool_calls: [ { name: "MeetingAction", args: { task: "update the project timeline", assignee: "Sarah", priority: "high" }, id: "call_456" } ] },
 *     { role: "tool", content: "Action item captured and added to meeting notes!", tool_call_id: "call_456", name: "MeetingAction" }
 *   ],
 *   structuredResponse: { task: "update the project timeline", assignee: "Sarah", priority: "high" }
 * }
 */
```

--------------------------------

### Inject Compliance Rules Middleware for Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware injects compliance rules into the LLM prompt based on user jurisdiction and industry. It reads `userJurisdiction`, `industry`, and `complianceFrameworks` from the runtime context. Specific rules for frameworks like GDPR and HIPAA, or for industries like finance, are generated and appended as a user message to the conversation. Dependencies include 'langchain' and 'zod'.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userJurisdiction: z.string(),
  industry: z.string(),
  complianceFrameworks: z.array(z.string()),
});

type Context = z.infer<typeof contextSchema>;

const injectComplianceRules = createMiddleware<Context>({
  name: "InjectComplianceRules",
  contextSchema,
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: get compliance requirements
    const { userJurisdiction, industry, complianceFrameworks } = request.runtime.context;  // [!code highlight]

    // Build compliance constraints
    const rules = [];
    if (complianceFrameworks.includes("GDPR")) {
      rules.push("- Must obtain explicit consent before processing personal data");
      rules.push("- Users have right to data deletion");
    }
    if (complianceFrameworks.includes("HIPAA")) {
      rules.push("- Cannot share patient health information without authorization");
      rules.push("- Must use secure, encrypted communication");
    }
    if (industry === "finance") {
      rules.push("- Cannot provide financial advice without proper disclaimers");
    }

    if (rules.length > 0) {
      const complianceContext = `Compliance requirements for ${userJurisdiction}:
    ${rules.join("\n")}`;

      // Append at end - models pay more attention to final messages
      const messages = [
        ...request.messages,
        { role: "user", content: complianceContext }
      ];
      request = request.override({ messages });  // [!code highlight]
    }

    return handler(request);
  },
});
```

--------------------------------

### Create ToolMessage for Tool Calling in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates the process of creating an AIMessage with tool calls and a corresponding ToolMessage to return the result of a tool execution back to the model. This enables AI models to interact with external tools.

```typescript
import { AIMessage, ToolMessage, HumanMessage } from "langchain";

const aiMessage = new AIMessage({
  content: [],
  tool_calls: [{
    name: "get_weather",
    args: { location: "San Francisco" },
    id: "call_123"
  }]
});

const toolMessage = new ToolMessage({
  content: "Sunny, 72°F",
  tool_call_id: "call_123"
});

const messages = [
  new HumanMessage("What's the weather in San Francisco?"),
  aiMessage,  // Model's tool call
  toolMessage,  // Tool execution result
];

const response = await model.invoke(messages);  // Model processes the result
```

--------------------------------

### Create LLM-as-Judge Evaluator with Reference Trajectory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Configures the LLM-as-Judge evaluator to include a reference trajectory using `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE`. This allows for comparison against a known correct path. Dependencies include `agentevals`.

```typescript
import { TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE } from "agentevals";

const evaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,
});

const evaluation = await evaluator({
  outputs: result.messages,
  referenceOutputs: referenceTrajectory,
});
```

--------------------------------

### LangGraph Middleware with Runtime Context - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/runtime

This TypeScript code demonstrates how to use middleware in LangGraph to access runtime context. It includes dynamic prompt generation based on 'userName' and logging middleware to track request processing. The `runtime` parameter, typed with `z.infer<typeof contextSchema>`, allows access to context data like `userName`.

```typescript
import * as z from "zod";
import { createAgent, createMiddleware, type AgentState, SystemMessage } from "langchain";
import { type Runtime } from "@langchain/langgraph";

const contextSchema = z.object({
  userName: z.string(),
});

// Dynamic prompt middleware
const dynamicPromptMiddleware = createMiddleware({
  name: "DynamicPrompt",
  beforeModel: (state: AgentState, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    const userName = runtime.context?.userName;
    if (!userName) {
      throw new Error("userName is required");
    }

    const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`;
    return {
      messages: [new SystemMessage(systemMsg), ...state.messages]
    };
  }
});

// Logging middleware
const loggingMiddleware = createMiddleware({
  name: "Logging",
  beforeModel: (state: AgentState, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    console.log(`Processing request for user: ${runtime.context?.userName}`);
    return;
  },
  afterModel: (state: AgentState, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    console.log(`Completed request for user: ${runtime.context?.userName}`);
    return;
  }
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [
    /* ... */
  ],
  middleware: [dynamicPromptMiddleware, loggingMiddleware],
  contextSchema,
});

const result = await agent.invoke(
  { messages: [{ role: "user", content: "What's my name?" }] },
  { context: { userName: "John Smith" } }
);

```

--------------------------------

### Force Tool Usage in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to bind tools to a model in Langchain.js, specifying whether to force the use of any available tool or a specific one. This is useful for controlling model behavior when multiple tools are present.

```typescript
const modelWithTools = model.bindTools([tool_1], { toolChoice: "any" })
```

```typescript
const modelWithTools = model.bindTools([tool_1], { toolChoice: "tool_1" })
```

--------------------------------

### Tool Message Usage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to create and use `ToolMessage` objects, which are essential for passing the results of tool executions back to an AI model that supports tool calling.

```APIDOC
## Tool Message Usage

### Description
This documentation covers the creation and utilization of `ToolMessage` objects within Langchain. `ToolMessage` is used to convey the output of a tool's execution back to the model, enabling conversational AI agents to interact with external tools and utilize their results.

### Method
N/A (Client-side object creation and model invocation)

### Endpoint
N/A

### Parameters
#### Request Body (for `ToolMessage` constructor)
- **content** (string) - Required - The stringified output of the tool call.
- **tool_call_id** (string) - Required - The ID of the tool call that this message is responding to. This must match the ID of the tool call in the `AIMessage`.
- **name** (string) - Required - The name of the tool that was called.
- **artifact** (dict) - Optional - Additional data not sent to the model but accessible programmatically. Useful for storing raw results or metadata.

### Request Example
```typescript
import { AIMessage, ToolMessage, HumanMessage } from "langchain";

const aiMessage = new AIMessage({
  content: [],
  tool_calls: [{
    name: "get_weather",
    args: { location: "San Francisco" },
    id: "call_123"
  }]
});

const toolMessage = new ToolMessage({
  content: "Sunny, 72°F",
  tool_call_id: "call_123",
  name: "get_weather",
  artifact: { "source": "weather_api" } // Example artifact
});

const messages = [
  new HumanMessage("What's the weather in San Francisco?"),
  aiMessage,  // Model's tool call
  toolMessage,  // Tool execution result
];

// Assuming 'model' is an initialized Langchain model instance
// const response = await model.invoke(messages);
```

### Response
#### Success Response (from `model.invoke`)
- The structure of the response depends on the model's capabilities and the context of the invocation. It typically includes the model's final answer or subsequent actions.

#### Response Example
(Example shows the `ToolMessage` creation, the subsequent `model.invoke` response would vary)
```json
{
  "tool_call_id": "call_123",
  "content": "Sunny, 72°F",
  "name": "get_weather",
  "artifact": {
    "source": "weather_api"
  }
}
```

### Attributes
- **content** (`string`) - Required - The stringified output of the tool call.
- **tool_call_id** (`string`) - Required - The ID of the tool call that this message is responding to. (this must match the ID of the tool call in the [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html))
- **name** (`string`) - Required - The name of the tool that was called.
- **artifact** (`dict`) - Optional - Additional data not sent to the model but can be accessed programmatically. Useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.

#### Example: Using artifact for retrieval metadata

```typescript
import { ToolMessage } from "langchain";

// Artifact available downstream
const artifact = { document_id: "doc_123", page: 0 };

const toolMessage = new ToolMessage({
  content: "It was the best of times, it was the worst of times.",
  tool_call_id: "call_123",
  name: "search_books",
  artifact
});
```
```

--------------------------------

### Streaming AIMessageChunk Handling

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to process and combine incoming AIMessageChunk objects during a streaming operation to form a complete message.

```APIDOC
## Streaming AIMessageChunk Handling

### Description
This section explains how to handle `AIMessageChunk` objects received during streaming. It shows a pattern for combining sequential chunks into a single, complete message.

### Method
N/A (Client-side processing of stream data)

### Endpoint
N/A

### Parameters
N/A

### Request Example
N/A

### Response
#### Success Response
- **finalChunk** (`AIMessageChunk` | `undefined`) - The final combined `AIMessageChunk` object after processing all incoming chunks.

#### Response Example
```typescript
import { AIMessageChunk } from "langchain";

let finalChunk: AIMessageChunk | undefined;
for (const chunk of chunks) {
  finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;
}
```
```

--------------------------------

### Set Custom LangSmith Project Name (Static and Dynamic)

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Configure a custom project name for your LangSmith traces. This can be done statically by setting the LANGSMITH_PROJECT environment variable or dynamically within your code using `tracing_context`.

```bash
export LANGSMITH_PROJECT=my-agent-project
```

```python
import langsmith as ls

with ls.tracing_context(project_name="email-agent-test", enabled=True):
    response = agent.invoke({
        "messages": [{"role": "user", "content": "Send a welcome email"}]
    })
```

--------------------------------

### Add Metadata and Tags to LangSmith Traces

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Annotate your LangSmith traces with custom metadata and tags for better organization and filtering. This can be done directly in the `invoke` call or within the `tracing_context` manager.

```python
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Send a welcome email"}]},
    config={
        "tags": ["production", "email-assistant", "v1.0"],
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        }
    }
)
```

```python
with ls.tracing_context(
    project_name="email-agent-test",
    enabled=True,
    tags=["production", "email-assistant", "v1.0"],
    metadata={"user_id": "user_123", "session_id": "session_456", "environment": "production"}):
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "Send a welcome email"}]}
    )
```

--------------------------------

### Invoke Model with Dictionary Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates invoking a chat model using a list of dictionaries, following the OpenAI chat completions format. This method is convenient for specifying messages directly in a structured way.

```typescript
const messages = [
  { role: "system", content: "You are a poetry expert" },
  { role: "user", content: "Write a haiku about spring" },
  { role: "assistant", content: "Cherry blossoms bloom..." },
];
const response = await model.invoke(messages);
```

--------------------------------

### Handling Agent Interrupts with Langchain (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Demonstrates how to invoke a Langchain agent that may be interrupted, inspect the interrupt details including action requests and review configurations, and then resume the execution with a decision. Requires `@langchain/core/messages` and `@langchain/langgraph`.

```typescript
import { HumanMessage } from "@langchain/core/messages";
import { Command } from "@langchain/langgraph";

// You must provide a thread ID to associate the execution with a conversation thread,
// so the conversation can be paused and resumed (as is needed for human review).
const config = { configurable: { thread_id: "some_id" } }; // [!code highlight]

// Run the graph until the interrupt is hit.
const result = await agent.invoke(
    {
        messages: [new HumanMessage("Delete old records from the database")],
    },
    config // [!code highlight]
);


// The interrupt contains the full HITL request with action_requests and review_configs
console.log(result.__interrupt__);
// > [
// >    Interrupt(
// >       value: {
// >          action_requests: [
// >             {
// >                name: 'execute_sql',
// >                arguments: { query: 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \'30 days\';' },
// >                description: 'Tool execution pending approval\n\nTool: execute_sql\nArgs: {...}'
// >             }
// >          ],
// >          review_configs: [
// >             {
// >                action_name: 'execute_sql',
// >                allowed_decisions: ['approve', 'reject']
// >             }
// >          ]
// >       }
// >    )
// > ]

// Resume with approval decision
await agent.invoke(
    new Command({ // [!code highlight]
        resume: { decisions: [{ type: "approve" }] }, // or "edit", "reject" [!code highlight]
    }), // [!code highlight]
    config // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Augment Message with Retrieved Documents in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This JavaScript code snippet demonstrates a middleware function for a LangChain agent. It retrieves relevant documents using similarity search based on the last message's content and augments the message with the retrieved document content as context. This is a core part of implementing RAG.

```javascript
const lastMessage = state.messages[state.messages.length - 1].content;
      const retrievedDocs = await vectorStore.similaritySearch(lastMessage, 2);

      const docsContent = retrievedDocs
        .map((doc) => doc.pageContent)
        .join("\n\n");

      const augmentedMessageContent = [
          ...lastMessage.content,
          { type: "text", text: `Use the following context to answer the query:\n\n${docsContent}` }
      ]

      return {
        messages: [{
          ...lastMessage,
          content: augmentedMessageContent,
        }]
        context: retrievedDocs,
      }
```

--------------------------------

### Wrap-style Dynamic Model Selection Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Provides a Wrap-style middleware for dynamically selecting the LLM model based on conversation length. The `wrapModelCall` hook inspects the number of messages in the request and switches the model between 'gpt-4o' and 'gpt-4o-mini' accordingly. This optimizes cost and performance.

```typescript
import { createMiddleware, initChatModel } from "langchain";

const dynamicModelMiddleware = createMiddleware({
  name: "DynamicModelMiddleware",
  wrapModelCall: (request, handler) => {
    // Use different model based on conversation length
    const modifiedRequest = { ...request };
    if (request.messages.length > 10) {
      modifiedRequest.model = initChatModel("gpt-4o");
    } else {
      modifiedRequest.model = initChatModel("gpt-4o-mini");
    }
    return handler(modifiedRequest);
  },
});
```

--------------------------------

### Configure Human-in-the-Loop Middleware for Agents

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This code configures LangChain.js agents with human-in-the-loop middleware. It specifies which tool calls should trigger an interruption, allowing for manual approval, editing, or rejection. A checkpointer is added to the supervisor agent to enable pausing and resuming execution.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: CALENDAR_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { create_calendar_event: true },
      descriptionPrefix: "Calendar event pending approval",
    }),
  ],
});

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: EMAIL_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { send_email: true },
      descriptionPrefix: "Outbound email pending approval",
    }),
  ],
});

const supervisorAgent = createAgent({
  model: llm,
  tools: [scheduleEvent, manageEmail],
  systemPrompt: SUPERVISOR_PROMPT,
  checkpointer: new MemorySaver(),
});
```

--------------------------------

### System Message: Detailed Persona (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates creating a SystemMessage to define a detailed persona for the model, including expertise and response guidelines. This allows for more specific and tailored interactions.

```typescript
import { SystemMessage, HumanMessage } from "langchain";

const systemMsg = new SystemMessage(`
You are a senior TypeScript developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
`);

const messages = [
  systemMsg,
  new HumanMessage("How do I create a REST API?"),
];
const response = await model.invoke(messages);
```

--------------------------------

### Similarity Search with Scores

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This snippet illustrates how to search a vector store for documents similar to a string query and also return the similarity scores. The output contains a score and the corresponding Document object.

```typescript
const results2 = await vectorStore.similaritySearchWithScore(
  "What was Nike's revenue in 2023?"
);

console.log(results2[0]);
```

--------------------------------

### Multiple Streaming Modes in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

Shows how to enable multiple streaming modes simultaneously by passing an array to the 'streamMode' option. This allows for flexibility in receiving different types of streaming output, such as 'updates', 'messages', and 'custom' data.

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const [streamMode, chunk] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: ["updates", "messages", "custom"] }
)) {
    console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);
}
```

--------------------------------

### Inject File Context Middleware for Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware injects context about uploaded files into the LLM's message history. It retrieves file details from the request state and formats them into a user message, prepended to the existing conversation. This is useful for agents that need to reference specific documents during a conversation. Dependencies include the 'langchain' library.

```typescript
import { createMiddleware } from "langchain";

const injectFileContext = createMiddleware({
  name: "InjectFileContext",
  wrapModelCall: (request, handler) => {
    // request.state is a shortcut for request.state.messages
    const uploadedFiles = request.state.uploadedFiles || [];  // [!code highlight]

    if (uploadedFiles.length > 0) {
      // Build context about available files
      const fileDescriptions = uploadedFiles.map(file =>
        `- ${file.name} (${file.type}): ${file.summary}`
      );

      const fileContext = `Files you have access to in this conversation:
    ${fileDescriptions.join("\n")} 

    Reference these files when answering questions.`;

      // Inject file context before recent messages
      const messages = [  // [!code highlight]
        ...request.messages  // Rest of conversation
        { role: "user", content: fileContext }
      ];
      request = request.override({ messages });  // [!code highlight]
    }

    return handler(request);
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [injectFileContext],
});
```

--------------------------------

### Parsing AIMessage Content Blocks with OpenAI Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how an `AIMessage` originating from the OpenAI provider, containing 'reasoning' and 'text' blocks, can be accessed via LangChain's standard `contentBlocks` property.

```typescript
import { AIMessage } from "@langchain/core/messages";

const message = new AIMessage({
  content: [
    {
      "type": "reasoning",
      "id": "rs_abc123",
      "summary": [
        {"type": "summary_text", "text": "summary 1"},
        {"type": "summary_text", "text": "summary 2"},
      ],
    },
    {"type": "text", "text": "..."},
  ],
  response_metadata: { model_provider: "openai" },
});

console.log(message.contentBlocks);
```

--------------------------------

### Inspect Captured Interrupt Events (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Iterates through a list of captured interrupt events and logs details about each action request. This includes the interrupt ID and a description of the request, along with the specific tool and arguments that triggered the interruption. This is useful for debugging and understanding why an agent was paused.

```typescript
for (const interrupt of interrupts) {
  for (const request of interrupt.value.actionRequests) {
    console.log(`INTERRUPTED: ${interrupt.id}`);
    console.log(`${request.description}\n`);
  }
}
```

--------------------------------

### Selective Tracing with Python's tracing_context

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Use LangSmith's `tracing_context` manager in Python to selectively enable or disable tracing for specific parts of your application. Code within the `with ls.tracing_context(enabled=True):` block will be traced.

```python
import langsmith as ls

# This WILL be traced
with ls.tracing_context(enabled=True):
    agent.invoke({"messages": [{"role": "user", "content": "Send a test email to alice@example.com"}]})

# This will NOT be traced (if LANGSMITH_TRACING is not set)
agent.invoke({"messages": [{"role": "user", "content": "Send another email"}]})
```

--------------------------------

### Stream Reasoning Output from LangChain.js Models

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to stream reasoning steps from a LangChain.js model when processing a query. It iterates through the streamed chunks, filters for content blocks of type 'reasoning', and logs them or the chunk's text if no reasoning steps are present. This requires the underlying model to support and expose reasoning capabilities.

```typescript
const stream = model.stream("Why do parrots have colorful feathers?");
for await (const chunk of stream) {
    const reasoningSteps = chunk.contentBlocks.filter(b => b.type === "reasoning");
    console.log(reasoningSteps.length > 0 ? reasoningSteps : chunk.text);
}
```

--------------------------------

### Stream LLM Tokens with Messages Mode in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

This code snippet illustrates how to stream tokens as they are produced by the LLM using LangChain.js with `streamMode: 'messages'`. It captures the token content and associated metadata, such as the LangGraph node. Dependencies include `zod` and `langchain`.

```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const [token, metadata] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "messages" }
)) {
    console.log(`node: ${metadata.langgraph_node}`);
    console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);
}
```

--------------------------------

### ContentBlock.Multimodal.File

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents generic file data (e.g., PDF), which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.File

### Description
Generic files (PDF, etc).

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"file"`
- **url** (string) - Optional - URL pointing to the file location.
- **data** (string) - Optional - Base64-encoded file data.
- **fileId** (string) - Optional - Reference ID to an externally stored file.
- **mimeType** (string) - Optional - File MIME type (e.g., `application/pdf`)

### Request Example
```json
{
    "type": "file",
    "url": "http://example.com/document.pdf",
    "mimeType": "application/pdf"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"file"`
- **url** (string) - URL pointing to the file location.
- **data** (string) - Base64-encoded file data.
- **fileId** (string) - Reference ID to an externally stored file.
- **mimeType** (string) - File MIME type

#### Response Example
```json
{
    "type": "file",
    "url": "http://example.com/document.pdf",
    "mimeType": "application/pdf"
}
```
```

--------------------------------

### Configuring Base URL for OpenAI-Compatible APIs in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to configure a custom base URL for chat model integrations in Langchain JS, enabling the use of OpenAI-compatible APIs or proxy servers. This is done by passing the `baseUrl` parameter to `initChatModel`.

```python
model = initChatModel(
    "MODEL_NAME",
    {
        modelProvider: "openai",
        baseUrl: "BASE_URL",
        apiKey: "YOUR_API_KEY",
    }
)
```

--------------------------------

### Define schedule_event tool with customizable return format in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Creates a tool for scheduling calendar events using LangChain agents. The function invokes a calendar agent, extracts the last message, and can return either a plain text confirmation or a JSON payload. Dependencies include the LangChain tool utilities and Zod for schema validation; inputs are a natural‑language request string, and the output is either a string or JSON string.

```TypeScript
const scheduleEvent = tool(
  async ({ request }) => {
    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: request }]
    });

    const lastMessage = result.messages[result.messages.length - 1];

    // Option 1: Return just the confirmation message
    return lastMessage.text;

    // Option 2: Return structured data
    // return JSON.stringify({
    //   status: "success",
    //   event_id: "evt_123",
    //   summary: lastMessage.text
    // });
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### LangSmith Evaluate Function for Dataset-Based Evaluation

Source: https://docs.langchain.com/oss/javascript/langchain/test

Utilize the `evaluate` function from LangSmith for running evaluations on datasets stored in LangSmith. This approach simplifies the process of evaluating an agent against a predefined dataset and automatically logs results. It requires the `langsmith/evaluation` and `agentevals` packages.

```typescript
import { evaluate } from "langsmith/evaluation";
import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";

const trajectoryEvaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT,
});

async function runAgent(inputs: any) {
  const result = await agent.invoke(inputs);
  return result.messages;
}

await evaluate(
  runAgent,
  {
    data: "your_dataset_name",
    evaluators: [trajectoryEvaluator],
  }
);
```

--------------------------------

### Invoke Model with Conversation History (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to invoke a chat model with a list of messages representing a conversation history. The model uses message roles (system, user, assistant) to understand context. This requires constructing a message array, either as objects or specific message classes.

```typescript
const conversation = [
  { role: "system", content: "You are a helpful assistant that translates English to French." },
  { role: "user", content: "Translate: I love programming." },
  { role: "assistant", content: "J'adore la programmation." },
  { role: "user", content: "Translate: I love building applications." },
];

const response = await model.invoke(conversation);
console.log(response);  // AIMessage("J'adore créer des applications.")
```

```typescript
import { HumanMessage, AIMessage, SystemMessage } from "langchain";

const conversation = [
  new SystemMessage("You are a helpful assistant that translates English to French."),
  new HumanMessage("Translate: I love programming."),
  new AIMessage("J'adore la programmation."),
  new HumanMessage("Translate: I love building applications."),
];

const response = await model.invoke(conversation);
console.log(response);  // AIMessage("J'adore créer des applications.")
```

--------------------------------

### Runtime Context-Based Tool Selection Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware filters tools based on user roles or permissions defined in the Runtime Context. It allows for role-based access control, granting different sets of tools to different user roles (e.g., admin, editor).

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.string(),
});

const contextBasedTools = createMiddleware({
  name: "ContextBasedTools",
  contextSchema,
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: get user role
    const userRole = request.runtime.context.userRole;  // [!code highlight]

    let filteredTools = request.tools;

    if (userRole === "admin") {
      // Admins get all tools
    } else if (userRole === "editor") {
      filteredTools = request.tools.filter(t => t.name !== "delete_data");  // [!code highlight]
    } else {
      filteredTools = request.tools.filter(t => t.name.startsWith("read_"));  // [!code highlight]
    }

    return handler({ ...request, tools: filteredTools });  // [!code highlight]
  },
});
```

--------------------------------

### Customizing Agent Memory with Extended State Schema (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This snippet illustrates how to extend the default agent state schema to include custom fields like `userId` and `preferences`. It uses Zod for schema definition and `createMiddleware` to integrate the custom state into the agent.

```typescript
import * as z from "zod";
import { createAgent, createMiddleware } from "langchain";
import { MessagesZodState, MemorySaver } from "@langchain/langgraph";

const customStateSchema = z.object({
    messages: MessagesZodState.shape.messages,
    userId: z.string(),
    preferences: z.record(z.string(), z.any()),
});

const stateExtensionMiddleware = createMiddleware({
    name: "StateExtension",
    stateSchema: customStateSchema,
});

const checkpointer = new MemorySaver();
const agent = createAgent({
    model: "gpt-5",
    tools: [],
    middleware: [stateExtensionMiddleware] as const,
    checkpointer,
});

// Custom state can be passed in invoke
const result = await agent.invoke({
    messages: [{ role: "user", content: "Hello" }],
    userId: "user_123",
    preferences: { theme: "dark" },
});
```

--------------------------------

### Approve Tool Call with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Use the 'approve' decision type to execute a tool call as-is. This method is suitable when the agent's proposed tool call requires no modifications. It takes a configuration object that includes the 'decisions' array with an 'approve' type.

```typescript
await agent.invoke(
    new Command({
        // Decisions are provided as a list, one per action under review.
        // The order of decisions must match the order of actions
        // listed in the `__interrupt__` request.
        resume: {
            decisions: [
                {
                    type: "approve",
                }
            ]
        }
    }),
    config  // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Create LLM-as-Judge Evaluator without Reference Trajectory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Creates an LLM-as-Judge evaluator using `createTrajectoryLLMAsJudge` without providing a reference trajectory. It takes an LLM model and a prompt as input and evaluates agent outputs. Dependencies include `langchain`, `@langchain/core/tools`, `@langchain/core/messages`, `agentevals`, and `zod`.

```typescript
import { createAgent } from "langchain"
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather]
});

const evaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT,
});

async function testTrajectoryQuality() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in Seattle?")]
  });

  const evaluation = await evaluator({
    outputs: result.messages,
  });
  // {
  //     'key': 'trajectory_accuracy',
  //     'score': true,
  //     'comment': 'The provided agent trajectory is reasonable...'
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Invoke Model with Simple Text Prompt (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to invoke a chat model with a single string, suitable for straightforward generation tasks without requiring conversation history.

```typescript
const response = await model.invoke("Write a haiku about spring");
```

--------------------------------

### Representing Text Content in Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the structure for documenting text-based content, supporting plain text and markdown. It includes fields for content type, the text itself, an optional title, and MIME type.

```typescript
{
  type: "text-plain",
  text: "The text content",
  title?: "Title of the text content",
  mimeType?: "text/plain" | "text/markdown"
}
```

--------------------------------

### Define Agent with Response Format Type

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This snippet shows the basic type definition for `responseFormat` in LangChain's `createAgent`. It accepts either a Zod schema or a JSON schema, allowing agents to handle structured data.

```typescript
type ResponseFormat = (
    | ZodSchema<StructuredResponseT> // a Zod schema
    | Record<string, unknown> // a JSON Schema
)

const agent = createAgent({
    // ...
    responseFormat: ResponseFormat | ResponseFormat[]
})
```

--------------------------------

### ContentBlock.Text

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents standard text output with optional annotations.

```APIDOC
## ContentBlock.Text

### Description
Standard text output.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"text"`
- **text** (string) - Required - The text content
- **annotations** (Citation[]) - Optional - List of annotations for the text

### Request Example
```json
{
    "type": "text",
    "text": "Hello world",
    "annotations": []
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"text"`
- **text** (string) - The text content
- **annotations** (Citation[]) - List of annotations for the text

#### Response Example
```json
{
    "type": "text",
    "text": "Hello world",
    "annotations": []
}
```
```

--------------------------------

### Custom Tool Error Handling Middleware in Javascript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Illustrates creating custom middleware to handle errors that occur during tool execution. It catches exceptions and returns a specific `ToolMessage` with a user-friendly error to the model. Requires 'langchain' package.

```typescript
import { createAgent, createMiddleware, ToolMessage } from "langchain";

const handleToolErrors = createMiddleware({
  name: "HandleToolErrors",
  wrapToolCall: (request, handler) => {
    try {
      return handler(request);
    } catch (error) {
      // Return a custom error message to the model
      return new ToolMessage({
        content: `Tool error: Please check your input and try again. (${error})`,
        tool_call_id: request.toolCall.id!,
      });
    }
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [
    /* ... */
  ],
  middleware: [handleToolErrors] as const,
});
```

--------------------------------

### Similarity Search with Embedded Query

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This code demonstrates searching a vector store using an embedded query (vector representation of a question). It requires an embeddings object to generate the query vector and then uses 'similaritySearchVectorWithScore' to find the most similar document.

```typescript
const embedding = await embeddings.embedQuery(
  "How were Nike's margins impacted in 2023?"
);

const results3 = await vectorStore.similaritySearchVectorWithScore(
  embedding,
  1
);

console.log(results3[0]);
```

--------------------------------

### Import and Use Text and Image Content Blocks in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to import and instantiate Text and Image ContentBlock types from the 'langchain' library in TypeScript. These blocks are used to represent different types of content within messages.

```typescript
import { ContentBlock } from "langchain";

// Text block
const textBlock: ContentBlock.Text = {
    type: "text",
    text: "Hello world",
}

// Image block
const imageBlock: ContentBlock.Multimodal.Image = {
    type: "image",
    url: "https://example.com/image.png",
    mimeType: "image/png",
}
```

--------------------------------

### Implement Early Exit Middleware with Jump Targets in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript snippet shows how to implement middleware that can exit the agent's execution early using the `jumpTo` property. The `earlyExitMiddleware` uses the `beforeModel` hook to check a condition; if `shouldExit(state)` is true, it returns an object with `jumpTo: "end"` and potentially modified messages. Available jump targets include `"end"`, `"tools"`, and `"model"`.

```typescript
import { createMiddleware, AIMessage } from "langchain";

const earlyExitMiddleware = createMiddleware({
  name: "EarlyExitMiddleware",
  beforeModel: (state) => {
    // Check some condition
    if (shouldExit(state)) {
      return {
        messages: [new AIMessage("Exiting early due to condition.")],
        jumpTo: "end",
      };
    }
    return;
  },
});
```

--------------------------------

### Select Model Based on Conversation Length (State Middleware)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware dynamically selects a language model based on the number of messages in the conversation state. It initializes three models (large, standard, efficient) and chooses one based on whether the message count exceeds 20, 10, or falls below. This is useful for managing costs and performance in long-running conversations.

```typescript
import { createMiddleware, initChatModel } from "langchain";

// Initialize models once outside the middleware
const largeModel = initChatModel("claude-sonnet-4-5-20250929");
const standardModel = initChatModel("gpt-4o");
const efficientModel = initChatModel("gpt-4o-mini");

const stateBasedModel = createMiddleware({
  name: "StateBasedModel",
  wrapModelCall: (request, handler) => {
    // request.messages is a shortcut for request.state.messages
    const messageCount = request.messages.length;  // [!code highlight]
    let model;

    if (messageCount > 20) {
      model = largeModel;
    } else if (messageCount > 10) {
      model = standardModel;
    } else {
      model = efficientModel;
    }

    return handler({ ...request, model });  // [!code highlight]
  },
});
```

--------------------------------

### Enabling Log Probabilities in Langchain JS ChatOpenAI

Source: https://docs.langchain.com/oss/javascript/langchain/models

Illustrates how to enable token-level log probabilities for the ChatOpenAI model in Langchain JS. This is achieved by setting the `logprobs` parameter to `true` during model initialization. The log probabilities can then be accessed from the response metadata.

```typescript
const model = new ChatOpenAI({
    model: "gpt-4o",
    logprobs: true,
});

const responseMessage = await model.invoke("Why do parrots talk?");

responseMessage.response_metadata.logprobs.content.slice(0, 5);
```

--------------------------------

### Human Message: Using String Shortcut (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates a shortcut for using HumanMessage by passing a simple string directly to the model's invoke method. This is equivalent to using a HumanMessage object with text content.

```typescript
const response = await model.invoke("What is machine learning?");
```

--------------------------------

### Executing Tool Calls with Arguments

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to represent a function call to a tool, including the tool's name, arguments, and a unique identifier for the call. This is used for invoking tools within the Langchain ecosystem.

```typescript
{
  type: "tool_call",
  name: "search",
  args: { query: "weather" },
  id: "call_123"
}
```

--------------------------------

### Wrap-style Tool Call Monitoring Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

A Wrap-style middleware for monitoring tool calls. The `wrapToolCall` hook logs the tool name and arguments before executing the tool, and logs success or failure messages afterwards. This is useful for debugging and auditing tool usage within an agent.

```typescript
import { createMiddleware } from "langchain";

const toolMonitoringMiddleware = createMiddleware({
  name: "ToolMonitoringMiddleware",
  wrapToolCall: (request, handler) => {
    console.log(`Executing tool: ${request.toolCall.name}`);
    console.log(`Arguments: ${JSON.stringify(request.toolCall.args)}`);

    try {
      const result = handler(request);
      console.log("Tool completed successfully");
      return result;
    } catch (e) {
      console.log(`Tool failed: ${e}`);
      throw e;
    }
  },
});
```

--------------------------------

### Node-style Logging Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implements Node-style hooks for logging model calls. It uses `createMiddleware` to define `beforeModel` and `afterModel` hooks that log the number of messages before a model call and the content of the last model response after the call. This is useful for monitoring and debugging agent behavior.

```typescript
import { createMiddleware } from "langchain";

const loggingMiddleware = createMiddleware({
  name: "LoggingMiddleware",
  beforeModel: (state) => {
    console.log(`About to call model with ${state.messages.length} messages`);
    return;
  },
  afterModel: (state) => {
    const lastMessage = state.messages[state.messages.length - 1];
    console.log(`Model returned: ${lastMessage.content}`);
    return;
  },
});
```

--------------------------------

### Configure PII Redaction and Masking Middleware in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

This snippet demonstrates how to configure LangChain.js middleware for PII detection. It shows how to redact emails, mask credit card numbers, and block API keys using different strategies. The middleware processes both user input and AI output.

```typescript
import { createAgent, piiRedactionMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [customerServiceTool, emailTool],
  middleware: [
    // Redact emails in user input before sending to model
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToInput: true,
    }),
    // Mask credit cards in user input
    piiRedactionMiddleware({
      piiType: "credit_card",
      strategy: "mask",
      applyToInput: true,
    }),
    // Block API keys - raise error if detected
    piiRedactionMiddleware({
      piiType: "api_key",
      detector: /sk-[a-zA-Z0-9]{32}/,
      strategy: "block",
      applyToInput: true,
    }),
  ],
});

// When user provides PII, it will be handled according to the strategy
const result = await agent.invoke({
  messages: [{
    role: "user",
    content: "My email is john.doe@example.com and card is 4532-1234-5678-9010"
  }]
});

```

--------------------------------

### ContentBlock.Tools.ServerToolCall

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents a tool call intended for server-side execution, including its identifier, name, and arguments.

```APIDOC
## ServerToolCall Content Block

### Description
Represents a tool call intended for server-side execution. Includes an identifier, the tool's name, and its arguments.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"server_tool_call"`
- **id** (string) - Required - An identifier associated with the tool call
- **name** (string) - Required - The name of the tool to be called
- **args** (string) - Required - Partial tool arguments (may be incomplete JSON)

### Request Example
```json
{
  "type": "server_tool_call",
  "id": "server_call_abc",
  "name": "database_query",
  "args": "{\"sql\": \"SELECT * FROM users\""
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`server_tool_call`)
- **id** (string) - Identifier for the tool call
- **name** (string) - Name of the tool called
- **args** (string) - Partial tool arguments

#### Response Example
```json
{
  "type": "server_tool_call",
  "id": "server_call_abc",
  "name": "database_query",
  "args": "{\"sql\": \"SELECT * FROM users\""
}
```
```

--------------------------------

### Provider Strategy Function Signature

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Illustrates the function signature for `providerStrategy`, which is used to define the structured output schema for models that support native structured output. It accepts a Zod schema or a JSON schema.

```typescript
function providerStrategy<StructuredResponseT>(
    schema: ZodSchema<StructuredResponseT> | JsonSchemaFormat
): ProviderStrategy<StructuredResponseT>
```

--------------------------------

### Handle Multiple Tool Call Decisions in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

When multiple actions are pending review, provide a decision for each in the order they appear in the interrupt request. This ensures that each tool call is addressed correctly, whether it's approved, edited, or rejected.

```typescript
{
    decisions: [
        { type: "approve" },
        {
            type: "edit",
            editedAction: {
                name: "tool_name",
                args: { param: "new_value" }
            }
        },
        {
            type: "reject",
            message: "This action is not allowed"
        }
    ]
}
```

--------------------------------

### State-Based Tool Selection Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware dynamically filters tools based on the conversation state, such as authentication status and message count. It ensures sensitive tools are only available after authentication and limits access to certain tools during the initial conversation stages.

```typescript
import { createMiddleware } from "langchain";

const stateBasedTools = createMiddleware({
  name: "StateBasedTools",
  wrapModelCall: (request, handler) => {
    // Read from State: check authentication and conversation length
    const state = request.state;  // [!code highlight]
    const isAuthenticated = state.authenticated || false;  // [!code highlight]
    const messageCount = state.messages.length;

    let filteredTools = request.tools;

    // Only enable sensitive tools after authentication
    if (!isAuthenticated) {
      filteredTools = request.tools.filter(t => t.name.startsWith("public_"));  // [!code highlight]
    } else if (messageCount < 5) {
      filteredTools = request.tools.filter(t => t.name !== "advanced_search");  // [!code highlight]
    }

    return handler({ ...request, tools: filteredTools });  // [!code highlight]
  },
});
```

--------------------------------

### Custom Tool Updates Streaming in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

Demonstrates how to stream custom updates from tools during execution using the 'writer' parameter in Langchain.js. This allows for real-time feedback on tool operations within a LangGraph context. Ensure a writer function is provided when invoking tools outside of LangGraph.

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "custom" }
)) {
    console.log(chunk);
}
```

--------------------------------

### Audio Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Outlines the structure for an audio content block. The 'type' must be 'audio', and it supports specifying the audio source through 'url', 'data' (Base64), or 'fileId', along with the 'mimeType'.

```typescript
{
  type: "audio",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Configure Model Fallback Middleware in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This middleware enables automatic fallback to alternative LLMs when the primary model fails. It accepts a variable number of fallback model strings in order. This is useful for building resilient agents and cost optimization.

```typescript
import { createAgent, modelFallbackMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o", // Primary model
  tools: [...],
  middleware: [
    modelFallbackMiddleware(
      "gpt-4o-mini", // Try first on error
      "claude-3-5-sonnet-20241022" // Then this
    ),
  ],
});
```

--------------------------------

### Invoke Model with Message Objects (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates invoking a chat model with a list of message objects, including SystemMessage, HumanMessage, and AIMessage. This approach is ideal for managing multi-turn conversations and multimodal content.

```typescript
import { SystemMessage, HumanMessage, AIMessage } from "langchain";

const messages = [
  new SystemMessage("You are a poetry expert"),
  new HumanMessage("Write a haiku about spring"),
  new AIMessage("Cherry blossoms bloom..."),
];
const response = await model.invoke(messages);
```

--------------------------------

### ContentBlock.Multimodal.Video

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents video data, which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.Video

### Description
Video data.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"video"`
- **url** (string) - Optional - URL pointing to the video location.
- **data** (string) - Optional - Base64-encoded video data.
- **fileId** (string) - Optional - Reference ID to an externally stored video file.
- **mimeType** (string) - Optional - Video MIME type (e.g., `video/mp4`, `video/webm`)

### Request Example
```json
{
    "type": "video",
    "url": "http://example.com/video.mp4",
    "mimeType": "video/mp4"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"video"`
- **url** (string) - URL pointing to the video location.
- **data** (string) - Base64-encoded video data.
- **fileId** (string) - Reference ID to an externally stored video file.
- **mimeType** (string) - Video MIME type

#### Response Example
```json
{
    "type": "video",
    "url": "http://example.com/video.mp4",
    "mimeType": "video/mp4"
}
```
```

--------------------------------

### Access PDF Document Content and Metadata in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Demonstrates how to access the page content and metadata from a loaded PDF Document. The metadata includes source file, page number, and PDF specific information. This is useful for inspecting individual document chunks.

```typescript
console.log(docs[0].pageContent.slice(0, 200));
```

```typescript
console.log(docs[0].metadata);
```

--------------------------------

### Edit Email Subject in LangGraph.js using Command

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This snippet demonstrates how to intercept and modify agent actions, specifically editing the subject of an outbound email, by utilizing the `Command` class to specify decisions for interrupts. It processes a list of interrupts, identifies the 'send_email' action, modifies its subject, and then resumes the agent's execution with the specified edits. Dependencies include `@langchain/langgraph`.

```typescript
import { Command } from "@langchain/langgraph"; // [!code highlight]

const resume: Record<string, any> = {};
for (const interrupt of interrupts) {
  const actionRequest = interrupt.value.actionRequests[0];
  if (actionRequest.name === "send_email") {
    // Edit email
    const editedAction = { ...actionRequest };
    editedAction.arguments.subject = "Mockups reminder";
    resume[interrupt.id] = {
      decisions: [{ type: "edit", editedAction }]
    };
  } else {
    resume[interrupt.id] = { decisions: [{ type: "approve" }] };
  }
}

const resumeStream = await supervisorAgent.stream(
  new Command({ resume }), // [!code highlight]
  config
);

for await (const step of resumeStream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Stream Agent Execution and Capture Interrupts

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript code snippet shows how to stream agent execution with a user query and collect any interrupt events that occur. It iterates through the streamed updates, logs messages, and stores interrupt details for later inspection. The `config` object includes `thread_id` for managing conversational state.

```typescript
const query =
  "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
  "and send them an email reminder about reviewing the new mockups.";

const config = { configurable: { thread_id: "6" } };

const interrupts: any[] = [];
const stream = await supervisorAgent.stream(
  { messages: [{ role: "user", content: query }] },
  config
);

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    } else if (Array.isArray(update)) {
      const interrupt = update[0];
      interrupts.push(interrupt);
      console.log(`\nINTERRUPTED: ${interrupt.id}`);
    }
  }
}
```

--------------------------------

### Generate Structured Output with JSON Schema in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet shows how to use a raw JSON Schema to define the desired output structure for a LangChain model. This method offers maximum control and interoperability. It requires specifying the 'jsonSchema' method.

```typescript
const jsonSchema = {
  "title": "Movie",
  "description": "A movie with details",
  "type": "object",
  "properties": {
    "title": {
      "type": "string",
      "description": "The title of the movie",
    },
    "year": {
      "type": "integer",
      "description": "The year the movie was released",
    },
    "director": {
      "type": "string",
      "description": "The director of the movie",
    },
    "rating": {
      "type": "number",
      "description": "The movie's rating out of 10",
    },
  },
  "required": ["title", "year", "director", "rating"],
}

const modelWithStructure = model.withStructuredOutput(
  jsonSchema,
  { method: "jsonSchema" },
)

const response = await modelWithStructure.invoke("Provide details about the movie Inception")
console.log(response)  // {'title': 'Inception', 'year': 2010, ...}
```

--------------------------------

### LangChain.js Handle Multiple Exception Types in Structured Output

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This snippet demonstrates how to handle multiple distinct exception types within the `handleError` function for structured output. It allows you to define specific responses for different error categories, such as validation errors and custom user errors, providing tailored feedback for each scenario.

```typescript
const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        if (error instanceof CustomUserError) {
        return "This is a custom user error.";
        }
        return error.message;
    }
)
```

--------------------------------

### ContentBlock.Multimodal.Audio

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents audio data, which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.Audio

### Description
Audio data.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"audio"`
- **url** (string) - Optional - URL pointing to the audio location.
- **data** (string) - Optional - Base64-encoded audio data.
- **fileId** (string) - Optional - Reference ID to an externally stored audio file.
- **mimeType** (string) - Optional - Audio MIME type (e.g., `audio/mpeg`, `audio/wav`)

### Request Example
```json
{
    "type": "audio",
    "url": "http://example.com/audio.mp3",
    "mimeType": "audio/mpeg"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"audio"`
- **url** (string) - URL pointing to the audio location.
- **data** (string) - Base64-encoded audio data.
- **fileId** (string) - Reference ID to an externally stored audio file.
- **mimeType** (string) - Audio MIME type

#### Response Example
```json
{
    "type": "audio",
    "url": "http://example.com/audio.mp3",
    "mimeType": "audio/mpeg"
}
```
```

--------------------------------

### Custom Agent State Schema for Memory in LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/agents

This code illustrates how to define a custom state schema for a LangChain agent using Zod, enabling it to remember additional information beyond standard messages. It integrates `MessagesZodState` and defines `userPreferences` as a string-to-string record.

```typescript
import * as z from "zod";
import { MessagesZodState } from "@langchain/langgraph";
import { createAgent, type BaseMessage } from "langchain";

const customAgentState = z.object({
  messages: MessagesZodState.shape.messages,
  userPreferences: z.record(z.string(), z.string()),
});

const CustomAgentState = createAgent({
  model: "gpt-4o",
  tools: [],
  stateSchema: customAgentState,
});
```

--------------------------------

### Access Runtime in Tool for Context and Memory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/runtime

Illustrates how to access the `Runtime` object within a LangChain tool. This allows tools to read contextual information (like `userName`) and interact with the long-term memory store.

```typescript
import * as z from "zod";
import { tool } from "langchain";
import { type Runtime } from "@langchain/langgraph";

const contextSchema = z.object({
  userName: z.string(),
});

const fetchUserEmailPreferences = tool(
  async (_, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    const userName = runtime.context?.userName;
    if (!userName) {
      throw new Error("userName is required");
    }

    let preferences = "The user prefers you to write a brief and polite email.";
    if (runtime.store) {
      const memory = await runtime.store?.get(["users"], userName);
      if (memory) {
        preferences = memory.value.preferences;
      }
    }
    return preferences;
  },
  {
    name: "fetch_user_email_preferences",
    description: "Fetch the user's email preferences.",
    schema: z.object({}),
  }
);
```

--------------------------------

### Define Rate Limiting Middleware with Context Schema in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This snippet demonstrates how to create a middleware function in TypeScript that enforces rate limiting. It defines a context schema using Zod for validating required configuration like `maxRequestsPerMinute` and `apiKey`. The `beforeModel` hook accesses this context from the `runtime` object to perform the rate-limiting check and can optionally jump to 'END' if the limit is exceeded. Context is provided during agent invocation.

```typescript
import * as z from "zod";
import { createMiddleware, HumanMessage } from "langchain";

const rateLimitMiddleware = createMiddleware({
  name: "RateLimitMiddleware",
  contextSchema: z.object({
    maxRequestsPerMinute: z.number(),
    apiKey: z.string(),
  }),
  beforeModel: async (state, runtime) => {
    // Access context through runtime
    const { maxRequestsPerMinute, apiKey } = runtime.context;

    // Implement rate limiting logic
    const allowed = await checkRateLimit(apiKey, maxRequestsPerMinute);
    if (!allowed) {
      return { jumpTo: "END" };
    }

    return state;
  },
});

// Context is provided through config
await agent.invoke(
  { messages: [new HumanMessage("Process data")] },
  {
    context: {
      maxRequestsPerMinute: 60,
      apiKey: "api-key-123",
    },
  }
);
```

--------------------------------

### ContentBlock.Text

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents plain text content, including markdown files, with options for specifying a title and MIME type.

```APIDOC
## Text Content Block

### Description
Represents plain text content, including markdown files. Allows specifying a title and MIME type for the text.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"text-plain"`
- **text** (string) - Required - The text content
- **title** (string) - Optional - Title of the text content
- **mimeType** (string) - Optional - MIME type of the text (e.g., `text/plain`, `text/markdown`)

### Request Example
```json
{
  "type": "text-plain",
  "text": "# Hello World\nThis is a markdown document.",
  "title": "Greeting",
  "mimeType": "text/markdown"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`text-plain`)
- **text** (string) - The text content
- **title** (string) - Optional - Title of the text content
- **mimeType** (string) - Optional - MIME type of the text

#### Response Example
```json
{
  "type": "text-plain",
  "text": "# Hello World\nThis is a markdown document.",
  "title": "Greeting",
  "mimeType": "text/markdown"
}
```
```

--------------------------------

### Human Message: Using Message Object (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to create a HumanMessage object to represent user input. This method is part of constructing more complex message lists for conversational context.

```typescript
const response = await model.invoke([
  new HumanMessage("What is machine learning?"),
]);
```

--------------------------------

### Unordered Trajectory Match Evaluation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's trajectory contains the same tool calls as a reference trajectory, irrespective of their order. This is useful when the sequence of actions is not critical. It utilizes the 'langchain' and 'agentevals' libraries.

```typescript
import { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from "langchain"
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const getEvents = tool(
  async ({ city }: { city: string }) => {
    return `Concert at the park in ${city} tonight.`;
  },
  {
    name: "get_events",
    description: "Get events happening in a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather, getEvents]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "unordered",
});

async function testMultipleToolsAnyOrder() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's happening in SF today?")]
  });

  // Reference shows tools called in different order than actual execution
  const referenceTrajectory = [
    new HumanMessage("What's happening in SF today?"),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_1", name: "get_events", args: { city: "SF" } },

```

--------------------------------

### ContentBlock.Tools.ToolCall

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents a function call to a tool, including the tool's name, arguments, and a unique identifier.

```APIDOC
## ToolCall Content Block

### Description
Represents a function call to a tool, including the tool's name, arguments, and a unique identifier.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"tool_call"`
- **name** (string) - Required - Name of the tool to call
- **args** (object) - Required - Arguments to pass to the tool
- **id** (string) - Required - Unique identifier for this tool call

### Request Example
```json
{
  "type": "tool_call",
  "name": "search",
  "args": {
    "query": "weather"
  },
  "id": "call_123"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`tool_call`)
- **name** (string) - Name of the tool called
- **args** (object) - Arguments passed to the tool
- **id** (string) - Unique identifier for the tool call

#### Response Example
```json
{
  "type": "tool_call",
  "name": "search",
  "args": {
    "query": "weather"
  },
  "id": "call_123"
}
```
```

--------------------------------

### Exact Trajectory Match Evaluation in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's output messages exactly match a reference trajectory, including the order and content of tool calls and messages. This is useful for ensuring deterministic agent behavior. It requires defining the expected sequence of messages and tool interactions.

```typescript
import { createAgent } from "langchain";
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "exact",
});

async function testAgentExactMatch() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in SF?")]
  });

  const referenceTrajectory = [
    new HumanMessage({ content: "What's the weather in SF?" }),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_2", name: "get_weather", args: { city: "SF" } },
      ]
    }),
    new ToolMessage({
      content: "It's 75 degrees and sunny in SF.",
      tool_call_id: "call_2"
    }),
    new AIMessage("Today in SF: 75 degrees and sunny."),
  ];

  const evaluation = await evaluator({
    outputs: result.messages,
    referenceOutputs: referenceTrajectory,
  });
  // {
  //     'key': 'trajectory_exact_match',
  //     'score': true,
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Sanitize SQL Query (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Defines a function `sanitizeSqlQuery` to validate and sanitize SQL SELECT queries. It prevents multiple statements, enforces read-only operations (SELECT only), blocks DML/DDL commands, and appends a LIMIT clause if not present. This ensures query safety before execution.

```typescript
const DENY_RE = /\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\b/i;
const HAS_LIMIT_TAIL_RE = /\blimit\b\s+\d+(\s*,\s*\d+)?\s*;?\s*$/i;

function sanitizeSqlQuery(q) {
  let query = String(q ?? "").trim();

  // block multiple statements (allow one optional trailing ;)
  const semis = [...query].filter((c) => c === ";").length;
  if (semis > 1 || (query.endsWith(";") && query.slice(0, -1).includes(";"))) {
    throw new Error("multiple statements are not allowed.")
  }
  query = query.replace(/;+\s*$/g, "").trim();

  // read-only gate
  if (!query.toLowerCase().startsWith("select")) {
    throw new Error("Only SELECT statements are allowed")
  }
  if (DENY_RE.test(query)) {
    throw new Error("DML/DDL detected. Only read-only queries are permitted.")
  }

  // append LIMIT only if not already present
  if (!HAS_LIMIT_TAIL_RE.test(query)) {
    query += " LIMIT 5";
  }
  return query;
}
```

--------------------------------

### Video Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Details the properties for a video content block. It requires 'type' to be 'video' and allows for specifying the video source via 'url', 'data' (Base64), or 'fileId', including its 'mimeType'.

```typescript
{
  type: "video",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Streaming Tool Call Fragments

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the structure for receiving streaming fragments of tool calls. This is useful for real-time updates during tool execution, providing partial arguments and chunk information.

```typescript
{
  type: "tool_call_chunk",
  name?: "search",
  args?: "{ query: \"weather\" }",
  id?: "call_123",
  index: 0
}
```

--------------------------------

### Wrap-style Model Retry Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implements a Wrap-style middleware for retrying model calls. The `wrapModelCall` hook attempts to execute the handler multiple times, catching errors and retrying up to a specified limit. If all retries fail, the last error is re-thrown. This enhances robustness by handling transient model errors.

```typescript
import { createMiddleware } from "langchain";

const createRetryMiddleware = (maxRetries: number = 3) => {
  return createMiddleware({
    name: "RetryMiddleware",
    wrapModelCall: (request, handler) => {
      for (let attempt = 0; attempt < maxRetries; attempt++) {
        try {
          return handler(request);
        } catch (e) {
          if (attempt === maxRetries - 1) {
            throw e;
          }
          console.log(`Retry ${attempt + 1}/${maxRetries} after error: ${e}`);
        }
      }
      throw new Error("Unreachable");
    },
  });
};
```

--------------------------------

### Define and Invoke a Subagent Tool in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent

This snippet demonstrates how to define a tool that calls a subagent within a Langchain agent. It uses `createAgent` and `tool` from 'langchain', along with Zod for schema definition. The tool takes a query, invokes the subagent, and returns the last message's text. It's designed for direct subagent invocation.

```typescript
import { createAgent, tool } from "langchain";
import * as z from "zod";

const subagent1 = createAgent({...});

const callSubagent1 = tool(
  async ({ query }) => {
    const result = await subagent1.invoke({
      messages: [{ role: "user", content: query }]
    });
    return result.messages.at(-1)?.text;
  },
  {
    name: "subagent1_name",
    description: "subagent1_description",
    schema: z.object({
      query: z.string().describe("The query to to send to subagent1."),
    }),
  }
);

const agent = createAgent({
  model,
  tools: [callSubagent1]
});
```

--------------------------------

### Handling Invalid Tool Calls

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents malformed tool calls that failed during processing. It includes the tool name, raw arguments, and a specific error message detailing the failure reason, such as invalid JSON or missing fields.

```typescript
{
  type: "invalid_tool_call",
  name: "search",
  args: "{ query: weather }",
  error: "Invalid JSON"
}
```

--------------------------------

### Inspect Captured Interrupt Events Details

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript code iterates through a list of captured interrupt events and logs the description and arguments of each action request. This allows developers to review the details of interrupted tool calls, such as calendar event creation or email sending, providing context for manual intervention or automated handling.

```typescript
for (const interrupt of interrupts) {
  for (const request of interrupt.value.actionRequests) {
    console.log(`INTERRUPTED: ${interrupt.id}`);
    console.log(`${request.description}\n`);
  }
}
```

--------------------------------

### ContentBlock.Multimodal.Image

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents image data, which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.Image

### Description
Image data.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"image"`
- **url** (string) - Optional - URL pointing to the image location.
- **data** (string) - Optional - Base64-encoded image data.
- **fileId** (string) - Optional - Reference ID to an externally stored image.
- **mimeType** (string) - Optional - Image MIME type (e.g., `image/jpeg`, `image/png`)

### Request Example
```json
{
    "type": "image",
    "url": "http://example.com/image.jpg",
    "mimeType": "image/jpeg"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"image"`
- **url** (string) - URL pointing to the image location.
- **data** (string) - Base64-encoded image data.
- **fileId** (string) - Reference ID to an externally stored image.
- **mimeType** (string) - Image MIME type

#### Response Example
```json
{
    "type": "image",
    "url": "http://example.com/image.jpg",
    "mimeType": "image/jpeg"
}
```
```

--------------------------------

### Streaming Server-Side Tool Call Fragments

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Handles streaming fragments of server-side tool calls. It provides the tool call ID, name, partial arguments, and the chunk's position in the stream.

```typescript
{
  type: "server_tool_call_chunk",
  id: "server_call_abc",
  name: "search",
  args: "{ \"query\": \"weather\" }",
  index: 0
}
```

--------------------------------

### File Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Describes the generic file content block, identified by 'type' set to 'file'. Similar to other multimodal types, it can reference files using 'url', 'data' (Base64), or 'fileId', and specifies the 'mimeType'.

```typescript
{
  type: "file",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Split Documents with RecursiveCharacterTextSplitter (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Splits a large document into smaller chunks using RecursiveCharacterTextSplitter. This is useful for models with context window limitations. It takes an array of Document objects and returns an array of split Document objects. The chunkSize and chunkOverlap parameters control the size and overlap of the chunks.

```typescript
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});
const allSplits = await splitter.splitDocuments(docs);
console.log(`Split blog post into ${allSplits.length} sub-documents.`);
```

--------------------------------

### Superset Trajectory Match Evaluation in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's output trajectory contains at least all the tool calls specified in the reference trajectory, allowing for additional tool calls. This is useful when the exact sequence or number of tool calls is not critical, but essential steps must be present. The `trajectoryMatchMode` is set to 'superset'.

```typescript
import { createAgent } from "langchain";
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const getDetailedForecast = tool(
  async ({ city }: { city: string }) => {
    return `Detailed forecast for ${city}: sunny all week.`;
  },
  {
    name: "get_detailed_forecast",
    description: "Get detailed weather forecast for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather, getDetailedForecast]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "superset",
});

async function testAgentCallsRequiredToolsPlusExtra() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in Boston?")]
  });

  // Reference only requires getWeather, but agent may call additional tools
  const referenceTrajectory = [
    new HumanMessage("What's the weather in Boston?"),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_1", name: "get_weather", args: { city: "Boston" } },
      ]
    }),
    new ToolMessage({
      content: "It's 75 degrees and sunny in Boston.",
      tool_call_id: "call_1"
    }),
    new AIMessage("The weather in Boston is 75 degrees and sunny."),
  ];

  const evaluation = await evaluator({
    outputs: result.messages,
    referenceOutputs: referenceTrajectory,
  });
  // {
  //     'key': 'trajectory_superset_match',
  //     'score': true,
  //     'comment': null,
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Combine AIMessageChunk Objects in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to iterate over a stream of AIMessageChunk objects and combine them into a single, complete AIMessage object. This is useful for processing streaming responses from AI models.

```typescript
import { AIMessageChunk } from "langchain";

let finalChunk: AIMessageChunk | undefined;
for (const chunk of chunks) {
  finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;
}
```

--------------------------------

### ContentBlock.Tools.ServerToolCallChunk

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents streaming fragments of a server-side tool call.

```APIDOC
## ServerToolCallChunk Content Block

### Description
Represents streaming fragments of a server-side tool call. Used for partial updates during server tool execution.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"server_tool_call_chunk"`
- **id** (string) - Optional - An identifier associated with the tool call
- **name** (string) - Optional - Name of the tool being called
- **args** (string) - Optional - Partial tool arguments (may be incomplete JSON)
- **index** (number | string) - Optional - Position of this chunk in the stream

### Request Example
```json
{
  "type": "server_tool_call_chunk",
  "id": "server_call_abc",
  "index": 1,
  "args": ", \"limit\": 10\""
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`server_tool_call_chunk`)
- **id** (string) - Identifier for the tool call chunk
- **name** (string) - Name of the tool being called
- **args** (string) - Partial tool arguments
- **index** (number | string) - Position of this chunk in the stream

#### Response Example
```json
{
  "type": "server_tool_call_chunk",
  "id": "server_call_abc",
  "index": 1,
  "args": ", \"limit\": 10\""
}
```
```

--------------------------------

### Implement Safety Guardrail with After Agent Hook

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

This middleware validates the final AI response after the agent has finished processing. It uses a separate model to check for safety and appropriateness, returning a canned response if the output is deemed unsafe. This is useful for preventing harmful or inappropriate content from reaching the user.

```typescript
import { createMiddleware, AIMessage, initChatModel } from "langchain";

const safetyGuardrailMiddleware = () => {
  const safetyModel = initChatModel("gpt-4o-mini");

  return createMiddleware({
    name: "SafetyGuardrailMiddleware",
    afterAgent: async (state) => {
      // Get the final AI response
      if (!state.messages || state.messages.length === 0) {
        return;
      }

      const lastMessage = state.messages[state.messages.length - 1];
      if (lastMessage._getType() !== "ai") {
        return;
      }

      // Use a model to evaluate safety
      const safetyPrompt = `Evaluate if this response is safe and appropriate.
      Respond with only 'SAFE' or 'UNSAFE'.

      Response: ${lastMessage.content.toString()}`;

      const result = await safetyModel.invoke([
        { role: "user", content: safetyPrompt }
      ]);

      if (result.content.toString().includes("UNSAFE")) {
        return {
          messages: [
            new AIMessage(
              "I cannot provide that response. Please rephrase your request."
            )
          ],
          jumpTo: "end",
        };
      }

      return;
    },
  });
};

// Use the safety guardrail
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, calculatorTool],
  middleware: [safetyGuardrailMiddleware()],
});

const result = await agent.invoke({
  messages: [{ role: "user", content: "How do I make explosives?" }]
});
```

--------------------------------

### Access Usage Metadata from AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to invoke a chat model and access the `usage_metadata` field of the returned AIMessage. This field can contain information such as input tokens, output tokens, and total tokens used for the model interaction.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel("gpt-5-nano");

const response = await model.invoke("Hello!");
console.log(response.usage_metadata);
```

--------------------------------

### Trim Messages Utility in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

Demonstrates how to use the `trimMessages` utility from LangChain.js to manage message history within an agent's state. This function helps prevent exceeding LLM context window limits by selectively removing messages based on specified strategies and token counts. It's typically used with a `stateModifier` before calling the LLM.

```typescript
import {
    createAgent,
    trimMessages,
    type AgentState,
} from "langchain";
import { MemorySaver } from "@langchain/langgraph";

// This function will be called every time before the node that calls LLM
const stateModifier = async (state: AgentState) => {
    return {
        messages: await trimMessages(state.messages, {
        strategy: "last",
        maxTokens: 384,
        startOn: "human",
        endOn: ["human", "tool"],
        tokenCounter: (msgs) => msgs.length,
        }),
    };
};

const checkpointer = new MemorySaver();
const agent = createAgent({
    model: "gpt-5",
    tools: [],
    preModelHook: stateModifier,
    checkpointer,
});
```

--------------------------------

### Summarization Middleware for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implement automatic summarization of conversation history to manage token limits. This middleware helps preserve context in long-running conversations by summarizing older messages when a token threshold is reached. It requires specifying the summarization model, the token limit for triggering summarization, and the number of recent messages to retain.

```typescript
import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      maxTokensBeforeSummary: 4000, // Trigger summarization at 4000 tokens
      messagesToKeep: 20, // Keep last 20 messages after summary
      summaryPrompt: "Custom prompt for summarization...", // Optional
    }),
  ],
});
```

--------------------------------

### ContentBlock.Tools.ToolCallChunk

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents streaming fragments of a tool call, used for partial updates during tool execution.

```APIDOC
## ToolCallChunk Content Block

### Description
Represents streaming fragments of a tool call, used for partial updates during tool execution.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"tool_call_chunk"`
- **name** (string) - Optional - Name of the tool being called
- **args** (string) - Optional - Partial tool arguments (may be incomplete JSON)
- **id** (string) - Optional - Tool call identifier
- **index** (number | string) - Required - Position of this chunk in the stream

### Request Example
```json
{
  "type": "tool_call_chunk",
  "id": "call_123",
  "index": 0,
  "args": "{\"query\": \"weather\""
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`tool_call_chunk`)
- **name** (string) - Name of the tool being called
- **args** (string) - Partial tool arguments
- **id** (string) - Tool call identifier
- **index** (number | string) - Position of this chunk in the stream

#### Response Example
```json
{
  "type": "tool_call_chunk",
  "id": "call_123",
  "index": 0,
  "args": "{\"query\": \"weather\""
}
```
```

--------------------------------

### Save User Info Tool for Long-Term Memory in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory

This TypeScript code defines a tool to save user information, utilizing an in-memory store for persistence. It includes input validation via Zod schemas and demonstrates how to store and retrieve data associated with a user ID. This pattern is crucial for applications requiring persistent user data or chat history.

```typescript
import * as z from "zod";
import { tool, createAgent, type AgentRuntime } from "langchain";
import { InMemoryStore, type Runtime } from "@langchain/langgraph";

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.
const store = new InMemoryStore(); // [!code highlight]

const contextSchema = z.object({
    userId: z.string(),
});

// Schema defines the structure of user information for the LLM
const UserInfo = z.object({
    name: z.string(),
});

// Tool that allows agent to update user information (useful for chat applications)
const saveUserInfo = tool(
  async (userInfo: z.infer<typeof UserInfo>, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    const userId = runtime.context?.userId;
    if (!userId) {
      throw new Error("userId is required");
    }
    // Store data in the store (namespace, key, data)
    await runtime.store.put(["users"], userId, userInfo);
    return "Successfully saved user info.";
  },
  {
    name: "save_user_info",
    description: "Save user info",
    schema: UserInfo,
  }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [saveUserInfo],
    contextSchema,
    store, // [!code highlight]
});

// Run the agent
await agent.invoke(
    { messages: [{ role: "user", content: "My name is John Smith" }] },
    // userId passed in context to identify whose information is being updated
    { context: { userId: "user_123" } } // [!code highlight]
);

// You can access the store directly to get the value
const result = await store.get(["users"], "user_123");
console.log(result?.value); // Output: { name: "John Smith" }
```

--------------------------------

### ContentBlock.Tools.InvalidToolCall

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents an invalid or malformed tool call, including details about the error.

```APIDOC
## InvalidToolCall Content Block

### Description
Represents an invalid or malformed tool call, including details about the error. Common errors include invalid JSON or missing required fields.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"invalid_tool_call"`
- **name** (string) - Optional - Name of the tool that failed to be called
- **args** (string) - Optional - Raw arguments that failed to parse
- **error** (string) - Required - Description of what went wrong

### Request Example
```json
{
  "type": "invalid_tool_call",
  "name": "search",
  "args": "{\"query\": \"weather",
  "error": "Invalid JSON: Unexpected end of JSON input"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`invalid_tool_call`)
- **name** (string) - Name of the tool that failed
- **args** (string) - Raw arguments provided
- **error** (string) - Description of the error

#### Response Example
```json
{
  "type": "invalid_tool_call",
  "name": "search",
  "args": "{\"query\": \"weather",
  "error": "Invalid JSON: Unexpected end of JSON input"
}
```
```

--------------------------------

### Read Authentication Status from State in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This tool reads the current authentication status from the runtime state. It checks if the user is authenticated and returns a corresponding message. This is useful for tools that require user authentication to perform their actions.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const checkAuthentication = tool(
  async (_, { runtime }) => {
    // Read from State: check current auth status
    const currentState = runtime.state;
    const isAuthenticated = currentState.authenticated || false;

    if (isAuthenticated) {
      return "User is authenticated";
    } else {
      return "User is not authenticated";
    }
  },
  {
    name: "check_authentication",
    description: "Check if user is authenticated",
    schema: z.object({}),
  }
);

```

--------------------------------

### ContentBlock.Tools.ServerToolResult

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents the result of a server-side tool execution, linked to a specific tool call.

```APIDOC
## ServerToolResult Content Block

### Description
Represents the result of a server-side tool execution. It is linked to a specific server tool call via `tool_call_id`.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"server_tool_result"`
- **tool_call_id** (string) - Required - Identifier of the corresponding server tool call
- **id** (string) - Optional - Identifier associated with the server tool result

### Request Example
```json
{
  "type": "server_tool_result",
  "tool_call_id": "server_call_abc",
  "id": "result_xyz",
  "content": "[\n  {\"id\": 1, \"name\": \"Alice\"},\n  {\"id\": 2, \"name\": \"Bob\"}\n]"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`server_tool_result`)
- **tool_call_id** (string) - Identifier of the corresponding server tool call
- **id** (string) - Identifier associated with the server tool result
- **content** (string) - The actual result content from the tool execution

#### Response Example
```json
{
  "type": "server_tool_result",
  "tool_call_id": "server_call_abc",
  "id": "result_xyz",
  "content": "[\n  {\"id\": 1, \"name\": \"Alice\"},\n  {\"id\": 2, \"name\": \"Bob\"}\n]"
}
```
```

--------------------------------

### Strict Trajectory Match Evaluation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's trajectory exactly matches a reference trajectory in terms of messages and tool calls, maintaining the same order. This is useful for testing specific operational sequences. It requires the 'langchain' and 'agentevals' libraries.

```typescript
import { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from "langchain"
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({
      city: z.string(),
    }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "strict",
});

async function testWeatherToolCalledStrict() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in San Francisco?")]
  });

  const referenceTrajectory = [
    new HumanMessage("What's the weather in San Francisco?"),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_1", name: "get_weather", args: { city: "San Francisco" } }
      ]
    }),
    new ToolMessage({
      content: "It's 75 degrees and sunny in San Francisco.",
      tool_call_id: "call_1"
    }),
    new AIMessage("The weather in San Francisco is 75 degrees and sunny."),
  ];

  const evaluation = await evaluator({
    outputs: result.messages,
    referenceOutputs: referenceTrajectory
  });
  // {
  //     'key': 'trajectory_strict_match',
  //     'score': true,
  //     'comment': null,
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Configure PII Redaction Middleware in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This middleware detects and handles Personally Identifiable Information (PII) in conversations. It supports various strategies like redaction, masking, blocking, and hashing, and can be applied to input, output, or tool results. Custom PII types can be defined using regular expressions.

```typescript
import { createAgent, piiRedactionMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    // Redact emails in user input
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToInput: true,
    }),
    // Mask credit cards (show last 4 digits)
    piiRedactionMiddleware({
      piiType: "credit_card",
      strategy: "mask",
      applyToInput: true,
    }),
    // Custom PII type with regex
    piiRedactionMiddleware({
      piiType: "api_key",
      detector: /sk-[a-zA-Z0-9]{32}/,
      strategy: "block", // Throw error if detected
    }),
  ],
});
```

--------------------------------

### Human-in-the-Loop Middleware for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Enable human oversight for agent actions by pausing execution for approval, editing, or rejection of tool calls. This middleware is crucial for high-stakes operations or compliance workflows. It allows granular control over which tool calls require human intervention and the type of interaction allowed (accept, edit, respond).

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [readEmailTool, sendEmailTool],
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: {
        // Require approval, editing, or rejection for sending emails
        send_email: {
          allowAccept: true,
          allowEdit: true,
          allowRespond: true,
        },
        // Auto-approve reading emails
        read_email: false,
      }
    })
  ]
});
```

--------------------------------

### Split Documents using RecursiveCharacterTextSplitter in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Splits an array of Document objects into smaller chunks using the RecursiveCharacterTextSplitter. This splitter recursively divides text based on common separators, ensuring chunks are of a specified size with overlapping content to maintain context. Dependencies include the '@langchain/textsplitters' package.

```typescript
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});

const allSplits = await textSplitter.splitDocuments(docs);

console.log(allSplits.length);
```

--------------------------------

### Define Nested Structured Output Schema with Zod in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet illustrates how to define a Zod schema with nested objects and arrays for complex structured outputs. It shows a `MovieDetail` schema that includes a `cast` array of `Actor` objects and a `genres` array of strings.

```typescript
import * as z from "zod";

const Actor = z.object({
  name: z.string(),
  role: z.string(),
});

const MovieDetails = z.object({
  title: z.string(),
  year: z.number(),
  cast: z.array(Actor),
  genres: z.array(z.string()),
  budget: z.number().nullable().describe("Budget in millions USD"),
});

const modelWithStructure = model.withStructuredOutput(MovieDetails);
```

--------------------------------

### Image Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the properties for an image content block. It includes 'type' set to 'image' and can optionally specify the image location via 'url', 'data' (Base64), or 'fileId', along with its 'mimeType'.

```typescript
{
  type: "image",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Edit Email Subject Using Command in LangGraphJS

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This snippet shows how to use the `Command` class to specify decisions for interrupts, specifically editing the subject of an outbound email. It imports `Command` from `@langchain/langgraph` and processes a stream of agent events.

```typescript
import { Command } from "@langchain/langgraph"; // [!code highlight]

const resume: Record<string, any> = {};
for (const interrupt of interrupts) {
  const actionRequest = interrupt.value.actionRequests[0];
  if (actionRequest.name === "send_email") {
    // Edit email
    const editedAction = { ...actionRequest };
    editedAction.arguments.subject = "Mockups reminder";
    resume[interrupt.id] = {
      decisions: [{ type: "edit", editedAction }]
    };
  } else {
    resume[interrupt.id] = { decisions: [{ type: "approve" }] };
  }
}

const resumeStream = await supervisorAgent.stream(
  new Command({ resume }), // [!code highlight]
  config
);

for await (const step of resumeStream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Server-Side Tool Call Execution

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents a tool call intended for server-side execution. It includes a unique ID, the tool's name, and partial arguments, which may be streamed.

```typescript
{
  type: "server_tool_call",
  id: "server_call_abc",
  name: "search",
  args: "{ \"query\": \"weather\" }"
}
```

--------------------------------

### Representing Server Tool Results

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the structure for results returned from server-side tool executions. It links the result to the corresponding server tool call via `tool_call_id` and includes an optional identifier for the result itself.

```typescript
{
  type: "server_tool_result",
  tool_call_id: "server_call_abc",
  id: "result_xyz"
}
```

--------------------------------

### Node-style Conversation Length Limit Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Creates a Node-style middleware to enforce a maximum number of messages in a conversation. The `beforeModel` hook checks the message count and, if the limit is reached, returns a new AIMessage and jumps to the end of the agent's execution flow. This prevents excessively long conversations.

```typescript
import { createMiddleware, AIMessage } from "langchain";

const createMessageLimitMiddleware = (maxMessages: number = 50) => {
  return createMiddleware({
    name: "MessageLimitMiddleware",
    beforeModel: (state) => {
      if (state.messages.length === maxMessages) {
        return {
          messages: [new AIMessage("Conversation limit reached.")],
          jumpTo: "end",
        };
      }
      return;
    },
  });
};
```

--------------------------------

### Trim Message History Before Model Call in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This middleware trims the message history before a model call to manage token limits. It uses the `trimMessages` function from Langchain, with a strategy to keep the last messages and specifies token counting. This prevents exceeding context window limits for the model.

```typescript
import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware, trimMessages, type AgentState } from "langchain";

const trimMessageHistory = createMiddleware({
  name: "TrimMessages",
  beforeModel: async (state) => {
    const trimmed = await trimMessages(state.messages, {
      maxTokens: 384,
      strategy: "last",
      startOn: "human",
      endOn: ["human", "tool"],
      tokenCounter: (msgs) => msgs.length,
    });
    return { messages: trimmed };
  },
});

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [],
    middleware: [trimMessageHistory],
});
```

--------------------------------

### Limit Model Calls with LangChain.js Middleware

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implement model call limits in your LangChain.js agents using the modelCallLimitMiddleware. This middleware helps prevent excessive API calls and manage costs by setting maximum calls per thread and per run. It supports graceful termination or throwing an error when limits are reached.

```typescript
import { createAgent, modelCallLimitMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    modelCallLimitMiddleware({
      threadLimit: 10, // Max 10 calls per thread (across runs)
      runLimit: 5, // Max 5 calls per run (single invocation)
      exitBehavior: "end", // Or "error" to throw exception
    }),
  ],
});
```

--------------------------------

### Edit Tool Call with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Utilize the 'edit' decision type to modify a tool call before execution. This allows for changing the tool name or its arguments. The 'editedAction' field within the decision object specifies the new tool name and arguments.

```typescript
await agent.invoke(
    new Command({
        // Decisions are provided as a list, one per action under review.
        // The order of decisions must match the order of actions
        // listed in the `__interrupt__` request.
        resume: {
            decisions: [
                {
                    type: "edit",
                    // Edited action with tool name and args
                    editedAction: {
                        // Tool name to call.
                        // Will usually be the same as the original action.
                        name: "new_tool_name",
                        // Arguments to pass to the tool.
                        args: { key1: "new_value", key2: "original_value" },
                    }
                }
            ]
        }
    }),
    config  // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Use Artifact in ToolMessage for Metadata in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to include an 'artifact' field within a ToolMessage to pass supplementary data, such as retrieval metadata, that is not sent to the model but can be accessed programmatically. This is useful for storing debugging information or downstream processing data.

```typescript
import { ToolMessage } from "langchain";

// Artifact available downstream
const artifact = { document_id: "doc_123", page: 0 };

const toolMessage = new ToolMessage({
  content: "It was the best of times, it was the worst of times.",
  tool_call_id: "call_123",
  name: "search_books",
  artifact
});
```

--------------------------------

### Add Metadata to HumanMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to add custom metadata, such as a 'name' field, to a HumanMessage object. The behavior of the 'name' field can vary depending on the model provider, so refer to the provider's documentation for specifics.

```typescript
const humanMsg = new HumanMessage({
  content: "Hello!",
  name: "alice",
  id: "msg_123",
});
```

--------------------------------

### Validate Model Response After Model Call in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This middleware runs after a model call to validate the response. It checks if the last message contains the word 'confidential' (case-insensitive). If it does, the entire message history is removed using `RemoveMessage`. This helps in filtering out sensitive information.

```typescript
import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware, type AgentState } from "langchain";

const validateResponse = createMiddleware({
  name: "ValidateResponse",
  afterModel: (state) => {
    const lastMessage = state.messages.at(-1)?.content;
    if (typeof lastMessage === "string" && lastMessage.toLowerCase().includes("confidential")) {
      return {
        messages: [new RemoveMessage({ id: "all" }), ...state.messages],
      };
    }
    return;
  },
});

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [],
    middleware: [validateResponse],
});
```

--------------------------------

### Control max concurrency for batch requests in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

When processing a large number of inputs using `batch()`, you can control the maximum number of parallel calls by setting the `maxConcurrency` attribute within the `RunnableConfig` dictionary. This prevents overwhelming the model or exceeding rate limits.

```typescript
model.batch(
  listOfInputs,
  {
    maxConcurrency: 5,  // Limit to 5 parallel calls
  }
)
```

--------------------------------

### Remove Specific Messages from State (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This code snippet demonstrates how to remove the earliest two messages from a state object using the `RemoveMessage` class. It checks if there are more than two messages before performing the removal, ensuring the operation is only executed when applicable. This is useful for managing message history length.

```typescript
import { RemoveMessage } from "@langchain/core/messages";

const deleteMessages = (state) => {
    const messages = state.messages;
    if (messages.length > 2) {
        // remove the earliest two messages
        return {
        messages: messages
            .slice(0, 2)
            .map((m) => new RemoveMessage({ id: m.id })),
        };
    }
};
```